[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Benjamin-Chung Lab Manual",
    "section": "",
    "text": "1 Welcome to the Benjamin-Chung Lab!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to the Benjamin-Chung Lab!</span>"
    ]
  },
  {
    "objectID": "index.html#about-the-lab",
    "href": "index.html#about-the-lab",
    "title": "Benjamin-Chung Lab Manual",
    "section": "1.1 About the lab",
    "text": "1.1 About the lab\nWelcome to the lab of Dr. Jade Benjamin-Chung, Assistant Professor in Epidemiology & Population Health at Stanford University. Our mission is to improve population health by creating high quality evidence about what health interventions work in whom and where, when, and how to implement them. Most of our research is focused on infectious diseases, including malaria, diarrhea, soil-transmitted helminths, and influenza. Our focus is on improving the health of vulnerable populations from low-resource settings, both domestically and internationally. We use a variety of epidemiologic, computational, and statistical methods, including causal inference and machine learning methods, in pursuit of our mission. To learn more about the lab, visit jadebc.net.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to the Benjamin-Chung Lab!</span>"
    ]
  },
  {
    "objectID": "index.html#about-this-lab-manual",
    "href": "index.html#about-this-lab-manual",
    "title": "Benjamin-Chung Lab Manual",
    "section": "1.2 About this lab manual",
    "text": "1.2 About this lab manual\nThis lab manual covers our communication strategy, code of conduct, and best practices for reproducibility of computational workflows. It is a living document that is updated regularly.\nThis manual was created with input from a large number of team members and with inspiration from other scientists’ lab manuals. Contributors include Jade Benjamin-Chung, Kunal Mishra, Stephanie Djajadi, Nolan Pokpongkiat, Anna Nguyen, Iris Tong, and Gabby Barratt Heitmann.\nFeel free to draw from this manual (and please cite it if you do!).\n\n\n  This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to the Benjamin-Chung Lab!</span>"
    ]
  },
  {
    "objectID": "01-culture-and-conduct.html",
    "href": "01-culture-and-conduct.html",
    "title": "2  Culture and conduct",
    "section": "",
    "text": "2.1 Lab culture\nby Jade Benjamin-Chung\nWe are committed to a lab culture that is collaborative, supportive, inclusive, open, and free from discrimination and harassment.\nWe encourage students / staff of all experience levels to respectfully share their honest opinions and ideas on any topic. Our group has thrived upon such respectful honest input from team members over the years, and this document is a product of years of student and staff input (and even debate) that has gradually improved our productivity and overall quality of our work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "01-culture-and-conduct.html#diversity-equity-and-inclusion",
    "href": "01-culture-and-conduct.html#diversity-equity-and-inclusion",
    "title": "2  Culture and conduct",
    "section": "2.2 Diversity, equity, and inclusion",
    "text": "2.2 Diversity, equity, and inclusion\nThe Benjamin-Chung lab recognizes the importance of and is committed to cultivating a culture of diversity, equity, and inclusion. This means being a safe, supportive, and anti-racist environment in which students from diverse backgrounds are equally and inclusively supported in their education and training. Diversity takes many forms, and includes, but is not limited to, differences in race, ethnicity, gender, sexuality, socioeconomic status, religion, disability, and political affiliation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "01-culture-and-conduct.html#protecting-human-subjects",
    "href": "01-culture-and-conduct.html#protecting-human-subjects",
    "title": "2  Culture and conduct",
    "section": "2.3 Protecting human subjects",
    "text": "2.3 Protecting human subjects\nAll lab members must complete CITI Human Subjects Biomedical Group 1 training and share their certificate with Jade. She will add team members to relevant Institutional Review Board protocols prior to their start date to ensure they have permission to work with identifiable datasets.\nOne of the most relevant aspects of protecting human subjects in our work is maintaining confidentiality. For students supporting our data science efforts, in practice this means:\n\nBe sure to understand and comply with project-specific policies about where data can be saved, particularly if the data include personal identifiers.\nDo not share data with anyone without permission, including to other members of the group, who might not be on the same IRB protocol as you (check with Jade first).\n\nRemember, data that looks like it does not contain identifiers to you might still be classified as data that requires special protection by our IRB or under HIPAA, so always proceed with caution and ask for help if you have any concerns about how to maintain study participant confidentiality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "01-culture-and-conduct.html#authorship",
    "href": "01-culture-and-conduct.html#authorship",
    "title": "2  Culture and conduct",
    "section": "2.4 Authorship",
    "text": "2.4 Authorship\nWe adhere to the ICMJE Definition of authorship and are happy for team members who meet the definition of authorship to be included as co-authors on scientific manuscripts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "02-communication.html",
    "href": "02-communication.html",
    "title": "3  Communication and coordination",
    "section": "",
    "text": "3.1 Slack\nby Jade Benjamin-Chung\nOne benefit of the academic environment is its schedule flexibility. This means that lab members may choose to work in the early morning, evening, or weekends. That said, we do not expect lab members to respond outside of business hours (unless there are special circumstances).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "02-communication.html#slack",
    "href": "02-communication.html#slack",
    "title": "3  Communication and coordination",
    "section": "",
    "text": "Use Slack for scheduling, coding related questions, quick check ins, etc. If your Slack message exceeds 200 words, it might be time to use email.\nUse channels instead of direct messages unless you need to discuss something private.\nPlease make an effort to respond to messages that message you (e.g., @jade) as quickly as possible and always within 24 hours.\nIf you are unusually busy (e.g., taking MCAT/GRE, taking many exams) or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual and also set your status in Slack (e.g., it could say “On vacation”) so we know not to expect to see you online.\nPlease thread messages in Slack as much as possible.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "02-communication.html#email",
    "href": "02-communication.html#email",
    "title": "3  Communication and coordination",
    "section": "3.2 Email",
    "text": "3.2 Email\n\nUse email for longer messages (&gt;200 words) or messages that merit preservation.\nGenerally, strive to respond within 24 hours hours. As noted above, if you are unusually busy or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "02-communication.html#trello",
    "href": "02-communication.html#trello",
    "title": "3  Communication and coordination",
    "section": "3.3 Trello",
    "text": "3.3 Trello\n\nJade will add new cards within our shared Trello board that outline your tasks.\nThe higher a card is within your list, the higher priority it is.\nGenerally, strive to complete the tasks in your card by the date listed.\nUse checklists to break down a task into smaller chunks. Sometimes Jade will write this for you, but you can also add this yourself.\nJade will move your card to the “Completed” list when it is done.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "02-communication.html#google-drive",
    "href": "02-communication.html#google-drive",
    "title": "3  Communication and coordination",
    "section": "3.4 Google Drive",
    "text": "3.4 Google Drive\n\nWe mostly use Google Drive to create shared documents with longer descriptions of tasks. These documents are linked to in Trello. Jade often shares these with the whole team since tasks are overlapping, and even if a task is assigned to one person, others may have valuable insights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "02-communication.html#stanford-medicine-box",
    "href": "02-communication.html#stanford-medicine-box",
    "title": "3  Communication and coordination",
    "section": "3.5 Stanford Medicine Box",
    "text": "3.5 Stanford Medicine Box\n\nHuman subjects data for research studies are generally stored in Stanford Medicine Box. Please check with Jade about whether there are special storage and transfer requirements for the datasets you are working with for each study.\nIf you are in the School of Medicine, you can access Box via your SUNet ID. If you are no in the School of Medicine, please follow these instructions to get a Box account.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "02-communication.html#meetings",
    "href": "02-communication.html#meetings",
    "title": "3  Communication and coordination",
    "section": "3.6 Meetings",
    "text": "3.6 Meetings\n\nOur meetings start on the hour.\nIf you are going to be late, please send a message in our Slack channel.\nIf you are regularly not able to come on the hour, notify the team and we might choose the modify the agenda order or the start time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html",
    "href": "03-reproducibility.html",
    "title": "4  Reproducibility",
    "section": "",
    "text": "4.1 What is the reproducibility crisis?\nby Jade Benjamin-Chung\nOur lab adopts the following practices to maximize the reproducibility of our work.\nIn the past decade, an increasing number of studies have found that published study findings could not be reproduced. Researchers found that it was not possible to reproduce estimates from published studies: 1) with the same data and same or similar code and 2) with newly collected data using the same (or similar) study design. These “failures” of reproducibility were frequent enough and broad enough in scope, occurring across a range of disciplines (epidemiology, psychology, economics, and others) to be deeply troubling. Program and policy decisions based on erroneous research findings could lead to wasted resources, and at worst, could harm intended beneficiaries. This crisis has motivated new practices in reproducibility, transparency, and openness. Our lab is committed to adopting these best practices, and much of the remainder of the lab manual focuses on how to do so.\nRecommended readings on the “reproducibility crisis”:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#what-is-the-reproducibility-crisis",
    "href": "03-reproducibility.html#what-is-the-reproducibility-crisis",
    "title": "4  Reproducibility",
    "section": "",
    "text": "Nuzzo R. How scientists fool themselves – and how they can stop. 2015. https://www.nature.com/articles/526182a\nStoddart C. Is there a reproducibility crisis in science? 2016. https://www.nature.com/articles/d41586-019-00067-3\nMunafo MR, et al. A manifesto for reproducible science. Nature Human Behavior 2017 http://dx.doi.org/10.1038/s41562-016-0021",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#study-design",
    "href": "03-reproducibility.html#study-design",
    "title": "4  Reproducibility",
    "section": "4.2 Study design",
    "text": "4.2 Study design\nAppropriate study design is beyond the scope of this lab manual and is something trainees develop through their coursework and mentoring.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#register-study-protocols",
    "href": "03-reproducibility.html#register-study-protocols",
    "title": "4  Reproducibility",
    "section": "4.3 Register study protocols",
    "text": "4.3 Register study protocols\nWe register all randomized trials on clinicaltrials.gov, and in some cases register observational studies as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#write-and-register-pre-analysis-plans",
    "href": "03-reproducibility.html#write-and-register-pre-analysis-plans",
    "title": "4  Reproducibility",
    "section": "4.4 Write and register pre-analysis plans",
    "text": "4.4 Write and register pre-analysis plans\nWe write pre-analysis plans for most original research projects that are not exploratory in nature, although in some cases, we write pre-analysis plans for exploratory studies as well. The format and content of pre-analysis plans can vary from project to project. Here is an example of one: https://osf.io/tgbxr/. Generally, these include:\n\nBrief background on the study (a condensed version of the introduction section of the paper)\nHypotheses / objectives\nStudy design\nDescription of data\nDefinition of outcomes\nDefinition of interventions / exposures\nDefinition of covariates\nStatistical power calculation\nStatistical analysis:\n\n\nType of model\nCovariate selection / screening\nStandard error estimation method\nMissing data analysis\nAssessment of effect modification / subgroup analyses\nSensitivity analyses\nNegative control analyses",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#create-reproducible-workflows",
    "href": "03-reproducibility.html#create-reproducible-workflows",
    "title": "4  Reproducibility",
    "section": "4.5 Create reproducible workflows",
    "text": "4.5 Create reproducible workflows\nReproducible workflows allow a user to reproduce study estimates and ideally figures and tables with a “single click”. In practice, this typically means running a single bash script that sources all replication scripts in a repository. These replication scripts complete data processing, data analysis, and figure/table generation. The following chapters provide detailed guidance on this topic:\n\nChapter 5: Code repositories\nChapter 6: Coding practices\nChapter 7: Coding style\nChapter 8: Code publication\nChapter 9: Working with big data\nChapter 10: Github\nChapter 11: Unix",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#process-and-analyze-data-with-internal-replication-and-masking",
    "href": "03-reproducibility.html#process-and-analyze-data-with-internal-replication-and-masking",
    "title": "4  Reproducibility",
    "section": "4.6 Process and analyze data with internal replication and masking",
    "text": "4.6 Process and analyze data with internal replication and masking\nSee my video on this topic: https://www.youtube.com/watch?v=WoYkY9MkbRE",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#use-reporting-checklists-with-manuscripts",
    "href": "03-reproducibility.html#use-reporting-checklists-with-manuscripts",
    "title": "4  Reproducibility",
    "section": "4.7 Use reporting checklists with manuscripts",
    "text": "4.7 Use reporting checklists with manuscripts\nUsing reporting checklists helps ensure that peer-reviewed articles contain the information needed for readers to assess the validity of your work and/or attempt to reproduce it. A collection of reporting checklists is available here: https://www.equator-network.org/about-us/what-is-a-reporting-guideline/)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#publish-preprints",
    "href": "03-reproducibility.html#publish-preprints",
    "title": "4  Reproducibility",
    "section": "4.8 Publish preprints",
    "text": "4.8 Publish preprints\nA preprint is a scientific manuscript that has not been peer reviewed. Preprint servers create digital object identifiers (DOIs) and can be cited in other articles and in grant applications. Because the peer review process can take many months, publishing preprints prior to or during peer review enables other scientists to immediately learn from and build on your work. Importantly, NIH allows applicants to include preprint citations in their biosketches. In most cases, we publish preprints on medRxiv.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "03-reproducibility.html#publish-data-when-possible-and-replication-scripts",
    "href": "03-reproducibility.html#publish-data-when-possible-and-replication-scripts",
    "title": "4  Reproducibility",
    "section": "4.9 Publish data (when possible) and replication scripts",
    "text": "4.9 Publish data (when possible) and replication scripts\nPublishing data and replication scripts allows other scientists to reproduce your work and to build upon it. We typically publish data on Open Science Framework, share links to Github repositories, and archive code on Zenodo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "04-code-repositories.html",
    "href": "04-code-repositories.html",
    "title": "5  Code repositories",
    "section": "",
    "text": "5.1 Project Structure\nBy Kunal Mishra, Jade Benjamin-Chung, and Stephanie Djajadi\nEach study has at least one code repository that typically holds R code, shell scripts with Unix code, and research outputs (results .RDS files, tables, figures). Repositories may also include datasets. This chapter outlines how to organize these files. Adhering to a standard format makes it easier for us to efficiently collaborate across projects.\nWe recommend the following directory structure:\nFor brevity, not every directory is “expanded”, but we can glean some important takeaways from what we do see.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "04-code-repositories.html#project-structure",
    "href": "04-code-repositories.html#project-structure",
    "title": "5  Code repositories",
    "section": "",
    "text": "0-run-project.sh\n0-config.R\n1 - Data-Management/\n    0-prep-data.sh\n    1-prep-cdph-fluseas.R\n    2a-prep-absentee.R\n    2b-prep-absentee-weighted.R\n    3a-prep-absentee-adj.R\n    3b-prep-absentee-adj-weighted.R\n2 - Analysis/\n    0-run-analysis.sh\n    1 - Absentee-Mean/\n        1-absentee-mean-primary.R\n        2-absentee-mean-negative-control.R\n        3-absentee-mean-CDC.R\n        4-absentee-mean-peakwk.R\n        5-absentee-mean-cdph2.R\n        6-absentee-mean-cdph3.R\n    2 - Absentee-Positivity-Check/\n    3 - Absentee-P1/\n    4 - Absentee-P2/\n3 - Figures/\n    0-run-figures.sh\n    ...\n4 - Tables/\n    0-run-tables.sh\n    ...\n5 - Results/\n    1 - Absentee-Mean/\n        1-absentee-mean-primary.RDS\n        2-absentee-mean-negative-control.RDS\n        3-absentee-mean-CDC.RDS\n        4-absentee-mean-peakwk.RDS\n        5-absentee-mean-cdph2.RDS\n        6-absentee-mean-cdph3.RDS\n    ...\n.gitignore\n.Rproj",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "04-code-repositories.html#rproj-files",
    "href": "04-code-repositories.html#rproj-files",
    "title": "5  Code repositories",
    "section": "5.2 .Rproj files",
    "text": "5.2 .Rproj files\nAn “R Project” can be created within RStudio by going to File &gt;&gt; New Project. Depending on where you are with your research, choose the most appropriate option. This will save preferences, working directories, and even the results of running code/data (though I’d recommend starting from scratch each time you open your project, in general). Then, ensure that whenever you are working on that specific research project, you open your created project to enable the full utility of .Rproj files. This also automatically sets the directory to the top level of the project.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "04-code-repositories.html#configuration-config-file",
    "href": "04-code-repositories.html#configuration-config-file",
    "title": "5  Code repositories",
    "section": "5.3 Configuration (‘config’) File",
    "text": "5.3 Configuration (‘config’) File\nThis is the single most important file for your project. It will be responsible for a variety of common tasks, declare global variables, load functions, declare paths, and more. Every other file in the project will begin with source(\"0-config\"), and its role is to reduce redundancy and create an abstraction layer that allows you to make changes in one place (0-config.R) rather than 5 different files. To this end, paths which will be reference in multiple scripts (i.e. a merged_data_path) can be declared in 0-config.R and simply referred to by its variable name in scripts. If you ever want to change things, rename them, or even switch from a downsample to the full data, all you would then to need to do is modify the path in one place and the change will automatically update throughout your project. See the example config file for more details. The paths defined in the 0-config.R file assume that users have opened the .Rproj file, which sets the directory to the top level of the project.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "04-code-repositories.html#order-files-and-directories",
    "href": "04-code-repositories.html#order-files-and-directories",
    "title": "5  Code repositories",
    "section": "5.4 Order Files and Directories",
    "text": "5.4 Order Files and Directories\nThis makes the jumble of alphabetized filenames much more coherent and places similar code and files next to one another. This also helps us understand how data flows from start to finish and allows us to easily map a script to its output (i.e. 2 - Analysis/1 - Absentee-Mean/1-absentee-mean-primary.R =&gt; 5 - Results/1 - Absentee-Mean/1-absentee-mean-primary.RDS). If you take nothing else away from this guide, this is the single most helpful suggestion to make your workflow more coherent. Often the particular order of files will be in flux until an analysis is close to completion. At that time it is important to review file order and naming and reproduce everything prior to drafting a manuscript.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "04-code-repositories.html#using-bash-scripts-to-ensure-reproducibility",
    "href": "04-code-repositories.html#using-bash-scripts-to-ensure-reproducibility",
    "title": "5  Code repositories",
    "section": "5.5 Using Bash scripts to ensure reproducibility",
    "text": "5.5 Using Bash scripts to ensure reproducibility\nBash scripts are useful components of a reproducible workflow. At many of the directory levels (i.e. in 3 - Analysis), there is a bash script that runs each of the analysis scripts. This is exceptionally useful when data “upstream” changes – you simply run the bash script. See the Unix Chapter for further details.\nAfter running bash scripts, .Rout log files will be generated for each script that has been executed. It is important to check these files. Scripts may appear to have run correctly in the terminal, but checking the log files is the only way to ensure that everything has run completely.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html",
    "href": "05-coding-practices.html",
    "title": "6  Coding practices",
    "section": "",
    "text": "6.1 Organizing scripts\nby Kunal Mishra, Jade Benjamin-Chung, Stephanie Djajadi, and Iris Tong\nJust as your data “flows” through your project, data should flow naturally through a script. Very generally, you want to:\nEach of these sections should be “chunked together” using comments. See this file for a good example of how to cleanly organize a file in a way that follows this “flow” and functionally separate pieces of code that are doing different things.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#organizing-scripts",
    "href": "05-coding-practices.html#organizing-scripts",
    "title": "6  Coding practices",
    "section": "",
    "text": "describe the work completed in the script in a comment header\nsource your configuration file (0-config.R)\nload all your data\ndo all your analysis/computation\nsave your data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#documenting-your-code",
    "href": "05-coding-practices.html#documenting-your-code",
    "title": "6  Coding practices",
    "section": "6.2 Documenting your code",
    "text": "6.2 Documenting your code\n\n6.2.1 File headers\nEvery file in a project should have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. You may optionally wish to include the inputs and outputs of the script as well, though the next section makes this significantly less necessary.\n################################################################################\n# @Organization - Example Organization\n# @Project - Example Project\n# @Description - This file is responsible for [...]\n################################################################################\n\n\n\n6.2.2 Sections and subsections\nRstudio (v1.4 or more recent) supports the use of Sections and Subsections. You can easily navigate through longer scripts using the navigation pane in RStudio, as shown on the right below.\n# Section -------\n\n## Subsection -------\n\n### Sub-subsection -------\n\n\n6.2.3 Code folding\nConsider using RStudio’s code folding feature to collapse and expand different sections of your code. Any comment line with at least four trailing dashes (-), equal signs (=), or pound signs (#) automatically creates a code section. For example:\n\n\n6.2.4 Comments in the body of your script\nCommenting your code is an important part of reproducibility and helps document your code for the future. When things change or break, you’ll be thankful for comments. There’s no need to comment excessively or unnecessarily, but a comment describing what a large or complex chunk of code does is always helpful. See this file for an example of how to comment your code and notice that comments are always in the form of:\n# This is a comment -- first letter is capitalized and spaced away from the pound sign\n\n\n6.2.5 Function documentation\nEvery function you write must include a header to document its purpose, inputs, and outputs. For any reproducible workflows, they are essential, because R is dynamically typed. This means, you can pass a string into an argument that is meant to be a data.table, or a list into an argument meant for a tibble. It is the responsibility of a function’s author to document what each argument is meant to do and its basic type. This is an example for documenting a function (inspired by JavaDocs and R’s Plumber API docs):\n##############################################\n##############################################\n# Documentation: calc_fluseas_mean\n# Usage: calc_fluseas_mean(data, yname)\n# Description: Make a dataframe with rows for flu season and site\n# and the number of patients with an outcome, the total patients,\n# and the percent of patients with the outcome\n\n# Args/Options:\n# data: a data frame with variables flu_season, site, studyID, and yname\n# yname: a string for the outcome name\n# silent: a boolean specifying whether the function shouldn't output anything to the console (DEFAULT: TRUE)\n\n# Returns: the dataframe as described above\n# Output: prints the data frame described above if silent is not True\n\ncalc_fluseas_mean = function(data, yname, silent = TRUE) {\n ### function code here \n\n}\nThe header tells you what the function does, its various inputs, and how you might go about using the function to do what you want. Also notice that all optional arguments (i.e. ones with pre-specified defaults) follow arguments that require user input.\n\nNote: As someone trying to call a function, it is possible to access a function’s documentation (and internal code) by CMD-Left-Clicking the function’s name in RStudio\nNote: Depending on how important your function is, the complexity of your function code, and the complexity of different types of data in your project, you can also add “type-checking” to your function with the assertthat::assert_that() function. You can, for example, assert_that(is.data.frame(statistical_input)), which will ensure that collaborators or reviewers of your project attempting to use your function are using it in the way that it is intended by calling it with (at the minimum) the correct type of arguments. You can extend this to ensure that certain assumptions regarding the inputs are fulfilled as well (i.e. that time_column, location_column, value_column, and population_column all exist within the statistical_input tibble).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#object-naming",
    "href": "05-coding-practices.html#object-naming",
    "title": "6  Coding practices",
    "section": "6.3 Object naming",
    "text": "6.3 Object naming\nGenerally we recommend using nouns for objects and verbs for functions. This is because functions are performing actions, while objects are not.\nTry to make your variable names both more expressive and more explicit. Being a bit more verbose is useful and easy in the age of autocompletion! For example, instead of naming a variable vaxcov_1718, try naming it vaccination_coverage_2017_18. Similarly, flu_res could be named absentee_flu_residuals, making your code more readable and explicit.\n\nFor more help, check out Be Expressive: How to Give Your Variables Better Names\n\nWe recommend you use Snake_Case.\n\nBase R allows . in variable names and functions (such as read.csv()), but this goes against best practices for variable naming in many other coding languages. For consistency’s sake, snake_case has been adopted across languages, and modern packages and functions typically use it (i.e. readr::read_csv()). As a very general rule of thumb, if a package you’re using doesn’t use snake_case, there may be an updated version or more modern package that does, bringing with it the variety of performance improvements and bug fixes inherent in more mature and modern software.\nNote: you may also see camelCase throughout the R code you come across. This is okay but not ideal – try to stay consistent across all your code with snake_case.\nNote: again, its also worth noting there’s nothing inherently wrong with using . in variable names, just that it goes against style best practices that are cropping up in data science, so its worth getting rid of these bad habits now.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#function-calls",
    "href": "05-coding-practices.html#function-calls",
    "title": "6  Coding practices",
    "section": "6.4 Function calls",
    "text": "6.4 Function calls\nIn a function call, use “named arguments” and put each argument on a separate line to make your code more readable.\nHere’s an example of what not to do when calling the function a function calc_fluseas_mean (defined above):\nmean_Y = calc_fluseas_mean(flu_data, \"maari_yn\", FALSE)\nAnd here it is again using the best practices we’ve outlined:\nmean_Y = calc_fluseas_mean(\n  data = flu_data, \n  yname = \"maari_yn\",\n  silent = FALSE\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#the-here-package",
    "href": "05-coding-practices.html#the-here-package",
    "title": "6  Coding practices",
    "section": "6.5 The here package",
    "text": "6.5 The here package\nThe here package is one great R package that helps multiple collaborators deal with the mess that is working directories within an R project structure. Let’s say we have an R project at the path /home/oski/Some-R-Project. My collaborator might clone the repository and work with it at some other path, such as /home/bear/R-Code/Some-R-Project. Dealing with working directories and paths explicitly can be a very large pain, and as you might imagine, setting up a Config with paths requires those paths to flexibly work for all contributors to a project. This is where the here package comes in and this a great vignette describing it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#readingsaving-data",
    "href": "05-coding-practices.html#readingsaving-data",
    "title": "6  Coding practices",
    "section": "6.6 Reading/Saving Data",
    "text": "6.6 Reading/Saving Data\n\n6.6.1 .RDS vs .RData Files\nOne of the most common ways to load and save data in Base R is with the load() and save() functions to serialize multiple objects in a single .RData file. The biggest problems with this practice include an inability to control the names of things getting loaded in, the inherent confusion this creates in understanding older code, and the inability to load individual elements of a saved file. For this, we recommend using the RDS format to save R objects.\n\nNote: if you have many related R objects you would have otherwise saved all together using the save function, the functional equivalent with RDS would be to create a (named) list containing each of these objects, and saving it.\n\n\n\n6.6.2 CSVs\nOnce again, the readr package as part of the Tidvyerse is great, with a much faster read_csv() than Base R’s read.csv(). For massive CSVs (&gt; 5 GB), you’ll find data.table::fread() to be the fastest CSV reader in any data science language out there. For writing CSVs, readr::write_csv() and data.table::fwrite() outclass Base R’s write.csv() by a significant margin as well.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#integrating-box-and-dropbox",
    "href": "05-coding-practices.html#integrating-box-and-dropbox",
    "title": "6  Coding practices",
    "section": "6.7 Integrating Box and Dropbox",
    "text": "6.7 Integrating Box and Dropbox\nBox and Dropbox are cloud-based file sharing systems that are useful when dealing with large files. When our scripts generate large output files, the files can slow down the workflow if they are pushed to GitHub. This makes collaboration difficult when not everyone has a copy of the file, unless we decide to duplicate files and share them manually. The files might also take up a lot of local storage. Box and Dropbox help us avoid these issues by automatically storing the files, reading data, and writing data back to the cloud.\nBox and Dropbox are separate platforms, but we can use either one to store and share files. To use them, we can install the packages that have been created to integrate Box and Dropbox into R. The set-up instructions are detailed below.\nMake sure to authenticate before reading and writing from either Box or Dropbox. The authentication commands should go in the configuration file; it only needs to be done once. This will prompt you to give your login credentials for Box and Dropbox and will allow your application to access your shared folders.\n\n6.7.1 Box\nFollow the instructions in this section to use the boxr package. Note that there are a few setup steps that need to be done on the box website before you can use the boxr package, explained here in the section “Creating an Interactive App.” This gets the authentication keys that must be put in box. Once that is done, add the authentication keys to your code in the configuration file, with box_auth(client_id = \"&lt;your_client_id&gt;\", client_secret = \"&lt;your_client_secret_id&gt;\"). It is also important to set the default working directory so that the code can reference the correct folder in box: box_setwd(&lt;folder_id&gt;). The folder ID is the sequence of digits at the end of the URL.\nFurther details can be found here.\n\n\n6.7.2 Dropbox\nFollow the instructions at this link to use the rdrop2 package. Similar to the boxr package, you must authenticate before reading and writing from Dropbox, which can be done by adding drop_auth() to the configuration file.\nSaving the authentication token is not required, although it may be useful if you plan on using Dropbox frequently. To do so, save the token with the following commands. Tokens are valid until they are manually revoked.\n# first time only\n# save the output of drop_auth to an RDS file\ntoken &lt;- drop_auth()\n# this token only has to be generated once, it is valid until revoked\nsaveRDS(token, \"/path/to/tokenfile.RDS\")\n\n# all future usages\n# to use a stored token, provide the rdstoken argument\ndrop_auth(rdstoken = \"/path/to/tokenfile.RDS\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#tidyverse",
    "href": "05-coding-practices.html#tidyverse",
    "title": "6  Coding practices",
    "section": "6.8 Tidyverse",
    "text": "6.8 Tidyverse\nThroughout this document there have been references to the Tidyverse, but this section is to explicitly show you how to transform your Base R tendencies to Tidyverse (or Data.Table, Tidyverse’s performance-optimized competitor). For most of our work that does not utilize very large datasets, we recommend that you code in Tidyverse rather than Base R. Tidyverse is quickly becoming the gold standard in R data analysis and modern data science packages and code should use Tidyverse style and packages unless there’s a significant reason not to (i.e. big data pipelines that would benefit from Data.Table’s performance optimizations).\nThe package author has published a great textbook on R for Data Science, which leans heavily on many Tidyverse packages and may be worth checking out.\nThe following list is not exhaustive, but is a compact overview to begin to translate Base R into something better:\n\n\n\n\n\n\n\nBase R\nBetter Style, Performance, and Utility\n\n\n\n\n_\n_\n\n\nread.csv()\nreadr::read_csv() or data.table::fread()\n\n\nwrite.csv()\nreadr::write_csv() or data.table::fwrite()\n\n\nreadRDS\nreadr::read_rds()\n\n\nsaveRDS()\nreadr::write_rds()\n\n\n_\n_\n\n\ndata.frame()\ntibble::tibble() or data.table::data.table()\n\n\nrbind()\ndplyr::bind_rows()\n\n\ncbind()\ndplyr::bind_cols()\n\n\ndf$some_column\ndf %&gt;% dplyr::pull(some_column)\n\n\ndf$some_column = ...\ndf %&gt;% dplyr::mutate(some_column = ...)\n\n\ndf[get_rows_condition,]\ndf %&gt;% dplyr::filter(get_rows_condition)\n\n\ndf[,c(col1, col2)]\ndf %&gt;% dplyr::select(col1, col2)\n\n\nmerge(df1, df2, by = ..., all.x = ..., all.y = ...)\ndf1 %&gt;% dplyr::left_join(df2, by = ...) or dplyr::full_join or dplyr::inner_join or dplyr::right_join\n\n\n_\n_\n\n\nstr()\ndplyr::glimpse()\n\n\ngrep(pattern, x)\nstringr::str_which(string, pattern)\n\n\ngsub(pattern, replacement, x)\nstringr::str_replace(string, pattern, replacement)\n\n\nifelse(test_expression, yes, no)\nif_else(condition, true, false)\n\n\nNested: ifelse(test_expression1, yes1, ifelse(test_expression2, yes2, ifelse(test_expression3, yes3, no)))\ncase_when(test_expression1 ~ yes1,  test_expression2 ~ yes2, test_expression3 ~ yes3, TRUE ~ no)\n\n\nproc.time()\ntictoc::tic() and tictoc::toc()\n\n\nstopifnot()\nassertthat::assert_that() or assertthat::see_if() or assertthat::validate_that()\n\n\n\nFor a more extensive set of syntactical translations to Tidyverse, you can check out this document.\nWorking with Tidyverse within functions can be somewhat of a pain due to non-standard evaluation (NSE) semantics. If you’re an avid function writer, we’d recommend checking out the following resources:\n\nTidy Eval in 5 Minutes (video)\nTidy Evaluation (e-book)\nData Frame Columns as Arguments to Dplyr Functions (blog)\nStandard Evaluation for *_join (stackoverflow)\nProgramming with dplyr (package vignette)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#coding-with-r-and-python",
    "href": "05-coding-practices.html#coding-with-r-and-python",
    "title": "6  Coding practices",
    "section": "6.9 Coding with R and Python",
    "text": "6.9 Coding with R and Python\nIf you’re using both R and Python, you may wish to check out the Feather package for exchanging data between the two languages extremely quickly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#repeating-analyses-with-different-variations",
    "href": "05-coding-practices.html#repeating-analyses-with-different-variations",
    "title": "6  Coding practices",
    "section": "6.10 Repeating analyses with different variations",
    "text": "6.10 Repeating analyses with different variations\nIn many cases, we will need to apply our modeling on different combinations of interests (outcomes, exposures, etc.). We can certainly use a for loop to repeat the execution of a wrapper function, but generally, for loops request high memory usage and produce the results in long computation time.\nFortunately, R has some functions which implement looping in a compact form to help repeating your analyses with different variations (subgroups, outcomes, covariate sets, etc.) with better performances.\n\n6.10.1 lapply() and sapply()\nlapply() is a function in the base R package that applies a function to each element of a list and returns a list. It’s typically faster than for. Here is a simple generic example:\nresult &lt;- lapply(X = mylist, FUN = func)\nThere is another very similar function called sapply(). It also takes a list as its input, but if the output of the func is of the same length for each element in the input list, then sapply() will simplify the output to the simplest data structure possible, which will usually be a vector.\n\n\n6.10.2 mapply() and pmap()\nSometimes, we’d like to employ a wrapper function that takes arguments from multiple different lists/vectors. Then, we can consider using mapply() from the base R package or pmap() from the purrr package.\nPlease see the simple specific example below where the two input lists are of the same length and we are doing a pairwise calculation:\nmylist1 = list(0:3)\nmylist2 = list(6:9)\nmylists = list(mylist1, mylist2)\n\nsquare_sum &lt;- function(x, y) {\n  x^2 + y^2\n}\n\n#Use `mapply()`\nresult1 &lt;- mapply(FUN = square_sum, mylist1, mylist2)\n\n#Use `pmap()`\nlibrary(purrr)\nresult2 &lt;- pmap(.l = mylists, .f = square_sum)\n\n#unlist(as.list(result1)) = result2 = [36 50 68 90]\n\nThere are two major differences between mapply() and pmap(). The first difference is that mapply() takes seperate lists as its input arguments, while pmap() takes a list of list. Secondly, the output of mapply() will be in the form of a matrix or an array, but pmap() produces a list directly.\nHowever, when the input lists are of different lengths AND/OR the wrapper function doesn’t take arguments in pairs, mapply() and pmap() may not give the preferable results.\nBoth mapply() and pmap() will recycle shorter input lists to match the length of the longest input list. Assume that now mylist2 = list(6:12). Then, pmap(mylists, square_sum) will generate [36  50  68  90 100 122 148] where elements 0, 1, and 2 are recycled to match 10, 11, and 12. And it will return an error message that “longer object length is not a multiple of shorter object length.”\nThus, unless the recycling pattern described above is desirable feature for a certain experiment design, when the input lists are of different lengths, the best practice is probably to use lapply() and then combine the results.\nHere is an example where we’d like to find the square_sum for every element combination of mylist1 and mylist2.\nmylist1 &lt;- list(0:3)\nmylist2 &lt;- list(6:12)\n\nsquare_sum &lt;- function(x, y) {\n  x^2 + y^2\n}\n\nresults &lt;- list()\n\nfor (i in seq_along(mylist1[[1]])) {\n  result &lt;- lapply(X = mylist2, FUN = function(y) square_sum(mylist1[[1]][i], y))\n  results[[i]] &lt;- result\n}\n\nThis example doesn’t work in the way that 0 is paired to 6, 1 is paired to 7, and so on. Instead, every element in mylist1 will be paired with every element in mylist2. Thus, the “unlisted” results from the example will have \\(4*7 = 28\\) elements.\nWe can use flatten() or unlist() functions to decrease the depths of our results. If the results are data frames, then we will need to use bind_rows() to combine them.\n\n\n6.10.3 Parallel processing with parallel and future packages\nOne big drawback of lapply() is its long computation time, especially when the list length is long. Fortunately, computers nowadays must have multiple cores which makes parallel processing possible to help make computation much faster.\nAssume you have a list called mylist of length 1000, and lapply(X = mylist, FUN = func) applies the function to each of the 1000 elements one by one in \\(T\\) seconds. If we could execute the func in \\(n\\) processors simultaneously, then ideally, we would shrink the computation time to \\(T/n\\) seconds.\nIn practice, using functions under the parallel and the future packages, we can split mylist into smaller chunks and apply the function to each element of the several chunks in parallel in different cores to significantly reduce the run time.\n\n6.10.3.1 parLapply()\nBelow is a generic example of parLapply():\nlibrary(parallel)\n\n# Set how many processors will be used to process the list and make cluster\nn_cores &lt;- 4\ncl &lt;- makeCluster(n_cores)\n\n#Use parLapply() to apply func to each element in mylist\nresult &lt;- parLapply(cl = cl, x = mylist, FUN = func)\n\n#Stop the parallel processing\nstopCluster(cl)\nLet’s still assume mylist is of length 1000. The parLapply above splits mylist into 4 sub-lists each of length 250 and applies the function to the elements of each sub-list in parallel. To be more specific, first apply the function to element 1, 251, 501, 751; second apply to element 2, 252, 502, 752; so on and so forth. As such, the computation time will be greatly reduced.\nYou can use parallel::detectCores() to test how many cores your machine has and to help decide what to put for n_cores. It would be a good idea to leave at least one core free for the operating system to use.\nWe will always start parLapply() with makeCluster(). stopCluster() is not fully necessary but follows the best practices. If not stopped, the processing will continue in the back end and consuming the computation capacity for other software in your machine. But keep in mind that stopping the cluster is similar quitting R, meaning that you will need to re-load the packages needed when you need to do parallel processing use parLapply() again.\n\n\n6.10.3.2 future.lapply()\nBelow is a generic example of future.lapply():\nlibrary(future)\nlibrary(future.apply)\n\n# First, plan how the future_lapply() will be resolved\nfuture::plan(\n  multisession, workers = future::availableCores() - 1\n)\n\n# Use future_lapply() to apply func to each element in mylist\nfuture_lapply(x = mylist, FUN = func)\nHere, future::availableCores() checks how many cores your machine has. Similar to parLapply() showed above, future_lapply() parallelizes the computation of lapply() by executing the function func simultaneously on different sub-lists of mylist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "05-coding-practices.html#reviewing-code",
    "href": "05-coding-practices.html#reviewing-code",
    "title": "6  Coding practices",
    "section": "6.11 Reviewing Code",
    "text": "6.11 Reviewing Code\nBefore publishing new changes, it is important to ensure that the code has been tested and well-documented. GitHub makes it possible to document all of these changes in a pull request. Pull requests can be used to describe changes in a branch that are ready to be merged with the base branch (more information in the GitHub section). Github allows users to create a pull request template in a repository to standardize and customize the information in a pull request. When you add a pull request template to your repository, everyone will automatically see the template’s contents in the pull request body.\n\n6.11.1 Creating a Pull Request Template\nFollow the instructions below to add a pull request template to a repository. More details can be found at this GitHub link.\n\nOn GitHub, navigate to the main page of the repository.\nAbove the file list, click Create new file.\nName the file pull_request_template.md. GitHub will not recognize this as the template if it is named anything else. The file must be on the master branch.\n\nTo store the file in a hidden directory instead of the main directory, name the file .github/pull_request_template.md.\n\nIn the body of the new file, add your pull request template. This could include:\n\nA summary of the changes proposed in the pull request\nHow the change has been tested\n@mentions of the person or team responsible for reviewing proposed changes\n\n\nHere is an example pull request template.\n# Description\n\n## Summary of change\n\nPlease include a summary of the change, including any new functions added and example usage. \n\n## Link to Spec\n\nPlease include a link to the Trello card or Google document with details of the task. \n\n## Who should review the pull request?\n@ ...",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Coding practices</span>"
    ]
  },
  {
    "objectID": "06-coding-style.html",
    "href": "06-coding-style.html",
    "title": "7  Coding style",
    "section": "",
    "text": "7.1 Comments\nby Kunal Mishra, Jade Benjamin-Chung, and Stephanie Djajadi\n# This is a comment -- first letter is capitalized and spaced away from the pound sign",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Coding style</span>"
    ]
  },
  {
    "objectID": "06-coding-style.html#comments",
    "href": "06-coding-style.html#comments",
    "title": "7  Coding style",
    "section": "",
    "text": "File Headers - Every file in a project should have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. You may optionally wish to include the inputs and outputs of the script as well, though the next section makes this significantly less necessary.\n\n################################################################################\n# @Organization - Example Organization\n# @Project - Example Project\n# @Description - This file is responsible for [...]\n################################################################################\n\nFile Structure - Just as your data “flows” through your project, data should flow naturally through a script. Very generally, you want to 1) source your config =&gt; 2) load all your data =&gt; 3) do all your analysis/computation =&gt; save your data. Each of these sections should be “chunked together” using comments. See this file for a good example of how to cleanly organize a file in a way that follows this “flow” and functionally separate pieces of code that are doing different things.\n\nNote: If your computer isn’t able to handle this workflow due to RAM or requirements, modifying the ordering of your code to accomodate it won’t be ultimately helpful and your code will be fragile, not to mention less readable and messy. You need to look into high-performance computing (HPC) resources in this case.\n\nSingle-Line Comments - Commenting your code is an important part of reproducibility and helps document your code for the future. When things change or break, you’ll be thankful for comments. There’s no need to comment excessively or unnecessarily, but a comment describing what a large or complex chunk of code does is always helpful. See this file for an example of how to comment your code and notice that comments are always in the form of:\n\n\n\nMulti-Line Comments - Occasionally, multi-line comments are necessary. Don’t add line breaks manually to a single-line comment for the purpose of making it “fit” on the screen. Instead, in RStudio &gt; Tools &gt; Global Options &gt; Code &gt; “Soft-wrap R source files” to have lines wrap around. Format your multi-line comments like the file header from above.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Coding style</span>"
    ]
  },
  {
    "objectID": "06-coding-style.html#line-breaks",
    "href": "06-coding-style.html#line-breaks",
    "title": "7  Coding style",
    "section": "7.2 Line breaks",
    "text": "7.2 Line breaks\n\nFor ggplot calls and dplyr pipelines, do not crowd single lines. Here are some nontrivial examples of “beautiful” pipelines, where beauty is defined by coherence:\n# Example 1\nschool_names = list(\n  OUSD_school_names = absentee_all %&gt;%\n    filter(dist.n == 1) %&gt;%\n    pull(school) %&gt;%\n    unique %&gt;%\n    sort,\n\n  WCCSD_school_names = absentee_all %&gt;%\n    filter(dist.n == 0) %&gt;%\n    pull(school) %&gt;%\n    unique %&gt;%\n    sort\n)\n# Example 2\nabsentee_all = fread(file = raw_data_path) %&gt;%\n  mutate(program = case_when(schoolyr %in% pre_program_schoolyrs ~ 0,\n                             schoolyr %in% program_schoolyrs ~ 1)) %&gt;%\n  mutate(period = case_when(schoolyr %in% pre_program_schoolyrs ~ 0,\n                            schoolyr %in% LAIV_schoolyrs ~ 1,\n                            schoolyr %in% IIV_schoolyrs ~ 2)) %&gt;%\n  filter(schoolyr != \"2017-18\")\nAnd of a complex ggplot call:\n# Example 3\nggplot(data=data,\n       mapping=aes_string(x=\"year\", y=\"rd\", group=group)) +\n\n  geom_point(mapping=aes_string(col=group, shape=group),\n             position=position_dodge(width=0.2),\n             size=2.5) +\n\n  geom_errorbar(mapping=aes_string(ymin=\"lb\", ymax=\"ub\", col=group),\n                position=position_dodge(width=0.2),\n                width=0.2) +\n\n  geom_point(position=position_dodge(width=0.2),\n             size=2.5) +\n\n  geom_errorbar(mapping=aes(ymin=lb, ymax=ub),\n                position=position_dodge(width=0.2),\n                width=0.1) +\n\n  scale_y_continuous(limits=limits,\n                     breaks=breaks,\n                     labels=breaks) +\n\n  scale_color_manual(std_legend_title,values=cols,labels=legend_label) +\n  scale_shape_manual(std_legend_title,values=shapes, labels=legend_label) +\n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  xlab(\"Program year\") +\n  ylab(yaxis_lab) +\n  theme_complete_bw() +\n  theme(strip.text.x = element_text(size = 14),\n        axis.text.x = element_text(size = 12)) +\n  ggtitle(title)\nImagine (or perhaps mournfully recall) the mess that can occur when you don’t strictly style a complicated ggplot call. Trying to fix bugs and ensure your code is working can be a nightmare. Now imagine trying to do it with the same code 6 months after you’ve written it. Invest the time now and reap the rewards as the code practically explains itself, line by line.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Coding style</span>"
    ]
  },
  {
    "objectID": "06-coding-style.html#automated-tools-for-style-and-project-workflow",
    "href": "06-coding-style.html#automated-tools-for-style-and-project-workflow",
    "title": "7  Coding style",
    "section": "7.3 Automated Tools for Style and Project Workflow",
    "text": "7.3 Automated Tools for Style and Project Workflow\n\n7.3.1 Styling\n\nCode Autoformatting - RStudio includes a fantastic built-in utility (keyboard shortcut: CMD-Shift-A) for autoformatting highlighted chunks of code to fit many of the best practices listed here. It generally makes code more readable and fixes a lot of the small things you may not feel like fixing yourself. Try it out as a “first pass” on some code of yours that doesn’t follow many of these best practices!\nAssignment Aligner - A cool R package allows you to very powerfully format large chunks of assignment code to be much cleaner and much more readable. Follow the linked instructions and create a keyboard shortcut of your choosing (recommendation: CMD-Shift-Z). Here is an example of how assignment aligning can dramatically improve code readability:\n\n# Before\nOUSD_not_found_aliases = list(\n  \"Brookfield Village Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Brookfield\"),\n  \"Carl Munck Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Munck\"),\n  \"Community United Elementary School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Community United\"),\n  \"East Oakland PRIDE Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"East Oakland Pride\"),\n  \"EnCompass Academy\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"EnCompass\"),\n  \"Global Family School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Global\"),\n  \"International Community School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"International Community\"),\n  \"Madison Park Lower Campus\" = \"Madison Park Academy TK-5\",\n  \"Manzanita Community School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Manzanita Community\"),\n  \"Martin Luther King Jr Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"King\"),\n  \"PLACE @ Prescott\" = \"Preparatory Literary Academy of Cultural Excellence\",\n  \"RISE Community School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Rise Community\")\n)\n# After\nOUSD_not_found_aliases = list(\n  \"Brookfield Village Elementary\"      = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Brookfield\"),\n  \"Carl Munck Elementary\"              = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Munck\"),\n  \"Community United Elementary School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Community United\"),\n  \"East Oakland PRIDE Elementary\"      = str_subset(string = OUSD_school_shapes$schnam, pattern = \"East Oakland Pride\"),\n  \"EnCompass Academy\"                  = str_subset(string = OUSD_school_shapes$schnam, pattern = \"EnCompass\"),\n  \"Global Family School\"               = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Global\"),\n  \"International Community School\"     = str_subset(string = OUSD_school_shapes$schnam, pattern = \"International Community\"),\n  \"Madison Park Lower Campus\"          = \"Madison Park Academy TK-5\",\n  \"Manzanita Community School\"         = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Manzanita Community\"),\n  \"Martin Luther King Jr Elementary\"   = str_subset(string = OUSD_school_shapes$schnam, pattern = \"King\"),\n  \"PLACE @ Prescott\"                   = \"Preparatory Literary Academy of Cultural Excellence\",\n  \"RISE Community School\"              = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Rise Community\")\n)\n\nStyleR - Another cool R package from the Tidyverse that can be powerful and used as a first pass on entire projects that need refactoring. The most useful function of the package is the style_dir function, which will style all files within a given directory. See the function’s documentation and the vignette linked above for more details.\n\nNote: The default Tidyverse styler is subtly different from some of the things we’ve advocated for in this document. Most notably we differ with regards to the assignment operator (&lt;- vs =) and number of spaces before/after “tokens” (i.e. Assignment Aligner add spaces before = signs to align them properly). For this reason, we’d recommend the following: style_dir(path = ..., scope = \"line_breaks\", strict = FALSE). You can also customize StyleR even more if you’re really hardcore.\nNote: As is mentioned in the package vignette linked above, StyleR modifies things in-place, meaning it overwrites your existing code and replaces it with the updated, properly styled code. This makes it a good fit on projects with version control, but if you don’t have backups or a good way to revert back to the intial code, I wouldn’t recommend going this route.\n\nLinter - Linters are programming tools that check adherence to a given style, syntax errors, and possible semantic issues. The R linter, called lintr, can be found in this package. It helps keep files consistent across different authors and even different organizations. For example, it notifies you if you have unused variables, global variables with no visible binding, not enough or superflous whitespace, and improper use of parentheses or brackets. A list of its other purposes can be found in this link, and most guidelines are based on Hadley Wickham’s R Style Guide.\n\nNote: You can customize your settings to set defaults or to exclude files. More details can be found here.\nNote: The lintr package goes hand in hand with the styler package. The styler can be used to automatically fix the problems that the lintr catches.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Coding style</span>"
    ]
  },
  {
    "objectID": "07-big-data.html",
    "href": "07-big-data.html",
    "title": "8  Working with Big Data",
    "section": "",
    "text": "8.1 The data.table package\nby Kunal Mishra and Jade Benjamin-Chung\nIt may also be the case that you’re working with very large datasets. Generally I would define this as 10+ million rows. As is outlined in this document, the 3 main players in the data analysis space are Base R, Tidvyerse (more specificially, dplyr), and data.table. For a majority of things, Base R is inferior to both dplyr and data.table, with concise but less clear syntax and less speed. Dplyr is architected for medium and smaller data, and while its very fast for everyday usage, it trades off maximum performance for ease of use and syntax compared to data.table. An overview of the dplyr vs data.table debate can be found in this stackoverflow post and all 3 answers are worth a read.\nYou can also achieve a performance boost by running dplyr commands on data.tables, which I find to be the best of both worlds, given that a data.table is a special type of data.frame and fairly easy to convert with the as.data.table() function. The speedup is due to dplyr’s use of the data.table backend and in the future this coupling should become even more natural.\nIf you want to test whether using a certain coding approach increases speed, consider the tictoc package. Run tic() before a code chunk and toc() after to measure the amount of system time it takes to run the chunk. For example, you might use this to decide if you really need to switch a code chunk from dplyr to data.table.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "07-big-data.html#using-downsampled-data",
    "href": "07-big-data.html#using-downsampled-data",
    "title": "8  Working with Big Data",
    "section": "8.2 Using downsampled data",
    "text": "8.2 Using downsampled data\nIn our studies with very large datasets, we save “downsampled” data that usually includes a 1% random sample stratified by any important variables, such as year or household id. This allows us to efficiently write and test our code without having to load in large, slow datasets that can cause RStudio to freeze. Be very careful to be sure which dataset you are working with and to label results output accordingly.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "07-big-data.html#optimal-rstudio-set-up",
    "href": "07-big-data.html#optimal-rstudio-set-up",
    "title": "8  Working with Big Data",
    "section": "8.3 Optimal RStudio set up",
    "text": "8.3 Optimal RStudio set up\nUsing the following settings will help ensure a smooth experience when working with big data. In RStudio, go to the “Tools” menu, then select “Global Options”. Under “General”:\nWorkspace\n\nUncheck Restore RData into workspace at startup\nSave workspace to RData on exit – choose never\n\nHistory\n\nUncheck Always save history\n\nUnfortunately RStudio often gets slow and/or freezes after hours working with big datasets. Sometimes it is much more efficient to just use Terminal / gitbash to run code and make updates in git.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with Big Data</span>"
    ]
  },
  {
    "objectID": "08-data-masking.html",
    "href": "08-data-masking.html",
    "title": "9  Data masking",
    "section": "",
    "text": "9.1 General Overview\nby Anna Nguyen, Jade Benjamin-Chung, and Gabby Barratt Heitmann\nWhen you need to run a script that requires a large amount of RAM, large files, or that uses parallelization, you can use Sherlock, Stanford’s computing cluster. Sherlock uses Slurm, an open source, scalable cluster management and job scheduling system for computing clusters. Jade can email Sherlock managers to get you an account. Please refer to the Sherlock user guide to learn about the system and how to use it. Below, we include a few tips specific to how we use Sherlock in our lab.\nThis chapter covers data masking, a unique process in R in which columns are treated as distinct objects within their dataframe’s environment. In our lab, data masking most frequently comes up when writing wrapper functions where arguments to indicate column names are supplied as strings. We often do this when we repeat the same code on multiple columns, and want to apply a function to a vector of strings that correspond to column names in a dataframe. For example, we might want to clean multiple columns using the same function or estimate the same model under different feature sets. Here, we try to break down what data masking is, why this error comes up, and common approaches to solve this problem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data masking</span>"
    ]
  },
  {
    "objectID": "08-data-masking.html#general-overview",
    "href": "08-data-masking.html#general-overview",
    "title": "9  Data masking",
    "section": "",
    "text": "9.1.1 What is Data Masking?\nWithin certain tidyverse operations, columns are called as if they were variables. For example, while running df %&gt;% mutate(X = …) R recognizes that X specifically references a column in df without explicitly stating its membership df %&gt;% mutate(df$X = …) or calling the column name as a string df %&gt;% mutate(“X” = …).\n\nHowever, this behavior may introduce errors when we attempt to incorporate variables from the global environment within these tidyverse pipelines. In the example above, column_name = “X” followed by df %&gt;% mutate(X2 = column_name + 1) would yield an error, since column_name is not a column in df and the variable column_name is not defined within the environment of df\n\n\n9.1.2 Using tidy evaluation for data masking\nIn dplyr-based R programming, we make use of tidy evaluation. This allows us to avoid using base R syntax to reference specific columns in a data frame. By leveraging Tidy evaluation-based data masking, we can employ long pipes with several dplyr verbs to manipulate our data using stand-alone variables that store column names as strings.\nFor example, consider a data frame “df” that contains a column called “heavyrain” that we want to manipulate. Suppose we wanted to convert the values of “heavyrain” into a factor.\nUsing base R, which does not mask data, heavyrain must have quotes to be treated as a data-variable:\ndf[[“outcome”]] = as.factor(df[[“heavyrain”]])\nIn a dplyr pipe, heavyrain is being masked using tidy evaluation and will be correctly interpreted as a column because it is recognized as a data-variable: df %&gt;% mutate(outcome = as.factor(heavyrain))\nWith modified data masking, heavyrain is a string that is coerced into being recognized as a data-variable:\nvar_name = “heavyrain”\ndf %&gt;% mutate(outcome = as.factor(!!sym(var_name)) \nWhile cleaner and often more convenient, the data frame that var_name is in is now “masked” and we refer to the vectors in the dataframe (data-variables) as though it is an object of its own (an environmental-variable). This is why we can just say the variable’s name in the context of a pipe – we treat it as though it’s an object defined in our environment. Within normal scripts, this is usually fine, because the data frame is “held on to” in the pipe. However, it can cause some programming hurdles when writing functions that take strings of variable/column names as arguments. In the next section, we briefly describe how to troubleshoot common errors in data masking, as relevant to our lab’s work.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data masking</span>"
    ]
  },
  {
    "objectID": "08-data-masking.html#technical-overview",
    "href": "08-data-masking.html#technical-overview",
    "title": "9  Data masking",
    "section": "9.2 Technical Overview",
    "text": "9.2 Technical Overview\nThis section covers the R functions and tools that we often use in the context of data masking, focusing on the bang bang operator (!!) with symbol coercion (sym()) and the Walrus operator (:=).\nThe combined use of !! and sym() allows us to use strings, rather than data-variables, to reference column names within dplyr. Together, !!sym(“column_name”) forces dplyr to recognize “column_name” as a data-variable prior to evaluating the rest of the expression, enabling the ability to perform calculations on the column while referring to it as a string. sym() is a function that turns strings into symbols. In the context of a dplyr pipe, these symbols are interpreted as data-variables. The !! (bang bang) operator tells dplyr to evaluate the sym() expression first, e.g. to unquote its expression (e.g. “column_name”) and evaluate it as a pre-existing object, first. This is helpful because often we use sym(“column_name”) within a larger expression, and dplyr might evaluate other elements of the expression first without !!, causing errors.\nWhen we want to create a new column (via mutate or summarize), the Walrus operator (:=) allows us to specify the new column’s name using a string. For example, while df %&gt;% mutate(“new_column” = values) would yield an error, df %&gt;% mutate(“new_column” := values) will correctly create a new column called “new_column”. If we want to use a variable representing a string, we can use !! to force the variable to be evaluated before using := to assign the value of the new column.\ncol_name = “new_column”\ndf %&gt;% mutate(!!col_name := values)\n\n9.2.1 Example\nSuppose we want to write a function “generate_descriptive_table” to summarize how the prevalence of “outcome” varies under different levels of a “risk_factor” in a data frame “df”\nWe can start by writing the function shell:\ngenerate_descriptive_table &lt;- function (df, outcome, rf) {\noutcome_dist_by_rf &lt;- ….\nreturn(outcome_dist_by_rf)\n}\nNext, we can filter the data frame for only rows in which “rf” and “outcome” are not missing. We can use !! and sym() within filter to evaluate the strings stored in “rf” and “outcome”. Note that defining !!sym(outcome) or !!sym(outcome) in variables outside of the dplyr pipeline will not work.\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df %&gt;% \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) %&gt;%\n  ….\n  return(outcome_dist_by_rf)\n}\nSimilarly, we use !! and sym() in group_by to evaluate column name, stored as a string in the argument “rf”\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df %&gt;% \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) %&gt;%\n  ….\n  return(outcome_dist_by_rf)\n}\nFinally, we can use the walrus operator, !! and sym() with “summarize” to create a new column that takes the mean of the column referenced in “rf”. We also use “glue” or “paste” to give the new column an informative name that includes the “outcome” it describes.\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df %&gt;% \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) %&gt;% \n  group_by(!!sym(rf)) %&gt;%\n  summarize(!!(glue::glue(“{outcome}_prev”)) := mean(!!sym(outcome))) \n  return(outcome_dist_by_rf)\n}\nOR\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df %&gt;% \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) %&gt;% \n  group_by(!!sym(rf)) %&gt;%\n  summarize(!!(paste0(outcome, ”_prev”)) := mean(!!sym(outcome)))\n  return(outcome_dist_by_rf)\n}\nOR\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  new_column_name = paste0(outcome, ”_prev”)\n  outcome_dist_by_rf &lt;- df %&gt;% \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) %&gt;% \n  group_by(!!sym(rf)) %&gt;%\n  summarize(!!(new_column_name) := mean(!!sym(outcome))) \n  return(outcome_dist_by_rf)\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data masking</span>"
    ]
  },
  {
    "objectID": "09-github.html",
    "href": "09-github.html",
    "title": "10  Github",
    "section": "",
    "text": "10.1 Basics\nby Stephanie Djajadi and Nolan Pokpongkiat",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#basics",
    "href": "09-github.html#basics",
    "title": "10  Github",
    "section": "",
    "text": "A detailed tutorial of Git can be found here on the CS61B website.\nIf you are already familiar with Git, you can reference the summary at the end of Section B.\nIf you have made a mistake in Git, you can refer to this article to undo, fix, or remove commits in git.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#github-desktop",
    "href": "09-github.html#github-desktop",
    "title": "10  Github",
    "section": "10.2 Github Desktop",
    "text": "10.2 Github Desktop\nWhile knowing how to use Git on the command line will always be useful since the full power of Git and its customizations and flexibilty is designed for use with the command line, Github also provides Github Desktop as an graphical interface to do basic git commands; you can do all of the basic functions of Git using this desktop app. Feel free to use this as an alternative to Git on the command line if you prefer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#git-branching",
    "href": "09-github.html#git-branching",
    "title": "10  Github",
    "section": "10.3 Git Branching",
    "text": "10.3 Git Branching\nBranches allow you to keep track of multiple versions of your work simultaneously, and you can easily switch between versions and merge branches together once you’ve finished working on a section and want it to join the rest of your code. Here are some cases when it may be a good idea to branch:\n\nYou may want to make a dramatic change to your existing code (called refactoring) but it will break other parts of your project. But you want to be able to simultaneously work on other parts or you are collaborating with others, and you don’t want to break the code for them.\nYou want to start working on a new part of the project, but you aren’t sure yet if your changes will work and make it to the final product.\nYou are working with others and don’t want to mix up your current work with theirs, even if you want to bring your work together later in the future.\n\nA detailed tutorial on Git Branching can be found here. You can also find instructions on how to handle merge conflicts when joining branches together.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#example-workflow",
    "href": "09-github.html#example-workflow",
    "title": "10  Github",
    "section": "10.4 Example Workflow",
    "text": "10.4 Example Workflow\nA standard workflow when starting on a new project and contributing code looks like this:\n\n\n\nCommand\nDescription\n\n\n\n\nSETUP: FIRST TIME ONLY: git clone &lt;url&gt; &lt;directory_name&gt;\nClone the repo. This copies of all the project files in its current state on Github to your local computer.\n\n\n1. git pull origin master\nupdate the state of your files to match the most current version on GitHub\n\n\n2. git checkout -b &lt;new_branch_name&gt;\ncreate new branch that you’ll be working on and go to it\n\n\n3. Make some file changes\nwork on your feature/implementation\n\n\n4. git add -p\nadd changes to stage for commit, going through changes line by line\n\n\n5. git commit -m &lt;commit message&gt;\ncommit files with a message\n\n\n6. git push -u origin &lt;branch_name&gt;\npush branch to remote and set to track (-u only needed if this is first push)\n\n\n7. Repeat step 4-5.\nwork and commit often\n\n\n8. git push\npush work to remote branch for others to view\n\n\n9. Follow the link given from the git push command to submit a pull request (PR) on GitHub online\nPR merges in work from your branch into master\n\n\n(10.) Your changes and PR get approved, your reviewer deletes your remote branch upon merging\n\n\n\n11. git fetch --all --prune\nclean up your local git by untracking deleted remote branches\n\n\n\nOther helpful commands are listed below.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#commonly-used-git-commands",
    "href": "09-github.html#commonly-used-git-commands",
    "title": "10  Github",
    "section": "10.5 Commonly Used Git Commands",
    "text": "10.5 Commonly Used Git Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ngit clone &lt;url&gt; &lt;directory_name&gt;\nclone a repository, only needs to be done the first time\n\n\ngit pull origin master\npull from master before making any changes\n\n\ngit branch\ncheck what branch you are on\n\n\ngit branch -a\ncheck what branch you are on + all remote branches\n\n\ngit checkout -b &lt;new_branch_name&gt;\ncreate new branch and go to it (only necessary when you create a new branch)\n\n\ngit checkout &lt;branch name&gt;\nswitch to branch\n\n\ngit add &lt;file name&gt;\nadd file to stage for commit\n\n\ngit add -p\nadds changes to commit, showing you changes one by one\n\n\ngit commit -m &lt;commit message&gt;\ncommit file with a message\n\n\ngit push -u origin &lt;branch_name&gt;\npush branch to remote and set to track (-u only works if this is first push)\n\n\ngit branch --set-upstream-to origin &lt;branch_name&gt;\nset upstream to origin/&lt;branch_name&gt; (use if you forgot -u on first push)\n\n\ngit push origin &lt;branch_name&gt;\npush work to branch\n\n\ngit checkout &lt;branch_name&gt;  git merge master\nswitch to branch and merge changes from master into &lt;branch_name&gt; (two commands)\n\n\ngit merge &lt;branch_name&gt; master\nswitch to branch and merge changes from master into &lt;branch_name&gt; (one command)\n\n\ngit checkout --track origin/&lt;branch_name&gt;\npulls a remote branch and creates a local branch to track it (use when trying to pull someone else’s branch onto your local computer)\n\n\ngit push --delete &lt;remote_name&gt; &lt;branch_name&gt;\ndelete remote branch\n\n\ngit branch -d &lt;branch_name&gt;\ndeletes local branch, -D to force\n\n\ngit fetch --all --prune\nuntrack deleted remote branches",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#how-often-should-i-commit",
    "href": "09-github.html#how-often-should-i-commit",
    "title": "10  Github",
    "section": "10.6 How often should I commit?",
    "text": "10.6 How often should I commit?\nIt is good practice to commit every 15 minutes, or every time you make a significant change. It is better to commit more rather than less.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "09-github.html#what-should-be-pushed-to-github",
    "href": "09-github.html#what-should-be-pushed-to-github",
    "title": "10  Github",
    "section": "10.7 What should be pushed to Github?",
    "text": "10.7 What should be pushed to Github?\nNever push .Rout files! If someone else runs an R script and creates an .Rout file at the same time and both of you try to push to github, it is incredibly difficult to reconcile these two logs. If you run logs, keep them on your own system or (preferably) set up a shared directory where all logs are name and date timestamped.\nThere is a standardized .gitignore for R which you can download and add to your project. This ensures you’re not committing log files or things that would otherwise best be left ignored to GitHub. This is a great discussion of project-oriented workflows, extolling the virtues of a self-contained, portable projects, for your reference.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "10-unix.html",
    "href": "10-unix.html",
    "title": "11  Unix commands",
    "section": "",
    "text": "11.1 Basics\nby Stephanie Djajadi, Kunal Mishra, Anna Nguyen, and Jade Benjamin-Chung\nWe typically use Unix commands in Terminal (for Mac users) or Git Bash (for Windows users) to\nOn the computer, there is a desktop with two folders, folder1 and folder2, and a file called file1. Inside folder1, we have a file called file2. Mac users can run these commands on their terminal; it is recommended that Windows users use Git Bash, not Windows PowerShell.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "10-unix.html#basics",
    "href": "10-unix.html#basics",
    "title": "11  Unix commands",
    "section": "",
    "text": "Here is our example desktop.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "10-unix.html#syntax-for-both-macwindows",
    "href": "10-unix.html#syntax-for-both-macwindows",
    "title": "11  Unix commands",
    "section": "11.2 Syntax for both Mac/Windows",
    "text": "11.2 Syntax for both Mac/Windows\nWhen typing in directories or file names, quotes are necessary if the name includes spaces.\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ncd desktop/folder1\nChange directory to folder1\n\n\npwd\nPrint working directory\n\n\nls\nList files in the directory\n\n\ncp \"file2\" \"newfile2\"\nCopy file (remember to include file extensions when typing in file names like .pdf or .R)\n\n\nmv “newfile2” “file3”\nRename newfile2 to file3\n\n\ncd ..\nGo to parent of the working directory (in this case, desktop)\n\n\nmv “file1” folder2\nMove file1 to folder2\n\n\nmkdir folder3\nMake a new folder in folder2\n\n\nrm &lt;filename&gt;\nRemove files\n\n\nrm -rf folder3\nRemove directories (-r will attempt to remove the directory recursively, -rf will force removal of the directory)\n\n\nclear\nClear terminal screen of all previous commands\n\n\n\n\n\n\nHere is an example of what your terminal might look like after executing the commands in the order listed above.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "10-unix.html#running-bash-scripts",
    "href": "10-unix.html#running-bash-scripts",
    "title": "11  Unix commands",
    "section": "11.3 Running Bash Scripts",
    "text": "11.3 Running Bash Scripts\n\n\n\n\n\n\n\n\nWindows\nMac / Linux\nDescription\n\n\n\n\nchmod +750 &lt;filename.sh&gt;\nchmod +x &lt;filename.sh&gt;\nChange access permissions for a file (only needs to be done once)\n\n\n./&lt;filename.sh&gt;\n./&lt;filename.sh&gt;\nRun file (./ to run any executable file)\n\n\nbash bash_script_name.sh &\nbash bash_script_name.sh &\nRun shell script in the background",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "10-unix.html#running-rscripts-in-windows",
    "href": "10-unix.html#running-rscripts-in-windows",
    "title": "11  Unix commands",
    "section": "11.4 Running Rscripts in Windows",
    "text": "11.4 Running Rscripts in Windows\nNote: This code seems to work only with Windows Command Prompt, not with Git Bash.\nWhen R is installed, it comes with a utility called Rscript. This allows you to run R commands from the command line. If Rscript is in your PATH, then typing Rscript into the command line, and pressing enter, will not error. Otherwise, to use Rscript, you will either need to add it to your PATH (as an environment variable), or append the full directory of the location of Rscript on your machine. To find the full directory, search for where R is installed your computer. For instance, it may be something like below (this will vary depending on what version of R you have installed):\nC:\\Program Files\\R\\R-3.6.0\\bin\nFor appending the PATH variable, please view this link. I strongly recommend completing this option.\nIf you add the PATH as an environment variable, then you can run this line of code to test: Rscript -e “cat(‘this is a test’)\", where the -e flag refers to the expression that will be executed.\nIf you do not add the PATH as an environment variable, then you can run this line of code to replicate the results from above: “C:\\Program Files\\R\\R-3.6.0\\bin” -e “cat(‘this is a test’)”\nTo run an R script from the command line, we can say: Rscript -e “source(‘C:/path/to/script/some_code.R’)”\n\n11.4.1 Common Mistakes\n\nRemember to include all of the quotation marks around file paths that have a spaces.\nIf you attempt to run an R script but run into Error: '\\U' used without hex digits in character string starting \"'C:\\U\", try replacing all \\ with \\\\ or /.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "10-unix.html#checking-tasks-and-killing-jobs",
    "href": "10-unix.html#checking-tasks-and-killing-jobs",
    "title": "11  Unix commands",
    "section": "11.5 Checking tasks and killing jobs",
    "text": "11.5 Checking tasks and killing jobs\n\n\n\n\n\n\n\n\nWindows\nMac / Linux\nDescription\n\n\n\n\ntasklist\nps -v\nList all processes on the command line\n\n\n\ntop -o [cpu/rsize]\nList all running processes, sorted by CPU or memory usage\n\n\ntaskkill /F /PID pid_number\nkill &lt;PID_number&gt;\nKill a process by its process ID\n\n\ntaskkill /IM \"process name\" /F\n\nKill a process by its name\n\n\nstart /b program.exe\n\nRuns jobs in the background (exclude /b if you want the program to run in a new console)\n\n\n\nnohup\nPrevents jobs from stopping\n\n\n\ndisown\nKeeps jobs running in the background even if you close R\n\n\ntaskkill /?\n\nHelp, lists out other commands\n\n\n\nTo kill a task in Windows, you can also go to Task Manager &gt; More details &gt; Select your desired app &gt; Click on End Task.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "10-unix.html#running-big-jobs",
    "href": "10-unix.html#running-big-jobs",
    "title": "11  Unix commands",
    "section": "11.6 Running big jobs",
    "text": "11.6 Running big jobs\nFor big data workflows, the concept of “backgrounding” a bash script allows you to start a “job” (i.e. run the script) and leave it overnight to run. At the top level, a bash script (0-run-project.sh) that simply calls the directory-level bash scripts (i.e. 0-prep-data.sh, 0-run-analysis.sh, 0-run-figures.sh, etc.) is a powerful tool to rerun every script in your project. See the included example bash scripts for more details.\n\nRunning Bash Scripts in Background: Running a long bash script is not trivial. Normally you would run a bash script by opening a terminal and typing something like ./run-project.sh. But what if you leave your computer, log out of your server, or close the terminal? Normally, the bash script will exit and fail to complete. To run it in background, type ./run-project.sh &; disown. You can see the job running (and CPU utilization) with the command top or ps -v and check your memory with free -h.\n\nAlternatively, to keep code running in the background even when an SSH connection is broken, you can use tmux. In terminal or gitbash follow the steps below. This site has useful tips on using tmux.\n# create a new tmux session called session_name\ntmux new -ssession_name\n\n# run your job of interest\nR CMD BATCH myjob.R & \n  \n# check that it is running\nps -v\n\n# to exit the tmux session (Mac)\nctrl + b \nd\n\n# to reopen the tmux session to kill the job or \n# start another job\ntmux attach -tsession_name \n\nDeleting Previously Computed Results: One helpful lesson we’ve learned is that your bash scripts should remove previous results (computed and saved by scripts run at a previous time) so that you never mix results from one run with a previous run. This can happen when an R script errors out before saving its result, and can be difficult to catch because your previously saved result exists (leading you to believe everything ran correctly).\nEnsuring Things Ran Correctly: You should check the .Rout files generated by the R scripts run by your bash scripts for errors once things are run. A utility file is include in this repository, called runFileSaveLogs, and is used by the example bash scripts to… run files and save the generated logs. It is an awesome utility and one I definitely recommend using. Before using runFileSaveLogs, it is necessary to put the file in the home working directory. For help and documentation, you can use the command ./runFileSaveLogs -h. See example code and example usage for runFileSaveLogs below.\n\n\n11.6.1 Example code for runfileSaveLogs\n#!/usr/bin/env python3\n# Type \"./runFileSaveLogs -h\" for help\n\nimport os\nimport sys\nimport argparse\nimport getpass\nimport datetime\nimport shutil\nimport glob\nimport pathlib\n\n# Setting working directory to this script's current directory\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\n# Setting up argument parser\nparser = argparse.ArgumentParser(description='Runs the argument R script(s) - in parallel if specified - and moves the subsequent generated .Rout log files to a timestamped directory.')\n\n# Function ensuring that the file is valid\ndef is_valid_file(parser, arg):\n    if not os.path.exists(arg):\n        parser.error(\"The file %s does not exist!\" % arg)\n    else:\n        return arg\n\n# Function ensuring that the directory is valid\ndef is_valid_directory(parser, arg):\n    if not os.path.isdir(arg):\n        parser.error(\"The specified path (%s) is not a directory!\" % arg)\n    else:\n        return arg\n\n# Additional arguments that can be added when running runFileSaveLogs\nparser.add_argument('-p', '--parallel', action='store_true', help=\"Runs the argument R scripts in parallel if specified\")\nparser.add_argument(\"-i\", \"--identifier\", help=\"Adds an identifier to the directory name where this is saved\")\nparser.add_argument('filenames', nargs='+', type=lambda x: is_valid_file(parser, x))\n\nargs = parser.parse_args()\nargs_dict = vars(args)\n\nprint(args_dict)\n\n# Run given R Scripts\nfor filename in args_dict[\"filenames\"]:\n  system_call = \"R CMD BATCH\" + \" \" + filename\n  if args_dict[\"parallel\"]: \n    system_call = \"nohup\" + \" \" + system_call + \" &\"\n\n  os.system(system_call)\n\n# Create the directory (and any parents) of the log files\ncurrentUser = getpass.getuser()\ncurrentTime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\nlogDirPrefix = \"/home/kaiserData/logs/\" # Change to the directory where the logs should be saved\nlogDir = logDirPrefix + currentTime + \"-\" + currentUser \n\n# If specified, adds the identifier to the filename of the log\nif args.identifier is not None:\n  logDir += \"-\" + args.identifier\n\nlogDir += \"/\"\n\npathlib.Path(logDir).mkdir(parents=True, exist_ok=True)\n\n# Find and move all logs to this new directory\ncurrentLogPaths = glob.glob('./*.Rout')\n\nfor currentLogPath in currentLogPaths:\n  filename = currentLogPath.split(\"/\")[-1]\n  shutil.move(currentLogPath, logDir + filename)\n\n\n11.6.2 Example usage for runfileSaveLogs\nThis example bash script runs files and generates logs for five scripts in the kaiserflu/3-figures folder. Note that the -i flag is used as an identifier to add figures to the filename of each log.\n#!/bin/bash\n\n# Copy utility run script into this folder for concision in call\ncp ~/kaiserflu/runFileSaveLogs ~/kaiserflu/3-figures/\n\n# Run folder scripts and produce output\ncd ~/kaiserflu/3-figures/\n./runFileSaveLogs -i \"figures\" \\\nfig-mean-season-age.R \\\nfig-monthly-rate.R \\\nfig-point-estimates-combined.R \\\nfig-point-estimates.R \\\nfig-weekly-rate.R\n\n# Remove copied utility run script\nrm runFileSaveLogs",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unix commands</span>"
    ]
  },
  {
    "objectID": "11-reproducible-environments.html",
    "href": "11-reproducible-environments.html",
    "title": "12  Reproducible Environments",
    "section": "",
    "text": "12.1 Package Version Control with renv\nby Anna Nguyen",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reproducible Environments</span>"
    ]
  },
  {
    "objectID": "11-reproducible-environments.html#package-version-control-with-renv",
    "href": "11-reproducible-environments.html#package-version-control-with-renv",
    "title": "12  Reproducible Environments",
    "section": "",
    "text": "12.1.1 Introduction\nReplicable code should produce the same results, regardless of when or where it’s run. However, our analyses often leverage open-source R packages that are developed by other teams. These packages continue to be developed after research projects are completed, which may include changes to analysis functions that could impact how code runs for both other team members and external replicators.\nFor example, suppose we had used a function that took in one argument, such that our code contained example_function(arg_a = “a”). A few months after we publish our code, the package developers update the function to take in another mandatory argument arg_b. If someone runs our code, but has the most recent version of the package, they’ll receive an error message that the argument arg_b is missing and will not be able to full reproduce our results.\nTo ensure that the right functions are used in replication efforts, it is important for us to keep track of package versions used in each project.\nrenv can be to promote reproducible environments within R projects. renv creates individual package libraries for each project instead of having all projects, which may use different versions of the same package, share the same package library. However, for projects that use many packages, this process can be memory intensive and increase the time needed for a new users to start running code.\nIn this lab manual chapter, we provide a quick tutorial for integrating renv into research workflows. For more detailed instructions, please refer to the renv package vignette.\n\n\n12.1.2 Implementing renv in projects\nIdeally, renv should be initiated at the start of projects and updated continuously when new packages are introduced in the codebase. However, this process can be initated at any point in a project\nTo add renv to your workflow, follow these steps:\n\nInstall the renv package by running install.packages(“renv”)\nCreate an RProject file and ensure that your working directory is set to the correct folder\nIn the R console, run renv::init() to intiialize renv in your R Project\nThis will create the following files: renv.lock, .Rprofile, renv/settings.json and renv/activate.R. Commit and push these files to GitHub so that they’re accessible to other users.\nAs you write code, update the project’s R library by running renv::snapshot() in the R console\nAdd renv::restore() to the head of your config file, to make sure that all users that run your code are on the same package versions.\n\n\n\n12.1.3 Using projects with renv\nIf you’re starting to work on an ongoing project that already has renv set up, follow these steps to ensure that you’re using the same project versions.\n\nInstall the renv package by running install.packages(“renv”)\nPull the most updated version of the project from GitHub\nOpen the project’s RProject file\nRun renv::restore(). In our lab’s projects, this is often already found at the top of the config file, so you can just run scripts as is.\nThis will pull up a list of the project’s packages that need to be updated for you to be consistent with the project. The console will ask if you want to proceed with updating these packages - type “Y” to continue.\nWait for the correct versions of each package to install/update. This may take some time, depending on how many packages the project uses.\nYour R environment should now be using the same package versions as specified in the renv lock file. You should now be able to replicate the code.\nIf you make edits to the code and introduce new/updated packages, see the section above for instructions on how to make updates.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Reproducible Environments</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html",
    "href": "12-code-publication.html",
    "title": "13  Code Publication",
    "section": "",
    "text": "13.1 Checklist overview\nby Nolan Pokpongkiat",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#checklist-overview",
    "href": "12-code-publication.html#checklist-overview",
    "title": "13  Code Publication",
    "section": "",
    "text": "Fill out file headers\nClean up comments\nDocument functions\nRemove deprecated filepaths\nEnsure project runs via bash\nComplete the README\nClean up feature branches\nCreate Github release",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#fill-out-file-headers",
    "href": "12-code-publication.html#fill-out-file-headers",
    "title": "13  Code Publication",
    "section": "13.2 Fill out file headers",
    "text": "13.2 Fill out file headers\nEvery file in a project should have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. See template here.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#clean-up-comments",
    "href": "12-code-publication.html#clean-up-comments",
    "title": "13  Code Publication",
    "section": "13.3 Clean up comments",
    "text": "13.3 Clean up comments\nMake sure comments in the code are for code documentation purposes only. Do not leave comments to self in the final script files.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#document-functions",
    "href": "12-code-publication.html#document-functions",
    "title": "13  Code Publication",
    "section": "13.4 Document functions",
    "text": "13.4 Document functions\nEvery function you write must include a header to document its purpose, inputs, and outputs. See template for the function documentation here.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#remove-deprecated-filepaths",
    "href": "12-code-publication.html#remove-deprecated-filepaths",
    "title": "13  Code Publication",
    "section": "13.5 Remove deprecated filepaths",
    "text": "13.5 Remove deprecated filepaths\nAll file paths should be defined in 0-config.R, and should be set relative to the project working directory. All absolute file paths from your local computer should be removed, and replaced with a relative path. If a third party were to re-run this analysis, if they need to download data from a separate source and change a filepath in the 0-config.R to match, make sure to specify in the README which line of 0-config.R needs to be substituted.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#ensure-project-runs-via-bash",
    "href": "12-code-publication.html#ensure-project-runs-via-bash",
    "title": "13  Code Publication",
    "section": "13.6 Ensure project runs via bash",
    "text": "13.6 Ensure project runs via bash\nThe project should be configured to be entirely reproducible by running a master bash script, run-project.sh, which should live at the top directory. This bash script can call other bash scripts in subfolders, if necessary. Bash scripts should use the runFileSaveLogs utility script, which is a wrapper around the Rscript command, allowing you to specify where .Rout log files are moved after the R scripts are run.\nSee usage and documentation here.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#complete-the-readme",
    "href": "12-code-publication.html#complete-the-readme",
    "title": "13  Code Publication",
    "section": "13.7 Complete the README",
    "text": "13.7 Complete the README\nA README.md should live at the top directory of the project. This usually includes a Project Overview and a Directory Structure, along with the names of the contributors and the Creative Commons License. See below for a template:\n\nOverview\nTo date, coronavirus testing in the US has been extremely limited. Confirmed COVID-19 case counts underestimate the total number of infections in the population. We estimated the total COVID-19 infections – both symptomatic and asymptomatic – in the US in March 2020. We used a semi-Bayesian approach to correct for bias due to incomplete testing and imperfect test performance.\nDirectory structure\n\n0-config.R: configuration file that sets data directories, sources base functions, and loads required libraries\n0-base-functions: folder containing scripts with functions used in the analysis\n\n0-base-functions.R: R script containing general functions used across the analysis\n0-bias-corr-functions.R: R script containing functions used in bias correction\n0-bias-corr-functions-undertesting.R: R script containing functions used in bias correction to estimate the percentage of underestimation due to incomplete testing vs. imperfect test accuracy\n0-prior-functions.R: R script containing functions to generate priors\n\n1-data: folder containing data processing scripts NOTE: some scripts are deprecated\n2-analysis: folder containing analysis scripts. To rerun all scripts in this subdirectory, run the bash script 0-run-analysis.sh.\n\n1-obtain-priors-state.R: obtain priors for each state\n2-est-expected-cases-state.R: estimate expected cases in each state\n3-est-expected-cases-state-perf-testing.R: estimate expected cases in each state, estimate the percentage of underestimation due to incomplete testing vs. imperfect test accuracy\n4-obtain-testing-protocols.R: find testing protocols for each state.\n5-summarize-results.R: summarize results; obtain results for in text numerical results.\n\n3-figure-table-scripts: folder containing figure scripts. To rerun all scripts in this subdirectory, run the bash script 0-run-figs.sh.\n\n1-fig-testing.R: creates plot of testing patterns by state over time\n2-fig-cases-usa-state-bar.R: creates bar plot of confirmed vs. estimated infections by state\n3a-fig-map-usa-state.R: creates map of confirmed vs. estimated infections by state\n3b-fig-map-usa-state-shiny.R: creates map of confirmed vs. estimated infections by state with search functionality by state\n4-fig-priors.R: creates figure with priors for US as a whole\n5-fig-density-usa.R: creates figure of distribution of estimated cases in the US\n6-table-data-quality.R: creates table of data quality grading from COVID Tracking Project\n7-fig-testpos.R: creates figure of the probability of testing positive among those tested by state\n8-fig-percent-undertesting-state.R: creates figure of the percentage of under estimation due to incomplete testing\n\n4-figures: folder containing figure files.\n5-results: folder containing analysis results objects.\n6-sensitivity: folder containing scripts to run the sensitivity analyses\n\nContributors: Jade Benjamin-Chung, Sean L. Wu, Anna Nguyen, Stephanie Djajadi, Nolan N. Pokpongkiat, Anmol Seth, Andrew Mertens\nWu SL, Mertens A, Crider YS, Nguyen A, Pokpongkiat NN, Djajadi S, et al. Substantial underestimation of SARS-CoV-2 infection in the United States due to incomplete testing and imperfect test accuracy. medRxiv. 2020; 2020.05.12.20091744. doi:10.1101/2020.05.12.20091744\n\nWhen possible, also include a description of the RDS results that are generated, detailing what data sources were used, where the script lives that creates it, and what information the RDS results hold.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#clean-up-feature-branches",
    "href": "12-code-publication.html#clean-up-feature-branches",
    "title": "13  Code Publication",
    "section": "13.8 Clean up feature branches",
    "text": "13.8 Clean up feature branches\nIn the remote repository on Github, all feature branches aside from master should be merged in and deleted. All outstanding PRs should be closed.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "12-code-publication.html#create-github-release",
    "href": "12-code-publication.html#create-github-release",
    "title": "13  Code Publication",
    "section": "13.9 Create Github release",
    "text": "13.9 Create Github release\nOnce all of these items are verified, create a tag to make a Github release, which will tag the repository, creating a marker at this specific point in time.\nDetailed instructions here.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html",
    "href": "13-data-publication.html",
    "title": "14  Data Publication",
    "section": "",
    "text": "14.1 Overview\nAdapted from Fanice Nyatigo and Ben Arnold’s chapter in the Proctor-UCSF Lab Manual\nIf you are releasing data into the public domain, then consider making available at minimum a .csv file and a codebook of the same name (note: you should have a codebook for internal data as well). We often also make available .rds files as well. For example, your mystudy/data/public directory could include three files for a single dataset, two with the actual data in .rds and .csv formats, and a third that describes their contents:\nIn general, datasets are usually too big to save on GitHub, but occasionally they are small. Here is an example of where we actually pushed the data directly to GitHub: https://github.com/ben-arnold/enterics-seroepi/tree/master/data .\nIf the data are bigger, then maintaining them under version control in your git repository can be unweildy. Instead, we recommend using another stable repository that has version control, such as the Open Science Framework (osf.io). For example, all of the data from the WASH Benefits trials (led by investigators at Berkeley, icddr,b, IPA-Kenya and others) are all stored through data components nested within in OSF projects: https://osf.io/tprw2/. Another good option is Dryad (datadryad.org) or the (Stanford Digital Repository)[https://sdr.stanford.edu/].\nWe recommend cross-linking public files in GitHub (scripts/notebooks only) and OSF/Dryad/Stanford Digital Repository.\nBelow are the main steps to making data public, after finalizing the analysis datasets and scripts:\n1. Remove Protected Health Information (PHI)\n2. Create public IDs or join already created public IDs to the data\n3. Create an OSF repository and/or Dryad/Stanford Digital Repository\n4. Edit analysis scripts to run using the public datasets and test (optional)\n5. Create a public github page for analysis scripts and link to OSF and/or Dryad/Zenodo\n6. Go live",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#overview",
    "href": "13-data-publication.html#overview",
    "title": "14  Data Publication",
    "section": "",
    "text": "Warning!  NEVER push a dataset into the public domain (e.g., GitHub, OSF) without first checking with Jade to ensure that it is appropriately de-identified and we have approval from the sponsor and/or human subjects review board to do so. For example, we will need to re-code participant IDs (even if they contain no identifying information) before making data public to completely break the link between IDs and identifiable information stored on our servers. \n\n\n\nanalysis_data_public.csv\nanalysis_data_public.rds\nanalysis_data_public_codebook.txt",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#removing-phi",
    "href": "13-data-publication.html#removing-phi",
    "title": "14  Data Publication",
    "section": "14.2 Removing PHI",
    "text": "14.2 Removing PHI\nOnce the data is finalized for analysis, the first step is to strip it of Protected Health Information (PHI), or any other data that could be used to link back to specific participants, such as names, birth dates, or GPS coordinates at the village/neighborhood level or below. PHI includes, but is not limited to:\n\n14.2.1 Personal information\nThese are identifiers that directly point to specific individuals, such as:\n- Names, addresses, photographs, date of birth\n- A combination of age, sex, and geographic location (below population 20,000) is considered identifiable\n\n\n14.2.2 Dates\nAny specific dates (e.g., study visit dates, birth dates, treatment dates) are usually problematic.\n- If a dataset requires high resolution temporal information, coarsen visit or measurement dates to be two variables: year and week of the year (1-52).\n- If a dataset requires age, provide that information without a birth date (typically month resolution is sufficient)\n\n\nCaution! If making changes to the format of dates or ages, make sure your analysis code runs on these modified versions of the data (step 3)! \n\n\n\n\n14.2.3 Geographic information\nDo not include GPS coordinates (longitude, latitude) except in special circumstances where they have been obfuscated/shifted. Reach out to Jade before doing this because it can be complicated.\nDo not include place names or codes (e.g., US Zip Codes) if the place contains &lt;20,000 people. For villages or neighborhoods, code them with uninformative IDs. For sub-districts or districts, names are fine.\nIf an analysis requires GPS locations (e.g., to make a map), then typically we include a disclaimer in the article’s data availability statement that explains we cannot make GPS locations public to protect participant confidentiality. As a middle ground, we typically make our code public that runs on the geo-located data for transparency, even if independent researchers can’t actually run that code (although please be careful to ensure the code itself does not in any way include geographic identifiers).\nFor more examples of what constitutes PHI, please refer to this link: https://cphs.berkeley.edu/hipaa/hipaa18.html",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#create-public-ids",
    "href": "13-data-publication.html#create-public-ids",
    "title": "14  Data Publication",
    "section": "14.3 Create public IDs",
    "text": "14.3 Create public IDs\n\n14.3.1 Rationale\nThe Stanford IRB requires that public datasets not include the original study IDs to identify participants or other units in the study (such as village IDs). The reason is that those IDs are linked in our private datasets to PHI. By creating a new set of public IDs, the public dataset is one step further removed from the potential to link to PHI.\n\n\n14.3.2 A single set of public IDs for each study\nFor each study, it is ideal to create a single set of public IDs whenever possible. We could create a new set of public IDs for every public dataset, but the downside is that independent researchers could no longer link data that might be related. By creating a single set of public IDs associated with each internal study ID, public files retain the link.\nMaintaining a single set of public IDs requires a shared “bridge” dataset, that includes a row for each study ID and has the associated public ID. For studies with multiple levels of ID, we would typically have separate bridge datasets for each type of ID (e.g,. cluster ID, participant ID, etc.)\nCreate a public ID that can be used to uniquely identify participants and that can internally be linked to the original study IDs. We recommend creating a subdirectory in the study’s shared data directory to store the public IDs. The shared location enables multiple projects to use the same IDs. Create the IDs using a script that reads in the study IDs, creates a unique (uninformative) public ID for the study IDs, and then saves the bridge dataset. The script should be saved in the same directory as the public ID files.\n\n\nCaution! Note that small differences may arise if the new public IDs do not necessarily order participants in the same way as the internal IDs. The small differences are all in estimates that rely on resampling, such as Bootstrap CIs, permutation P-values, and TMLE, as the resampling process may lead tp slightly different re-samples. The key here, to ensure the results are consistent irrespective of the dataset used, is simply to not assign public IDs randomly. Use rank() on the internal ID instead of row_number() to ensure that the order is always the same. \n\n\n\n\n14.3.3 Example scripts\nWe have created a self-contained and reproducible example that you can run and replicate when making data public for your projects. It contains the following files and folders:\n\ndata/final/- folder containing the projects final data in both csv and rds formats\n\ncode/DEMO_generate_public_IDs.R- creates randomly generated public IDs that can be matched to the trial’s assigned patient IDs.\n\ndata/make_public/DEMO_internal_to_publicID.csv- the output from step #2, a bridge dataset with two variables- the new public ID and the patient’s assigned ID.\n\ncode/DEMO_create_public_datasets.R- joins the public IDs to the trial’s full dataset, and strips it of the assigned patient ID.\ndata/public/- folder containing the output from step #3- de-identified public dataset, in csv and rds formats, with uniquely identifying public IDs that cannot be easily linked back to the patient’s ID.\n\nThe example workflow is accessible via GitHub: https://github.com/proctor-ucsf/dcc-handbook/tree/master/templates/making-data-public",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#create-a-data-repository",
    "href": "13-data-publication.html#create-a-data-repository",
    "title": "14  Data Publication",
    "section": "14.4 Create a data repository",
    "text": "14.4 Create a data repository\nFirst, ensure that you create a codebook and metadata file for each public dataset See the DCC guide on Documenting datasets. Use the same name as the datasets, but with “-codebook.txt” / “-codebook.html” / “-codebook.csv” at the end (depending on the file format for the codebook). One nice option is the R codebook package, which also generates JSON output that is machine-readable.\n\n14.4.1 Steps for creating an Open Science Framework (OSF) repository:\n\nCreate a new OSF project per these instructions: https://help.osf.io/article/252-create-a-project\nCreate a data component and upload the datasets in .csv and .rds format along with the codebooks. The primary format for public dissemination is .csv but we make the .rds files available too as auxiliary files for convenience.\nCreate a notebook component and upload the final .html files (which will not be on github… but see optional item below)\nOn the OSF landing Wiki, provide some context. Here is a recent example: https://osf.io/954bt/\nCreate a Digital Object Identifier (DOI) for the repository. A DOI is a unique identifier that provides a persistent link to content, such as a dataset in this case. Learn more about DOIs\nOptional: Complete the software checklist and system requirement guide for the analysis to guide others. Include it on the GitHub README for the project: https://github.com/proctor-ucsf/mordor-antibody",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#edit-and-test-analysis-scripts",
    "href": "13-data-publication.html#edit-and-test-analysis-scripts",
    "title": "14  Data Publication",
    "section": "14.5 Edit and test analysis scripts",
    "text": "14.5 Edit and test analysis scripts\nMake minor changes to the analysis scripts so that they run on public data. If using version control in GitHub, the most straight-forward way is to create a branch from the main git branch that reads in the public files, and then renames the new public ID variable, e.g., “id_public” to the internally recognized ID variable name, e.g. “recordID”, when reading in the public data. Re-run all the analysis scripts to ensure that they still work with the public version of the dataset.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#create-a-public-github-page-for-public-scripts",
    "href": "13-data-publication.html#create-a-public-github-page-for-public-scripts",
    "title": "14  Data Publication",
    "section": "14.6 Create a public GitHub page for public scripts",
    "text": "14.6 Create a public GitHub page for public scripts\nAt minimum, we should include all of the scripts required to run the analyses. IMPORTANT: ensure you have taken a snapshot and saved your computing environment using the renv package (renv).\nSee examples:\n- ACTION - https://github.com/proctor-ucsf/ACTION-public\n- NAITRE - https://github.com/proctor-ucsf/NAITRE-primary\n\n\nCaution! Read through the scripts carefully to ensure there is no PHI in the code itself \n\n\nOnce a public GitHub page exists, you can create a new component on an OSF project (step 3, above) and link it to the public version of the GitHub repo.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "13-data-publication.html#go-live",
    "href": "13-data-publication.html#go-live",
    "title": "14  Data Publication",
    "section": "14.7 Go live",
    "text": "14.7 Go live\nOn GitHub, it is useful to create an official “release” version to freeze the repository, where you can have “associated files” with each version. Include the .html notebook output as additional files — since they aren’t tracked in GitHub, it does provide a way of freezing / saving the HTML output for us and others. OSF examples of a studies from UCSF’s Proctor Foundation:\n- ACTION - https://osf.io/ca3pe/\n- NAITRE - https://osf.io/ujeyb/\n- MORDOR Niger antibody study - https://osf.io/dgsq3/\nFurther reading on end-to-end data management: How to Store and Manage Your Data - PLOS",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "14-slurm.html",
    "href": "14-slurm.html",
    "title": "15  Slurm and cluster computing",
    "section": "",
    "text": "15.1 Getting started\nby Anna Nguyen, Jade Benjamin-Chung, and Gabby Barratt Heitmann\nWhen you need to run a script that requires a large amount of RAM, large files, or that uses parallelization, you can use Sherlock, Stanford’s computing cluster. Sherlock uses Slurm, an open source, scalable cluster management and job scheduling system for computing clusters. Jade can email Sherlock managers to get you an account. Please refer to the Sherlock user guide to learn about the system and how to use it. Below, we include a few tips specific to how we use Sherlock in our lab.\nTo access Sherlock, in terminal, log in using the following syntax and replace “USERNAME” with your Stanford alias. You will be prompted to enter your Stanford password (the same one you use for your email and other accounts) and to complete two-factor authentication.\nOnce you log in, you can view the contents of your home directory in command line by entering cd $HOME. You can create subfolders within this directory using the mkdir command. For example, you could make a “code” subdirectory and clone a Github repository there using the following code:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "14-slurm.html#getting-started",
    "href": "14-slurm.html#getting-started",
    "title": "15  Slurm and cluster computing",
    "section": "",
    "text": "ssh USERNAME@login.sherlock.stanford.edu\n\ncd $HOME\nmkdir code\ngit clone https://github.com/jadebc/covid19-infections.git\n\n15.1.1 One-Time System Set-Up\nTo keep the install packages consistent across different nodes, you will need to explicitly set the pathway to your R library directory.\nOpen your ~/.Renviron file (vi ~/.Renviron) and append the following line:\nNote: Once you open the file using vi [file_name], you must press i (on Mac OS) or Insert (on Windows) to make edits. After you finish, hit Esc to exit editing mode and type :wq to save and close the file.\nR_LIBS=~/R/x86_64-pc-linux-gnu-library/4.0.2\nAlternatively, run an R script with the following code on Sherlock:\nr_environ_file_path = file.path(Sys.getenv(\"HOME\"), \".Renviron\")\nif (!file.exists(r_environ_file_path)) file.create(r_environ_file_path)\n\ncat(\"\\nR_LIBS=~/R/x86_64-pc-linux-gnu-library/4.0.2\",\n    file = r_environ_file_path, sep = \"\\n\", append = TRUE)\nTo load packages that run off of C++, you’ll need to set the correct compiler options in your R environment.\nOpen the Makevars file in Sherlock (vi ~/.R/Makevars) and append the following lines\nCXX14FLAGS=-O3 -march=native -mtune=native -fPIC\nCXX14=g++\nAlternatively, create an R script with the following code, and run it on Sherlock:\ndotR = file.path(Sys.getenv(\"HOME\"), \".R\")\nif (!file.exists(dotR)) dir.create(dotR)\n\nM = file.path(dotR, \"Makevars\")\nif (!file.exists(M)) file.create(M)\n\ncat(\"\\nCXX14FLAGS=-O3 -march=native -mtune=native -fPIC\",\n    \"CXX14=g++\",\n    file = M, sep = \"\\n\", append = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "14-slurm.html#moving-files-to-sherlock",
    "href": "14-slurm.html#moving-files-to-sherlock",
    "title": "15  Slurm and cluster computing",
    "section": "15.2 Moving files to Sherlock",
    "text": "15.2 Moving files to Sherlock\nThe $HOME directory is a good place to store code and small test files (quota: 15 GB per user). Save large files to the $SCRATCH directory (quota: 100 TB per user). You can read more about storage options on Sherlock here. On the $SCRATCH directory, files that are not modified after 90 days are automatically deleted. For this reason, it’s best to create a bash script that records the file transfer process for a given project. See example code below:\n# note: the following steps should be done from your local \n# (not after ssh-ing into sherlock)\n\n# securely transfer folders from Box to sherlock home directory\n# note: the -r option is for folders and is not needed for files\nscp -r \"Box/malaria-project/folder-1/\" USERNAME@login.sherlock.stanford.edu:/home/users/USERNAME/\n\n# securely transfer folders from Box to your sherlock scratch directory\nscp -r \"Box/malaria-project/folder-2/\" USERNAME@login.sherlock.stanford.edu:/scratch/users/USERNAME/\n\n# securely transfer folders from Box to our shared scratch directory\nscp -r \"Box/malaria-project/folder-3/\" USERNAME@login.sherlock.stanford.edu:/scratch/group/jadebc/",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "14-slurm.html#installing-packages-on-sherlock",
    "href": "14-slurm.html#installing-packages-on-sherlock",
    "title": "15  Slurm and cluster computing",
    "section": "15.3 Installing packages on Sherlock",
    "text": "15.3 Installing packages on Sherlock\nWhen you begin working on Sherlock, you will most likely encounter problems with installing packages. To install packages, login to Sherlock on the command line and open a development node using the command sdev. Do not attempt to do this in the RStudio Server (see next section), as you will have to re-do it for every new session you open.\nssh USERNAME@login.sherlock.stanford.edu\n\nsdev\nThere is a package installation file explicitly written for Sherlock that you should run before testing any code and sourcing the configuration file. You should only have to install packages once. Sherlock requires that you specify the repository where the package is downloaded from. You may also need to add an additional argument to install.packages to prevent the packages from locking after installation:\ninstall.packages(&lt;PACKAGE NAME&gt;, repos=“http://cran.us.ur-project.org”, \n                  INSTALL_opts = \"--no-lock\"\")\nIn order for some R packages to work on Sherlock, it is necessary to load specific software modules before running R. These must be loaded in Sherlock each time you want to use the package in R. For example, for spatial and random effects analyses, you may need the modules/packages below. These modules must also be loaded on the command line prior to opening R in order for package installation to work.\nmodule --force purge # remove any previously loaded modules, including math and devel\nmodule load math\nmodule load math gmp/6.1.2\nmodule load devel\nmodule load gcc/10\nmodule load system\nmodule load json-glib/1.4.4\nmodule load curl/7.81.0\nmodule load physics\nmodule load physics udunits geos\nmodule load physics gdal/2.2.1 # for R/4.0.2\nmodule load physics proj/4.9.3 # for R/4.0.2\nmodule load pandoc/2.7.3\n\nmodule load R/4.0.2\n\nR # Open R in the Shell window to install individual packages or test code\nRscript install-packages-sherlock.R # Alternatively, run the entire package installation script in the Shell window\nFiguring out the issues with some packages will require some trial and error. If you are still encountering problems installing a package, you may have to install other dependencies manually by reading through the error messages. If you try to install a dependency from CRAN and it isn’t working, it may be a module. You can search for it using the module spider command:\nmodule spider DEPENDENCY NAME\nHowever, you can also reach out to the Sherlock team for help. You can email them at srcc-support@stanford.edu. They also hold office hours.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "14-slurm.html#testing-your-code",
    "href": "14-slurm.html#testing-your-code",
    "title": "15  Slurm and cluster computing",
    "section": "15.4 Testing your code",
    "text": "15.4 Testing your code\nBoth of the following ways to test code on Sherlock are recommended for making small changes, such as editing file paths and making sure the packages and source files load. You should write and test the functionality of your script locally, only testing on Sherlock once major bugs are out.\n\n15.4.1 The command line\nThere are two main ways to explore and test code on Sherlock. The first way is best for users who are comfortable working on the command line and editing code in base R. Even if you are not comfortable yet, this is probably the better way because these commands will transfer between Sherlock and other cluster computers using Slurm.\nTypically, you will want to initially test your scripts by initiating a development node using the command sdev. This will allocate a small amount of computing resources for 1 hour. You can access R via command line using the following code.\n# open development node\nsdev\n\n# Load all the modules required by the packages you are using\nmodule load MODULE NAME  \n\n# Load R (default version)*\nmodule load R \n\n# initiate R in command line\nR\n*Note: for collaboration purposes, it’s best for everyone to work with one version of R. Check what version is being used for the project you are working on. Some packages only work with some versions of R, so it’s best to keep it consistent.\n\n\n15.4.2 The Sherlock OnDemand Dashboard\nThe second way to test and edit code is to use the Sherlock OnDemand Dashboard, accessed by typing login.sherlock.stanford.edu into a web browser. You will be prompted to authenticate the way you would for any Stanford website. This is the best way to edit code for people who are not comfortable accessing & editing in base R in a Shell application.\nYou can test your code via the Rstudio server on Sherlock. To access this, login to the Dashboard, then click on Interactive Apps in the menu bar and choose R Studio Server. Similar to the sdev node, you have to set various parameters for your session. Choose a version of R and set the time – max. 2 hours. You can play with the other configurations, but this is likely unnecessary, as you should not need huge computing power to test small amounts of code. Keep in mind the more computing power you request, the lower priority your request becomes. You will then wait for the resources to become available, and you will be able to click “Launch” when they are (if you don’t mess with the CPU or GPU, this is usually less than 2 minutes). The screen that opens will look very similar to the RStudio on your local.\nDo NOT use the RStudio Server’s Terminal to install packages, set your R environment, and do everything else needed to configure Sherlock because you will likely need to re-do it for every session/project. It’s best to use the Dashboard/RStudio Server if you are more comfortable testing & editing in RStudio rather than through base R in a Shell application.\n\n\n15.4.3 Filepaths & configuration on Sherlock\nIn most cases, you will want to test that the file paths work correctly on Sherlock. You will likely need to add code to the configuration file in the project repository that specifies Sherlock-specific file paths. Here is an example:\n# set sherlock-specific file paths\nif(Sys.getenv(\"LMOD_SYSHOST\")==\"sherlock\"){\n  \n  sherlock_path = paste0(Sys.getenv(\"HOME\"), \"/malaria-project/\")\n  \n  data_path = paste0(sherlock_path, \"data/\")\n  results_path = paste0(sherlock_path, \"results/\")\n}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "14-slurm.html#storage-group-storage-access",
    "href": "14-slurm.html#storage-group-storage-access",
    "title": "15  Slurm and cluster computing",
    "section": "15.5 Storage & group storage access",
    "text": "15.5 Storage & group storage access\n\n15.5.1 Individual storage\nThere are multiple places to store your files on Sherlock. Each user has their own $HOME directory as well as a $SCRATCH directory. These are directories that can be accessed via the command line once you’ve logged in to Sherlock:\ncd $HOME \ncd /home/users/USERNAME # Alternatively, use the full path\n\ncd $SCRATCH\ncd /scratch/users/USERNAME # Full path\nYou can also navigate to these using the File Explorer on Sherlock OnDemand.\n$HOME has a volume quota of 15 GB. $SCRATCH has a volume quota of 100 TB, but files here get deleted 90 days after their last modification. Thus, use $SCRACTH for test files, exploratory analyses, and temporary storage. Use $HOME for long-term storage of important files and more finalized analyses.\nYou can read more about storage options on Sherlock here.\n\n\n15.5.2 Group storage\nThe lab also has a $GROUP_HOME and a $GROUP_SCRATCH to store files for collaborative use. $GROUP_HOME has a volume quota of 1 TB and infinite retention time, whereas $GROUP_SCRATCH has a volume quota of 100 TB and the same 90-day retention limit. You can access these via the command line or navigate to them using the File Explorer:\ncd $GROUP_HOME\ncd /home/groups/jadebc\n\ncd $GROUP_SCRATCH\ncd /scratch/groups/jadebc\nHowever, saving files to group storage can be tricky. You can try using the scp command in the section “Moving files to Sherlock” to see if you have permission to add files to group directories. Read the next section to ensure any directories you create have the right permissions.\n\n\n15.5.3 Folder permissions\nGenerally, when we put folders in $GROUP_HOME or $GROUP_SCRATCH, it is so that we can collaborate on an analysis within the research group, so multiple people need to be able to access the folders. If you create a new folder in $GROUP_HOME or $GROUP_SCRATCH, please check the folder’s permissions to ensure that other group members are able to access its contents. To check the permissions of a folder, navigate to the level above it, and enter ls -l. You will see output like this:\ndrwxrwxrwx 2 jadebc jadebc  2204 Jun 17 13:12 myfolder\nPlease review this website to learn how to interpret the code on the left side of this output. The website also tells you how to change folder permissions. In order to ensure that all users and group members are able to access a folder’s contents, you can use the following command:\nchmod ugo+rwx FOLDER_NAME",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "14-slurm.html#running-big-jobs",
    "href": "14-slurm.html#running-big-jobs",
    "title": "15  Slurm and cluster computing",
    "section": "15.6 Running big jobs",
    "text": "15.6 Running big jobs\nOnce your test scripts run successfully, you can submit an sbatch script for larger jobs. These are text files with a .sh suffix. Use a text editor like Sublime to create such a script. Documentation on sbatch options is available here. Here is an example of an sbatch script with the following options:\n\njob-name=run_inc: Job name that will show up in the Sherlock system\nbegin=now: Requests to start the job as soon as the requested resources are available\ndependency=singleton: Jobs can begin after all previously launched jobs with the same name and user have ended.\nmail-type=ALL: Receive all types of email notification (e.g., when job starts, fails, ends)\ncpus-per-task=16: Request 16 processors per task. The default is one processor per task.\nmem=64G: Request 64 GB memory per node.\noutput=00-run_inc_log.out: Create a log file called 00-run_inc_log.out that contains information about the Slurm session\ntime=47:59:00: Set maximum run time to 47 hours and 59 minutes. If you don’t include this option, Sherlock will automatically exit scripts after 2 hours of run time.\n\nThe file analysis.out will contain the log file for the R script analysis.R.\n#!/bin/bash\n\n#SBATCH --job-name=run_inc\n#SBATCH --begin=now\n#SBATCH --dependency=singleton\n#SBATCH --mail-type=ALL\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=64G\n#SBATCH --mem=64G\n#SBATCH --output=00-run_inc_log.out\n#SBATCH --time=47:59:00\n\ncd $HOME/malaria-code-repo/2-analysis/\n\nmodule purge \n\n# load R version 4.0.2 (required for certain packages)\nmodule load R/4.0.2\n\n# load gcc, a C++ compiler (required for certain packages)\nmodule load gcc/10\n\n# load software required for spatial analyses in R\nmodule load physics gdal\nmodule load physics proj\n\nR CMD BATCH --no-save analysis.R analysis.out\nTo submit this job, save the code in the chunk above in a script called myjob.sh and then enter the following command into terminal:\nsbatch myjob.sh \nTo check on the status of your job, enter the following code into terminal:\nsqueue -u $USERNAME",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Slurm and cluster computing</span>"
    ]
  },
  {
    "objectID": "15-checklists.html",
    "href": "15-checklists.html",
    "title": "16  Checklists",
    "section": "",
    "text": "16.1 Pre-analysis plan checklist\nby Jade Benjamin-Chung",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "15-checklists.html#pre-analysis-plan-checklist",
    "href": "15-checklists.html#pre-analysis-plan-checklist",
    "title": "16  Checklists",
    "section": "",
    "text": "Brief background on the study (a condensed version of the introduction section of the paper)\nHypotheses / objectives\nStudy design\nDescription of data\nDefinition of outcomes\nDefinition of interventions / exposures\nDefinition of covariates\nStatistical power calculation\nStatistical model description\nCovariate selection / screening\nStandard error estimation method\nMissing data analysis\nAssessment of effect modification / subgroup analyses\nSensitivity analyses\nNegative control analyses",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "15-checklists.html#code-checklist",
    "href": "15-checklists.html#code-checklist",
    "title": "16  Checklists",
    "section": "16.2 Code checklist",
    "text": "16.2 Code checklist\n\nDoes the script run without errors?\nIs code self-contained within repo and/or associated Box folder?\nIs all commented out code / remarks removed?\nDoes the header accurately describe the process completed in the script?\nIs the script pushed to its github repository?\nDoes the code adhere to the coding style guide?\nAre all warnings ignorable? Should any warnings be intentionally suppressed or addressed?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "15-checklists.html#manuscript-checklist",
    "href": "15-checklists.html#manuscript-checklist",
    "title": "16  Checklists",
    "section": "16.3 Manuscript checklist",
    "text": "16.3 Manuscript checklist\nThis is adapted in part from this article.\n\nHave you completed the relevant reporting checklist, if applicable? (Collection of checklists)\nAre the study results within the manuscript replicable (i.e., if you rerun the code in the study’s repository, the tables and figures will be exactly replicated?)\nIs a target journal selected?\nIs the title declarative, in other words, does it state the object/findings rather than suggest them?\nIs the word count of the manuscript close to the target journal’s allowance?\nDoes the manuscript adhere to the formatting guide of the target journal?\nDoes the manuscript use a consistent voice (passive or active – usually active is preferred … pun intended)?\nIs each figure and table (including supplementary material) referenced in the main text?\nIs there a caption for each figure and table (including supplementary material)?\nAre tables/figures and supplementary material numbered in accordance with their appearance in the main text?\nDoes the text use past tense if it is reporting research findings or future tense if it is a study protocol?\nDoes the text avoid subjective wording (e.g., “interesting”, “dramatic”)?\nDoes the text use minimal abbreviations, and are all abbreviations defined at first use?\nDoes the text avoid directionless words? (e.g., instead of writing, ‘Precipitation influences disease risk’, write, ‘Precipitation was associated with increased disease risk’).\nDoes the text avoid making causal claims that are not supported by the study design? Be careful about the words “effect”, “increase”, and “decrease”, which are often interpreted as causal.\nDoes the text avoid describing results with the word “significant”, which can easily be confused with statistical significance? (see references on this topic here)\nHave you drafted author contributions? Do they follow the CRediT Taxonomy for author contributions?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "15-checklists.html#figure-checklist",
    "href": "15-checklists.html#figure-checklist",
    "title": "16  Checklists",
    "section": "16.4 Figure checklist",
    "text": "16.4 Figure checklist\n\nAre the x-axis and y-axis labeled?\nIf the figure includes panels, is each panel labeled?\nAre there sufficient numerical / text labels and breaks on the x-axis and y-axis?\nIs the font size appropriate (i.e., large enough to read, not so large that it distracts from the data presented in the figure?)\nAre the colors used colorblind friendly? See a colorblind-friendly palette here, a neat palette generator with colorblind options here, and an article on why this matters here\nAre colors/shapes/line types defined in a legend?\nAre the legends and other labels easy to understand with minimal abbreviations?\nIf there is overplotting, is transparency used to show overlapping data?\nAre 95% confidence intervals or other measures of precision shown, if applicable?",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "16-resources.html",
    "href": "16-resources.html",
    "title": "17  Resources",
    "section": "",
    "text": "17.1 Resources for R\nby Jade Benjamin-Chung and Kunal Mishra",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#resources-for-r",
    "href": "16-resources.html#resources-for-r",
    "title": "17  Resources",
    "section": "",
    "text": "dplyr and tidyr cheat sheet\nggplot cheat sheet\ndata table cheat sheet\nRMarkdown cheat sheet\nHadley Wickham’s R Style Guide\nJade’s R-for-epi course\nTidy Eval in 5 Minutes (video)\nTidy Evaluation (e-book)\nData Frame Columns as Arguments to Dplyr Functions (blog)\nStandard Evaluation for *_join (stackoverflow)\nProgramming with dplyr (package vignette)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#resources-for-git-github",
    "href": "16-resources.html#resources-for-git-github",
    "title": "17  Resources",
    "section": "17.2 Resources for Git & Github",
    "text": "17.2 Resources for Git & Github\n\nData Camp introduction to Git\nIntroduction to Github",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#scientific-figures",
    "href": "16-resources.html#scientific-figures",
    "title": "17  Resources",
    "section": "17.3 Scientific figures",
    "text": "17.3 Scientific figures\n\nTen Simple Rules for Better Figures",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#writing",
    "href": "16-resources.html#writing",
    "title": "17  Resources",
    "section": "17.4 Writing",
    "text": "17.4 Writing\n\nTips on how to write a great science paper\nICMJE Definition of authorship\nNature article on elements of style for scientific writing\nThe Pathway to Publishing: A Guide to Quantitative Writing in the Health Sciences\nSecret, actionable writing tips",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#presentations",
    "href": "16-resources.html#presentations",
    "title": "17  Resources",
    "section": "17.5 Presentations",
    "text": "17.5 Presentations\n\nHow to tell a compelling story in scientific presentations\nHow to give a killer narratively-driven scientific talk\nHow to make a better poster\nHow to make an even better poster",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#professional-advice",
    "href": "16-resources.html#professional-advice",
    "title": "17  Resources",
    "section": "17.6 Professional advice",
    "text": "17.6 Professional advice\n\nProfessional advice, especially for your first job",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#funding",
    "href": "16-resources.html#funding",
    "title": "17  Resources",
    "section": "17.7 Funding",
    "text": "17.7 Funding\n\nBuilding Your Funding Train",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "16-resources.html#ethics-and-global-health-research",
    "href": "16-resources.html#ethics-and-global-health-research",
    "title": "17  Resources",
    "section": "17.8 Ethics and global health research",
    "text": "17.8 Ethics and global health research\n\nGlobal Code of Conduct For Research in Resource-Poor Settings\nWho is a global health expert? Advice for aspiring global health experts\nTransforming Global Health Partnerships",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Resources</span>"
    ]
  }
]