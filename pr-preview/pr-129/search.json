[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCD-SeRG Lab Manual",
    "section": "",
    "text": "1 Welcome to UCD-SeRG!\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to UCD-SeRG!</span>"
    ]
  },
  {
    "objectID": "index.html#about-the-lab",
    "href": "index.html#about-the-lab",
    "title": "UCD-SeRG Lab Manual",
    "section": "1.1 About the lab",
    "text": "1.1 About the lab\nWelcome to the Seroepidemiology Research Group (SeRG) at the University of California, Davis, led by Drs. Kristen Aiemjoy and Ezra Morrison. Accurate methods to measure infectious disease burden are essential for guiding public health decisions, yet many infectious diseases remain under-recognized due to limited diagnostics and costly, resource-intensive surveillance systems. Our work addresses this gap by developing seroepidemiologic methods to characterize infection burden in populations. Currently, we focus on enteric fever (Salmonella Typhi and Paratyphi), Scrub Typhus (Orientia tsutsugamushi), Melioidosis (Burkholderia pseudomallei), Shigella (Shigella spp.), and Cholera (Vibrio cholerae). We are supported by the US National Institutes of Health, the Bill and Melinda Gates Foundation, and the Department of Defense, and collaborate with partners around the world. To learn more about the lab, visit ucdserg.ucdavis.edu.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to UCD-SeRG!</span>"
    ]
  },
  {
    "objectID": "index.html#about-this-lab-manual",
    "href": "index.html#about-this-lab-manual",
    "title": "UCD-SeRG Lab Manual",
    "section": "1.2 About this lab manual",
    "text": "1.2 About this lab manual\nThis lab manual covers our communication strategy, code of conduct, and best practices for reproducibility of computational workflows. It is a living document that is updated regularly.\nThis manual is a fork of the Benjamin-Chung Lab Manual (Benjamin-Chung et al. 2024), adapted for UCD-SeRG. We are grateful to Dr. Jade Benjamin-Chung and her team for developing and openly sharing their excellent lab manual. You can view the original manual at jadebc.github.io/lab-manual. Original contributors include Jade Benjamin-Chung, Kunal Mishra, Stephanie Djajadi, Nolan Pokpongkiat, Anna Nguyen, Iris Tong, and Gabby Barratt Heitmann.\nFeel free to draw from this manual (and please cite it if you do!).\n\n\n  This work is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License (“Creative Commons Attribution-NonCommercial 4.0 International License,” n.d.).\n\n\n\n\nBenjamin-Chung, Jade, Kunal Mishra, Stephanie Djajadi, Nolan Pokpongkiat, Anna Nguyen, Iris Tong, and Gabby Barratt Heitmann. 2024. “Benjamin-Chung Lab Manual.” https://jadebc.github.io/lab-manual/.\n\n\n“Creative Commons Attribution-NonCommercial 4.0 International License.” n.d. Creative Commons. http://creativecommons.org/licenses/by-nc/4.0/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to UCD-SeRG!</span>"
    ]
  },
  {
    "objectID": "culture-and-conduct.html",
    "href": "culture-and-conduct.html",
    "title": "2  Culture and conduct",
    "section": "",
    "text": "2.1 Lab culture\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung\nWe are committed to a lab culture that is collaborative, supportive, inclusive, open, and free from discrimination and harassment.\nWe encourage students / staff of all experience levels to respectfully share their honest opinions and ideas on any topic. Our group has thrived upon such respectful honest input from team members over the years, and this document is a product of years of student and staff input (and even debate) that has gradually improved our productivity and overall quality of our work.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "culture-and-conduct.html#diversity-equity-and-inclusion",
    "href": "culture-and-conduct.html#diversity-equity-and-inclusion",
    "title": "2  Culture and conduct",
    "section": "2.2 Diversity, equity, and inclusion",
    "text": "2.2 Diversity, equity, and inclusion\nUCD-SeRG recognizes the importance of and is committed to cultivating a culture of diversity, equity, and inclusion. This means being a safe, supportive, and anti-racist environment in which students from diverse backgrounds are equally and inclusively supported in their education and training. Diversity takes many forms, and includes, but is not limited to, differences in race, ethnicity, gender, sexuality, socioeconomic status, religion, disability, and political affiliation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "culture-and-conduct.html#protecting-human-subjects",
    "href": "culture-and-conduct.html#protecting-human-subjects",
    "title": "2  Culture and conduct",
    "section": "2.3 Protecting human subjects",
    "text": "2.3 Protecting human subjects\nAll lab members must complete CITI Human Subjects Biomedical Group 1 training and share their certificate with the lab leadership. Team members will be added to relevant Institutional Review Board protocols prior to their start date to ensure they have permission to work with identifiable datasets.\nOne of the most relevant aspects of protecting human subjects in our work is maintaining confidentiality. For students supporting our data science efforts, in practice this means:\n\nBe sure to understand and comply with project-specific policies about where data can be saved, particularly if the data include personal identifiers.\nDo not share data with anyone without permission, including to other members of the group, who might not be on the same IRB protocol as you (check with lab leadership first).\n\nRemember, data that looks like it does not contain identifiers to you might still be classified as data that requires special protection by our IRB or under HIPAA, so always proceed with caution and ask for help if you have any concerns about how to maintain study participant confidentiality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "culture-and-conduct.html#authorship",
    "href": "culture-and-conduct.html#authorship",
    "title": "2  Culture and conduct",
    "section": "2.4 Authorship",
    "text": "2.4 Authorship\nWe adhere to the ICMJE Definition of authorship (International Committee of Medical Journal Editors, n.d.) and are happy for team members who meet the definition of authorship to be included as co-authors on scientific manuscripts. To qualify for authorship, individuals must meet all four criteria:\n\nSubstantial contributions to conception/design, or acquisition/analysis/interpretation of data\nDrafting the work or revising it critically for important intellectual content\nFinal approval of the version to be published\nAgreement to be accountable for all aspects of the work\n\nAuthorship practices:\n\nFirst authorship: Typically goes to the person who led the work\nCorresponding author: Usually the PI, unless otherwise agreed\nCo-authorship: Determined by substantial intellectual contributions\nAuthor order: Should be discussed and agreed upon by all authors\nAcknowledgments: For contributions that don’t meet authorship criteria\n\nAuthorship should be discussed early in a project and revisited as the work evolves to ensure transparency and fairness. We encourage using the CRediT Taxonomy to document specific author contributions.\n\n\n\n\nInternational Committee of Medical Journal Editors. n.d. “Defining the Role of Authors and Contributors.” ICMJE. http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Culture and conduct</span>"
    ]
  },
  {
    "objectID": "communication.html",
    "href": "communication.html",
    "title": "3  Communication and coordination",
    "section": "",
    "text": "3.1 Microsoft Teams\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung\nOne benefit of the academic environment is its schedule flexibility and autonomy. This means that lab members may choose to work in the early morning, afternoon, evening, or weekends. That said, we do not expect lab members to respond outside of normal business hours (unless there are special circumstances).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#microsoft-teams",
    "href": "communication.html#microsoft-teams",
    "title": "3  Communication and coordination",
    "section": "",
    "text": "Use Microsoft Teams for scheduling, coding related questions, quick check ins, etc. If your Teams message exceeds 200 words, it might be time to use email.\nUse channels instead of direct messages unless you need to discuss something private.\nPlease make an effort to respond to messages that mention you (e.g., @username) as quickly as possible and always within 24 hours.\nIf you are unusually busy (e.g., taking MCAT/GRE, taking many exams) or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual and also set your status in Teams (e.g., it could say “On vacation”) so we know not to expect to see you online.\nPlease thread messages in Teams as much as possible.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#email",
    "href": "communication.html#email",
    "title": "3  Communication and coordination",
    "section": "3.2 Email",
    "text": "3.2 Email\n\nUse email for longer messages (&gt;200 words) or messages that merit preservation.\nGenerally, strive to respond within 24 hours hours. As noted above, if you are unusually busy or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#task-management",
    "href": "communication.html#task-management",
    "title": "3  Communication and coordination",
    "section": "3.3 Task Management",
    "text": "3.3 Task Management\nWe use a combination of tools to track and manage project tasks:\n\nGitHub Issues and Projects: For code-related tasks, feature requests, and bug tracking. Lab leadership will assign issues and organize them in GitHub Projects. Issues are prioritized within projects, and you can track your assigned tasks there.\nMicrosoft To-Do and other M365 task tracking tools: For general lab tasks and personal task management. Lab leadership may assign tasks through these tools, which integrate with Microsoft Teams.\nGenerally, strive to complete assigned tasks by the date listed.\nUse checklists to break down tasks into smaller chunks. Sometimes leadership will create these for you, but you can also add them yourself.\nUpdate task status as you make progress so the team can stay coordinated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#google-drive",
    "href": "communication.html#google-drive",
    "title": "3  Communication and coordination",
    "section": "3.4 Google Drive",
    "text": "3.4 Google Drive\n\nWe mostly use Google Drive to create shared documents with longer descriptions of tasks. These documents may be linked to in GitHub Issues or other task tracking tools. Lab leadership often shares these with the whole team since tasks are overlapping, and even if a task is assigned to one person, others may have valuable insights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#uc-davis-box-and-sharepoint",
    "href": "communication.html#uc-davis-box-and-sharepoint",
    "title": "3  Communication and coordination",
    "section": "3.5 UC Davis Box and SharePoint",
    "text": "3.5 UC Davis Box and SharePoint\n\nHuman subjects data for research studies are generally stored in UC Davis Box or SharePoint. Please check with lab leadership about whether there are special storage and transfer requirements for the datasets you are working with for each study.\nYou can access Box via your UC Davis credentials. For more information, visit UC Davis Box Support.\nSharePoint is also used for collaborative document storage and team file sharing. Access SharePoint through your UC Davis Microsoft 365 account.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#meetings",
    "href": "communication.html#meetings",
    "title": "3  Communication and coordination",
    "section": "3.6 Meetings",
    "text": "3.6 Meetings\n\nOur meetings start on the hour.\nIf you are going to be late, please send a message in our Teams channel.\nIf you are regularly not able to come on the hour, notify the team and we might choose the modify the agenda order or the start time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "communication.html#code-review",
    "href": "communication.html#code-review",
    "title": "3  Communication and coordination",
    "section": "3.7 Code Review",
    "text": "3.7 Code Review\nWhen submitting code to or reviewing code from colleagues, use best practices to provide and receive constructive feedback:\n\nTidyverse code review principles (Tidyverse Team 2023): Best practices for reviewing R code, including what to look for and how to provide constructive feedback.\n\n\n\n\n\nTidyverse Team. 2023. Tidyverse Code Review Principles. https://code-review.tidyverse.org/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Communication and coordination</span>"
    ]
  },
  {
    "objectID": "reproducibility.html",
    "href": "reproducibility.html",
    "title": "4  Reproducibility",
    "section": "",
    "text": "4.1 What is the reproducibility crisis?\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung\nOur lab adopts the following practices to maximize the reproducibility of our work.\nIn the past decade, an increasing number of studies have found that published study findings could not be reproduced. Researchers found that it was not possible to reproduce estimates from published studies: 1) with the same data and same or similar code and 2) with newly collected data using the same (or similar) study design. These “failures” of reproducibility were frequent enough and broad enough in scope, occurring across a range of disciplines (epidemiology, psychology, economics, and others) to be deeply troubling. Program and policy decisions based on erroneous research findings could lead to wasted resources, and at worst, could harm intended beneficiaries. This crisis has motivated new practices in reproducibility, transparency, and openness. Our lab is committed to adopting these best practices, and much of the remainder of the lab manual focuses on how to do so.\nRecommended readings on the “reproducibility crisis”:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#what-is-the-reproducibility-crisis",
    "href": "reproducibility.html#what-is-the-reproducibility-crisis",
    "title": "4  Reproducibility",
    "section": "",
    "text": "Nuzzo R. How scientists fool themselves – and how they can stop (Nuzzo 2015)\nStoddart C. Is there a reproducibility crisis in science? (Stoddart 2019)\nMunafò MR, et al. A manifesto for reproducible science (Munafò et al. 2017)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#study-design",
    "href": "reproducibility.html#study-design",
    "title": "4  Reproducibility",
    "section": "4.2 Study design",
    "text": "4.2 Study design\nAppropriate study design is beyond the scope of this lab manual and is something trainees develop through their coursework and mentoring.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#register-study-protocols",
    "href": "reproducibility.html#register-study-protocols",
    "title": "4  Reproducibility",
    "section": "4.3 Register study protocols",
    "text": "4.3 Register study protocols\nWe register all randomized trials on clinicaltrials.gov, and in some cases register observational studies as well.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#write-and-register-pre-analysis-plans",
    "href": "reproducibility.html#write-and-register-pre-analysis-plans",
    "title": "4  Reproducibility",
    "section": "4.4 Write and register pre-analysis plans",
    "text": "4.4 Write and register pre-analysis plans\nWe write pre-analysis plans for most original research projects that are not exploratory in nature, although in some cases, we write pre-analysis plans for exploratory studies as well. The format and content of pre-analysis plans can vary from project to project. Here is an example of one: https://osf.io/tgbxr/. Generally, these include:\n\nBrief background on the study (a condensed version of the introduction section of the paper)\nHypotheses / objectives\nStudy design\nDescription of data\nDefinition of outcomes\nDefinition of interventions / exposures\nDefinition of covariates\nStatistical power calculation\nStatistical analysis:\n\n\nType of model\nCovariate selection / screening\nStandard error estimation method\nMissing data analysis\nAssessment of effect modification / subgroup analyses\nSensitivity analyses\nNegative control analyses",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#create-reproducible-workflows",
    "href": "reproducibility.html#create-reproducible-workflows",
    "title": "4  Reproducibility",
    "section": "4.5 Create reproducible workflows",
    "text": "4.5 Create reproducible workflows\nReproducible workflows allow a user to reproduce study estimates and ideally figures and tables with a “single click”. In practice, this typically means running a single bash script that sources all replication scripts in a repository. These replication scripts complete data processing, data analysis, and figure/table generation. The following chapters provide detailed guidance on this topic:\n\nChapter 5: Code repositories\nChapter 6: Coding practices\nChapter 7: Coding style\nChapter 8: Code publication\nChapter 9: Working with big data\nChapter 10: Github\nChapter 11: Unix\n\nFor additional learning resources on reproducible research practices, see the UC Davis DataLab workshop on reproducible research.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#process-and-analyze-data-with-internal-replication-and-masking",
    "href": "reproducibility.html#process-and-analyze-data-with-internal-replication-and-masking",
    "title": "4  Reproducibility",
    "section": "4.6 Process and analyze data with internal replication and masking",
    "text": "4.6 Process and analyze data with internal replication and masking\nSee my video on this topic: https://www.youtube.com/watch?v=WoYkY9MkbRE",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#use-reporting-checklists-with-manuscripts",
    "href": "reproducibility.html#use-reporting-checklists-with-manuscripts",
    "title": "4  Reproducibility",
    "section": "4.7 Use reporting checklists with manuscripts",
    "text": "4.7 Use reporting checklists with manuscripts\nUsing reporting checklists helps ensure that peer-reviewed articles contain the information needed for readers to assess the validity of your work and/or attempt to reproduce it. A collection of reporting checklists is available from the EQUATOR Network (“EQUATOR Network: Enhancing the QUAlity and Transparency of Health Research,” n.d.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#publish-preprints",
    "href": "reproducibility.html#publish-preprints",
    "title": "4  Reproducibility",
    "section": "4.8 Publish preprints",
    "text": "4.8 Publish preprints\nA preprint is a scientific manuscript that has not been peer reviewed. Preprint servers create digital object identifiers (DOIs) and can be cited in other articles and in grant applications. Because the peer review process can take many months, publishing preprints prior to or during peer review enables other scientists to immediately learn from and build on your work. Importantly, NIH allows applicants to include preprint citations in their biosketches. In most cases, we publish preprints on medRxiv (“medRxiv: The Preprint Server for Health Sciences,” n.d.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "reproducibility.html#publish-data-when-possible-and-replication-scripts",
    "href": "reproducibility.html#publish-data-when-possible-and-replication-scripts",
    "title": "4  Reproducibility",
    "section": "4.9 Publish data (when possible) and replication scripts",
    "text": "4.9 Publish data (when possible) and replication scripts\nPublishing data and replication scripts allows other scientists to reproduce your work and to build upon it. We typically publish data on the Open Science Framework (“Open Science Framework,” n.d.), share links to Github repositories, and archive code on Zenodo.\n\n\n\n\n“EQUATOR Network: Enhancing the QUAlity and Transparency of Health Research.” n.d. EQUATOR Network. https://www.equator-network.org/.\n\n\n“medRxiv: The Preprint Server for Health Sciences.” n.d. Cold Spring Harbor Laboratory. https://www.medrxiv.org/.\n\n\nMunafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S. Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017. “A Manifesto for Reproducible Science.” Nature Human Behaviour 1. https://doi.org/10.1038/s41562-016-0021.\n\n\nNuzzo, Regina. 2015. “How Scientists Fool Themselves – and How They Can Stop.” Nature 526: 182–85. https://doi.org/10.1038/526182a.\n\n\n“Open Science Framework.” n.d. Center for Open Science. https://osf.io/.\n\n\nStoddart, Charlotte. 2019. “Is There a Reproducibility Crisis in Science?” Nature. https://doi.org/10.1038/d41586-019-00067-3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducibility</span>"
    ]
  },
  {
    "objectID": "code-repositories.html",
    "href": "code-repositories.html",
    "title": "5  Code repositories",
    "section": "",
    "text": "5.1 Package Structure\nAdapted by UCD-SeRG team from original by Kunal Mishra, Jade Benjamin-Chung, and Stephanie Djajadi\nEach study has at least one code repository that typically holds R code, shell scripts with Unix code, and research outputs (results .RDS files, tables, figures). Repositories may also include datasets. This chapter outlines how to organize these files. Adhering to a standard format makes it easier for us to efficiently collaborate across projects.\nUCD-SeRG projects use R package structure for most R-based work. This provides benefits for reproducibility, collaboration, and code quality even for analysis-only projects.\nAll R projects in our lab should be structured as R packages, even if they are primarily analysis projects and not intended for distribution on CRAN or Bioconductor. This standardized structure provides numerous benefits:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "code-repositories.html#package-structure",
    "href": "code-repositories.html#package-structure",
    "title": "5  Code repositories",
    "section": "",
    "text": "5.1.1 Why Use R Package Structure?\n\nOrganized code: Clear separation of functions (R/), documentation (man/), tests (tests/), data (data/), and vignettes/analyses\nDependency management: DESCRIPTION file explicitly declares all package dependencies and version restrictions, which simplifies installing those dependencies.\nAutomatic documentation: roxygen2 generates help files from inline comments\nBuilt-in testing: testthat framework integrates seamlessly with package structure\nCode quality: Tools like devtools::check() and lintr enforce best practices\nReproducibility: Package structure makes it easy to share and reproduce analyses\nReusable functions: Decompose complex analyses into well-documented, testable functions\nVersion control: Track changes to code, documentation, and data together\n\n\n\n5.1.2 Basic Package Structure\nmyproject/\n├── DESCRIPTION          # Package metadata and dependencies\n├── NAMESPACE            # Auto-generated, don't edit manually\n├── R/                   # All R functions (reusable code)\n│   ├── analysis_functions.R\n│   ├── data_prep.R\n│   └── plotting.R\n├── man/                 # Auto-generated documentation\n├── tests/              \n│   └── testthat/       # Unit tests\n├── data/               # Processed data objects (.rda files)\n├── data-raw/           # Raw data and data processing scripts\n│   ├── 0-prep-data.sh  # Shell scripts for data preparation\n│   ├── process_survey_data.R\n│   └── clean_lab_results.R\n├── vignettes/          # Long-form documentation\n│   ├── intro.qmd       # Main vignettes (shipped with package)\n│   ├── tutorial.qmd\n│   └── articles/       # Website-only articles (not shipped)\n│       ├── advanced-topics.qmd\n│       └── case-studies.qmd\n├── inst/               # Additional files to include in package\n│   ├── extdata/        # External data files and .RDS results\n│   │   ├── analysis_results.rds\n│   │   └── processed_data.rds\n│   ├── output/         # Figure and table outputs\n│   │   ├── figures/\n│   │   │   ├── fig1.pdf\n│   │   │   └── fig2.png\n│   │   └── tables/\n│   │       ├── table1.csv\n│   │       └── table2.xlsx\n│   └── analyses/       # Analyses using restricted data (see below)\n└── .Rproj              # RStudio project file\n\n\n5.1.3 Where to Place Analysis Files\n\n5.1.3.1 Vignettes vs Articles\nVignettes (vignettes/*.qmd): - Shipped with the package when installed - Accessible via vignette() and browseVignettes() in R - Displayed on CRAN - Built at package build time - Use for core package documentation and tutorials - Created with usethis::use_vignette(\"name\")\nArticles (vignettes/articles/*.qmd): - Website-only (not shipped with the package) - Only appear on the pkgdown website - Not accessible via vignette() in R - Not displayed on CRAN - Use for supplementary content, blog posts, extended tutorials, or frequently updated material - Created with usethis::use_article(\"name\") - Automatically added to .Rbuildignore\nWhen to use each: - Vignette: Essential tutorials users need offline, core package workflows - Article: Supplementary material, case studies, advanced topics, blog-style content\n\n\n5.1.3.2 Public Analyses (vignettes/)\nUse vignettes/ for analysis workbooks that:\n\nUse publicly available data\nShould be accessible to all package users\nAre core to understanding the package\n\nUse vignettes/articles/ for:\n\nExtended case studies\nBlog-style posts\nSupplementary analyses\nMaterial that updates frequently\n\nAll vignettes and articles will be rendered by pkgdown::build_site() on your package website.\n\n\n5.1.3.3 Analyses with Restricted Data (inst/analyses/)\nFor analyses that rely on private, sensitive, or restricted data, place .qmd or .qmd files in inst/analyses/:\nmyproject/\n├── inst/\n│   ├── analyses/\n│   │   ├── 01-confidential-data-analysis.qmd\n│   │   ├── 02-unpublished-results.qmd\n│   │   └── README.md  # Document data access requirements\n│   └── extdata/\n└── vignettes/\n    ├── 01-public-analysis.qmd\n    └── 02-demo-with-simulated-data.qmd\nBenefits of this approach:\n\nAnalyses with restricted data are included in version control alongside your code\nThey’re clearly separated from public documentation\ninst/analyses/ is excluded from pkgdown builds and package documentation\nCollaborators with data access can still run these analyses\nYou maintain a complete record of all project work\n\nNote on privacy: Files in inst/analyses/ are not inherently private—they will be visible if your repository is public. Use this folder for analyses that rely on restricted data that is stored separately, not for storing the restricted data itself. If you need to keep the analysis code private, use a private repository.\nBest practices for analyses with restricted data:\n\nDocument data requirements: Include a README.md in inst/analyses/ explaining:\n\nWhat data is required\nWhere to obtain it (if permissible)\nData access restrictions\nHow to set up data paths\n\nUse relative paths carefully: Structure your code so data paths can be configured:\n\n# In inst/analyses/01-analysis.qmd\n# Users should set this based on their local setup\ndata_dir &lt;- Sys.getenv(\"MYPROJECT_DATA\", \n                       default = \"~/restricted_data/myproject\")\nraw_data &lt;- readr::read_csv(file.path(data_dir, \"sensitive.csv\"))\n\nCreate public alternatives: When possible, create companion vignettes in vignettes/ using:\n\nSimulated data that mimics the structure\nPublicly available datasets\nAggregated/de-identified summaries\n\nAdd to .Rbuildignore: Ensure inst/analyses/ doesn’t cause package checks to fail:\n\n# Use usethis to add to .Rbuildignore\nusethis::use_build_ignore(\"inst/analyses\")\n\n\n\n5.1.4 Keep Analysis Workbooks Tidy\nDecompose reusable functions from your analysis notebooks into the R/ directory. Your vignettes should:\n\nBe clean, readable narratives of your analysis\nCall well-documented functions from your package\nFocus on the “what” and “why” rather than implementation details\nBe reproducible by others with a single click (or with documented data access for private analyses)\n\nExample of what NOT to do (all code in vignette):\n# Bad: 100 lines of data manipulation in vignette\nraw_data &lt;- read_csv(\"data.csv\")\n# ... 100 lines of cleaning, transforming, reshaping ...\ncleaned_data &lt;- final_result\nExample of what TO do (functions in R/, simple calls in vignette):\n# Good: Clean vignette calling documented functions\nraw_data &lt;- read_csv(\"data.csv\")\ncleaned_data &lt;- prep_study_data(raw_data)  # Function in R/data_prep.R\n\n\n5.1.5 Shell Scripts and Automation\nShell scripts are useful for automating workflows and ensuring reproducibility. Place shell scripts in data-raw/ alongside the R scripts they coordinate:\ndata-raw/\n├── 0-prep-data.sh          # Shell script to run all data prep\n├── 01-load-survey.R\n├── 02-clean-survey.R\n├── 03-merge-datasets.R\n└── 04-create-analysis-data.R\nUsing shell scripts:\n# data-raw/0-prep-data.sh\n#!/bin/bash\nRscript data-raw/01-load-survey.R\nRscript data-raw/02-clean-survey.R\nRscript data-raw/03-merge-datasets.R\nRscript data-raw/04-create-analysis-data.R\nThis is especially useful when data upstream changes — you can simply run the shell script to reproduce everything. After running shell scripts, .Rout log files will be generated for each script. It is important to check these files to ensure everything has run correctly.\n\n\n5.1.6 Storing Analysis Outputs\nResults files (.RDS): Save analysis results in inst/extdata/:\n# Save results\nreadr::write_rds(analysis_results, here(\"inst\", \"extdata\", \"analysis_results.rds\"))\n\n# Load results later\nresults &lt;- readr::read_rds(here(\"inst\", \"extdata\", \"analysis_results.rds\"))\nFigures and tables: Save publication outputs in inst/output/:\n# Save figure\nggsave(here(\"inst\", \"output\", \"figures\", \"fig1_incidence_trends.pdf\"), \n       width = 8, height = 6)\n\n# Save table\nreadr::write_csv(summary_table, \n                 here(\"inst\", \"output\", \"tables\", \"table1_demographics.csv\"))\nOrganization:\ninst/\n├── extdata/\n│   ├── analysis_results.rds\n│   ├── model_fits.rds\n│   └── processed_data.rds\n└── output/\n    ├── figures/\n    │   ├── fig1_incidence_trends.pdf\n    │   ├── fig2_risk_factors.png\n    │   └── figS1_sensitivity.pdf\n    └── tables/\n        ├── table1_demographics.csv\n        ├── table2_main_results.xlsx\n        └── tableS1_detailed_results.csv",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "code-repositories.html#rproj-files",
    "href": "code-repositories.html#rproj-files",
    "title": "5  Code repositories",
    "section": "5.2 .Rproj files",
    "text": "5.2 .Rproj files\nAn “R Project” can be created within RStudio by going to File &gt;&gt; New Project. Depending on where you are with your research, choose the most appropriate option. This will save preferences, working directories, and even the results of running code/data (though I’d recommend starting from scratch each time you open your project, in general). Then, ensure that whenever you are working on that specific research project, you open your created project to enable the full utility of .Rproj files. This also automatically sets the directory to the top level of the project.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "code-repositories.html#organizing-the-data-raw-folder",
    "href": "code-repositories.html#organizing-the-data-raw-folder",
    "title": "5  Code repositories",
    "section": "5.3 Organizing the data-raw folder",
    "text": "5.3 Organizing the data-raw folder\nThe data-raw folder serves as a catch-all for scripts that do not (yet) fit into the package structure described above. The data-raw folder should still be organized. We recommend the following subdirectory structure for data-raw:\n0-run-project.sh\n0-config.R\n1 - Data-Management/\n    0-prep-data.sh\n    1-prep-cdph-fluseas.R\n    2a-prep-absentee.R\n    2b-prep-absentee-weighted.R\n    3a-prep-absentee-adj.R\n    3b-prep-absentee-adj-weighted.R\n2 - Analysis/\n    0-run-analysis.sh\n    1 - Absentee-Mean/\n        1-absentee-mean-primary.R\n        2-absentee-mean-negative-control.R\n        3-absentee-mean-CDC.R\n        4-absentee-mean-peakwk.R\n        5-absentee-mean-cdph2.R\n        6-absentee-mean-cdph3.R\n    2 - Absentee-Positivity-Check/\n    3 - Absentee-P1/\n    4 - Absentee-P2/\n3 - Figures/\n    0-run-figures.sh\n    ...\n4 - Tables/\n    0-run-tables.sh\n    ...\n5 - Results/\n    1 - Absentee-Mean/\n        1-absentee-mean-primary.RDS\n        2-absentee-mean-negative-control.RDS\n        3-absentee-mean-CDC.RDS\n        4-absentee-mean-peakwk.RDS\n        5-absentee-mean-cdph2.RDS\n        6-absentee-mean-cdph3.RDS\n    ...\n.gitignore\nFor brevity, not every directory is “expanded”, but we can glean some important takeaways from what we do see.\n\n5.3.1 Configuration (‘config’) File\nThis is the single most important file for your project. It will be responsible for a variety of common tasks, declare global variables, load functions, declare paths, and more. Every other file in the project will begin with source(\"0-config\"), and its role is to reduce redundancy and create an abstraction layer that allows you to make changes in one place (0-config.R) rather than 5 different files. To this end, paths which will be reference in multiple scripts (i.e. a merged_data_path) can be declared in 0-config.R and simply referred to by its variable name in scripts. If you ever want to change things, rename them, or even switch from a downsample to the full data, all you would then to need to do is modify the path in one place and the change will automatically update throughout your project. See the example config file for more details. The paths defined in the 0-config.R file assume that users have opened the .Rproj file, which sets the directory to the top level of the project.\n\n\n5.3.2 Order Files and Directories\nThis makes the jumble of alphabetized filenames much more coherent and places similar code and files next to one another. This also helps us understand how data flows from start to finish and allows us to easily map a script to its output (i.e. 2 - Analysis/1 - Absentee-Mean/1-absentee-mean-primary.R =&gt; 5 - Results/1 - Absentee-Mean/1-absentee-mean-primary.RDS). If you take nothing else away from this guide, this is the single most helpful suggestion to make your workflow more coherent. Often the particular order of files will be in flux until an analysis is close to completion. At that time it is important to review file order and naming and reproduce everything prior to drafting a manuscript.\n\n\n5.3.3 Using Bash scripts to ensure reproducibility\nBash scripts are useful components of a reproducible workflow. At many of the directory levels (i.e. in 3 - Analysis), there is a bash script that runs each of the analysis scripts. This is exceptionally useful when data “upstream” changes – you simply run the bash script. See Chapter 12 for further details.\nAfter running bash scripts, .Rout log files will be generated for each script that has been executed. It is important to check these files. Scripts may appear to have run correctly in the terminal, but checking the log files is the only way to ensure that everything has run completely.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Code repositories</span>"
    ]
  },
  {
    "objectID": "coding-practices.html",
    "href": "coding-practices.html",
    "title": "6  R Coding Practices",
    "section": "",
    "text": "6.1 Lab Protocols for Code and Data\nAdapted by UCD-SeRG team from original by Kunal Mishra, Jade Benjamin-Chung, Stephanie Djajadi, and Iris Tong\nJust as wet labs have strict safety protocols to ensure reproducible results and prevent contamination, our computational lab has protocols for coding and data management. These protocols are not suggestions—they are essential practices that:\nViolating these protocols can have serious consequences, including invalid results, wasted time, inability to publish, and damage to scientific credibility. Treat coding and data management protocols with the same seriousness as you would safety protocols in a wet lab.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#lab-protocols-for-code-and-data",
    "href": "coding-practices.html#lab-protocols-for-code-and-data",
    "title": "6  R Coding Practices",
    "section": "",
    "text": "Ensure reproducibility: Others (including your future self) can recreate your analysis\nPrevent errors: Systematic approaches reduce the risk of mistakes\nEnable collaboration: Consistent practices allow team members to work together efficiently\nMaintain data integrity: Proper handling prevents data corruption and loss\nSupport publication: Well-documented, reproducible code is increasingly required for publication",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-r-package-tools",
    "href": "coding-practices.html#sec-r-package-tools",
    "title": "6  R Coding Practices",
    "section": "6.2 Essential R Package Development Tools",
    "text": "6.2 Essential R Package Development Tools\nThe following tools are essential for R package development in our lab:\n\n6.2.1 usethis: Package Setup and Management\nusethis automates common package development tasks:\n# Install usethis\ninstall.packages(\"usethis\")\n\n# Create a new package\nusethis::create_package(\"~/myproject\")\n\n# Add common components\nusethis::use_mit_license()          # Add a license\nusethis::use_git()                  # Initialize git\nusethis::use_github()               # Connect to GitHub\nusethis::use_testthat()             # Set up testing infrastructure\nusethis::use_vignette(\"intro\")      # Create a vignette (shipped with package)\nusethis::use_article(\"case-study\")  # Create an article (website-only)\nusethis::use_data_raw(\"dataset\")    # Create data processing script\nusethis::use_package(\"dplyr\")       # Add a dependency\nusethis::use_pipe()                 # Import magrittr pipe operator (no longer recommended)\n\n# Increment version\nusethis::use_version()              # Increment package version\n\n\n6.2.2 devtools: Development Workflow\ndevtools provides the core development workflow:\n# Install devtools\ninstall.packages(\"devtools\")\n\n# Load your package for interactive development\ndevtools::load_all()                # Like library(), but for development\n\n# Documentation\ndevtools::document()                # Generate documentation from roxygen2\n\n# Testing\ndevtools::test()                    # Run all tests\ndevtools::test_active_file()        # Run tests in current file\n\n# Checking\ndevtools::check()                   # R CMD check (comprehensive validation)\ndevtools::check_man()               # Check documentation only\n\n# Dependencies\ndevtools::install_dev_deps()        # Install all development dependencies\n\n# Building\ndevtools::build()                   # Build package bundle\ndevtools::install()                 # Install package locally\n\n\n6.2.3 pkgdown: Package Websites\npkgdown builds beautiful documentation websites from your package:\n# Install pkgdown\ninstall.packages(\"pkgdown\")\n\n# Set up pkgdown\nusethis::use_pkgdown()\n\n# Build website locally\npkgdown::build_site()\n\n# Preview in browser\npkgdown::build_site(preview = TRUE)\n\n# Build components separately\npkgdown::build_reference()          # Function reference\npkgdown::build_articles()           # Vignettes\npkgdown::build_home()               # Home page from README\nConfigure your pkgdown site with _pkgdown.yml:\nurl: https://ucd-serg.github.io/YOURPROJECT\n\ntemplate:\n  bootstrap: 5\n\nreference:\n  - title: \"Data Preparation\"\n    desc: \"Functions for preparing and cleaning data\"\n    contents:\n    - prep_study_data\n    - validate_data\n  \n  - title: \"Analysis\"\n    desc: \"Core analysis functions\"\n    contents:\n    - run_primary_analysis\n    - sensitivity_analysis\n\narticles:\n  - title: \"Analysis Workflow\"\n    navbar: Analysis\n    contents:\n    - 01-data-preparation\n    - 02-primary-analysis\n    - 03-sensitivity-analysis",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-r-workflow",
    "href": "coding-practices.html#sec-r-workflow",
    "title": "6  R Coding Practices",
    "section": "6.3 Complete Package Development Workflow",
    "text": "6.3 Complete Package Development Workflow\nHere’s the typical workflow for developing an R package in our lab:\n\n6.3.1 1. Initial Setup\nStarting from a template (recommended):\nUsing our R package template is the fastest way to get started with a new R package, as it provides pre-configured settings, GitHub Actions workflows, and development tools:\n\nUCD-SeRG R Package Template - Our recommended template with pre-configured development tools and CI workflows:\n\nRepository: https://github.com/UCD-SERG/r.package.template\nClick “Use this template” → “Create a new repository” on GitHub\nClone your new repository and start developing\n\n\nThe template includes pre-configured:\n\nGitHub Actions workflows for R CMD check, test coverage, and pkgdown deployment\nDevelopment tools setup ({usethis}, {devtools}, {roxygen2})\nTesting infrastructure ({testthat})\nCode styling and linting configurations\nPackage documentation structure\n\nWhile the template jumpstarts your project with up-to-date configuration and workflow files, you should still come up to speed on what all the config files do so you can modify and debug them as needed. The template serves as a central location for the most current versions of these files and best practices.\nStarting from scratch:\nIf you prefer to start from scratch or need to understand each setup step, you can create a new package manually:\n# Create package structure\nusethis::create_package(\"~/myproject\")\n\n# Set up infrastructure\nusethis::use_git()\nusethis::use_github()\nusethis::use_testthat()\nusethis::use_pkgdown()\nusethis::use_mit_license()\nusethis::use_readme_rmd()\n\n\n6.3.2 2. Add Dependencies\n# Add packages your project depends on\nusethis::use_package(\"dplyr\")\nusethis::use_package(\"ggplot2\")\nusethis::use_package(\"readr\")\n\n# Add packages only needed for development/testing\nusethis::use_package(\"testthat\", type = \"Suggests\")\n\n\n6.3.3 3. Write Functions\nCreate functions in R/ directory with roxygen2 documentation:\n#' Prepare Study Data\n#'\n#' Clean and prepare raw study data for analysis.\n#'\n#' @param raw_data A data frame containing raw study data\n#' @param validate Logical; whether to run validation checks\n#'\n#' @returns A cleaned data frame ready for analysis\n#'\n#' @examples\n#' raw_data &lt;- read_csv(\"data.csv\")\n#' clean_data &lt;- prep_study_data(raw_data)\n#'\n#' @export\nprep_study_data &lt;- function(raw_data, validate = TRUE) {\n  # Function implementation\n}\n\n\n6.3.4 4. Document\n# Generate documentation from roxygen2 comments\ndevtools::document()\n\n\n6.3.5 5. Test\nCreate tests in tests/testthat/:\n# tests/testthat/test-data_prep.R\ntest_that(\"prep_study_data handles missing values\", {\n  raw_data &lt;- data.frame(x = c(1, NA, 3))\n  result &lt;- prep_study_data(raw_data)\n  expect_false(anyNA(result$x))\n})\nRun tests:\ndevtools::test()\n\n\n6.3.6 6. Check\n# Comprehensive package check\ndevtools::check()\nFix any warnings or errors before proceeding.\n\n\n6.3.7 7. Build Documentation Site\npkgdown::build_site()\n\n\n6.3.8 8. Share and Publish\n# Push to GitHub\n# The pkgdown site can be automatically deployed to GitHub Pages\n# using GitHub Actions\n\n\n## Organizing scripts\n\n\nJust as your data \"flows\" through your project, data should flow naturally through a script. Very generally, you want to:\n\n1. describe the work completed in the script in a comment header\n2. source your configuration file (`0-config.R`) \n3. load all your data \n4. do all your analysis/computation \n5. save your data. \n\nEach of these sections should be \"chunked together\" using comments. See [this file](https://github.com/kmishra9/Flu-Absenteeism/blob/master/Master's%20Thesis%20-%20Spatial%20Epidemiology%20of%20Influenza/2a%20-%20Statistical-Inputs.R) for a good example of how to cleanly organize a file in a way that follows this \"flow\" and functionally separate pieces of code that are doing different things.\n\n\n\n## Testing Requirements {#sec-r-testing}\n\n**ALWAYS establish tests BEFORE modifying functions.** This ensures changes preserve existing behavior and new behavior is correctly validated.\n\n### When to Use Snapshot Tests\n\nUse snapshot tests (`expect_snapshot()`, `expect_snapshot_value()`) when:\n\n- Testing complex data structures (data frames, lists, model outputs)\n- Validating statistical results where exact values may vary slightly\n- Output format stability is important\n\n```r\ntest_that(\"prep_study_data produces expected structure\", {\n  result &lt;- prep_study_data(raw_data)\n  expect_snapshot_value(result, style = \"serialize\")\n})\n\n\n6.3.9 When to Use Explicit Value Tests\nUse explicit tests (expect_equal(), expect_identical()) when:\n\nTesting simple scalar outputs\nValidating specific numeric thresholds\nTesting Boolean returns or categorical outputs\n\ntest_that(\"calculate_mean returns correct value\", {\n  expect_equal(calculate_mean(c(1, 2, 3)), 2)\n  expect_equal(calculate_ratio(3, 7), 0.4285714, tolerance = 1e-6)\n})\n\n\n6.3.10 Testing Best Practices\n\nSeed randomness: Use withr::local_seed() for reproducible tests\nUse small test cases: Keep tests fast\nTest edge cases: Missing values, empty inputs, boundary conditions\nTest errors: Verify functions fail appropriately with invalid input\n\ntest_that(\"prep_study_data handles edge cases\", {\n  # Empty input\n  expect_error(prep_study_data(data.frame()))\n  \n  # Missing required columns\n  expect_error(prep_study_data(data.frame(x = 1)))\n  \n  # Valid input with missing values\n  result &lt;- prep_study_data(data.frame(id = 1:3, value = c(1, NA, 3)))\n  expect_true(all(!is.na(result$value)))\n})",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-iteration",
    "href": "coding-practices.html#sec-iteration",
    "title": "6  R Coding Practices",
    "section": "6.4 Iterative Operations",
    "text": "6.4 Iterative Operations\nWhen applying analyses with different variations (outcomes, exposures, subgroups), use functional programming approaches:\n\n6.4.1 lapply() and sapply()\n# Apply function to each element\nresults &lt;- lapply(outcomes, function(y) {\n  run_analysis(data, outcome = y)\n})\n\n# Simplify to vector if possible\nsummary_stats &lt;- sapply(data_list, mean)\n\n\n6.4.2 purrr::map() Family\nThe purrr package provides type-stable alternatives:\nlibrary(purrr)\n\n# Always returns a list\nresults &lt;- map(outcomes, ~ run_analysis(data, outcome = .x))\n\n# Type-specific variants\nmeans &lt;- map_dbl(data_list, mean)        # Returns numeric vector\nmodels &lt;- map(splits, ~ lm(y ~ x, data = .x))  # Returns list of models\n\n\n6.4.3 purrr::pmap() for Multiple Arguments\nWhen iterating over multiple parameter lists:\nparams &lt;- tibble(\n  outcome = c(\"outcome1\", \"outcome2\", \"outcome3\"),\n  exposure = c(\"exp1\", \"exp2\", \"exp3\"),\n  covariate_set = list(c(\"age\", \"sex\"), c(\"age\"), c(\"age\", \"sex\", \"bmi\"))\n)\n\nresults &lt;- pmap(params, function(outcome, exposure, covariate_set) {\n  run_analysis(\n    data = study_data,\n    outcome = outcome,\n    exposure = exposure,\n    covariates = covariate_set\n  )\n})\n\n\n6.4.4 Parallel Processing\nFor computationally intensive work, use future and furrr:\nlibrary(future)\nlibrary(furrr)\n\n# Set up parallel processing\nplan(multisession, workers = availableCores() - 1)\n\n# Parallel version of map()\nresults &lt;- future_map(large_list, time_consuming_function, .progress = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-data-io",
    "href": "coding-practices.html#sec-data-io",
    "title": "6  R Coding Practices",
    "section": "6.5 Reading and Saving Data",
    "text": "6.5 Reading and Saving Data\n\n6.5.1 RDS Files (Preferred)\nUse RDS format for R objects:\n# Save single object\nreadr::write_rds(analysis_results, here(\"results\", \"analysis.rds\"))\n\n# Read back\nresults &lt;- readr::read_rds(here(\"results\", \"analysis.rds\"))\nAvoid .RData files because: - You can’t control object names when loading - Can’t load individual objects - Creates confusion in older code\n\n\n6.5.2 CSV Files\nFor tabular data that may be shared with non-R users:\n# Write\nreadr::write_csv(data, here(\"data-raw\", \"clean_data.csv\"))\n\n# Read\ndata &lt;- readr::read_csv(here(\"data-raw\", \"clean_data.csv\"))\n\n# For very large files, use data.table\ndata.table::fwrite(large_data, \"big_file.csv\")\ndata &lt;- data.table::fread(\"big_file.csv\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-r-version-control",
    "href": "coding-practices.html#sec-r-version-control",
    "title": "6  R Coding Practices",
    "section": "6.6 Version Control and Collaboration",
    "text": "6.6 Version Control and Collaboration\n\n6.6.1 Version Numbers\nFollow semantic versioning (MAJOR.MINOR.PATCH):\n\nDevelopment versions: 0.0.0.9000, 0.0.0.9001, etc.\nFirst release: 0.1.0\nBug fixes: increment PATCH (e.g., 0.1.0 → 0.1.1)\nNew features: increment MINOR (e.g., 0.1.1 → 0.2.0)\nBreaking changes: increment MAJOR (e.g., 0.2.0 → 1.0.0)\n\n# Increment version\nusethis::use_version()\n\n\n6.6.2 NEWS File\nDocument all user-facing changes in NEWS.md:\n# myproject 0.2.0\n\n## New features\n\n- Added function for data validation\n- Improved error messages\n\n## Bug fixes\n\n- Fixed issue with missing values\n- Corrected calculation error in summary stats",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-r-ci",
    "href": "coding-practices.html#sec-r-ci",
    "title": "6  R Coding Practices",
    "section": "6.7 Continuous Integration",
    "text": "6.7 Continuous Integration\n\n6.7.1 Understanding GitHub Actions\nGitHub Actions is GitHub’s built-in automation platform that makes it easy to automate software workflows, including continuous integration and deployment (CI/CD). For R packages, this means you can automatically test your code, check for errors, and deploy documentation every time you push changes to GitHub.\nKey benefits of GitHub Actions:\n\nAutomated testing: Run R CMD check across multiple operating systems (Linux, macOS, Windows) and R versions\nImmediate feedback: Get notified of problems quickly, when they’re easier to fix\nBetter collaboration: External contributors can see if their changes pass all checks before you review\nQuality assurance: Catch platform-specific issues before they reach users\nDocumentation deployment: Automatically build and deploy your pkgdown website\n\nEven for solo developers, having automated checks run on different platforms helps avoid the “works on my machine” problem.\n\n\n6.7.2 Setting Up GitHub Actions\nThe easiest way to add GitHub Actions to your R package is using {usethis}. The tidyverse team maintains a collection of ready-to-use workflows at r-lib/actions that handle common R package tasks.\n\n6.7.2.1 Essential Workflows\n1. R CMD check (most important):\nusethis::use_github_action(\"check-standard\")\nThis runs R CMD check on Linux, macOS, and Windows to ensure your package works across platforms. If you only set up one workflow, make it this one.\n2. Test coverage:\nusethis::use_github_action(\"test-coverage\")\nCalculates what percentage of your code is covered by tests and reports to codecov.io.\n3. Package website:\nusethis::use_github_action(\"pkgdown\")\nAutomatically builds and deploys your pkgdown documentation site to GitHub Pages.\n\n\n6.7.2.2 Interactive Setup\nRunning usethis::use_github_action() without arguments shows a menu of recommended workflows:\nusethis::use_github_action()\n#&gt; Which action do you want to add? (0 to exit)\n#&gt; (See &lt;https://github.com/r-lib/actions/tree/v2/examples&gt; for other options)\n#&gt;\n#&gt; 1: check-standard: Run `R CMD check` on Linux, macOS, and Windows\n#&gt; 2: test-coverage: Compute test coverage and report to https://about.codecov.io\n#&gt; 3: pr-commands: Add /document and /style commands for pull requests\n\n\n\n6.7.3 How GitHub Actions Workflows Work\nWhen you set up a workflow, usethis creates a YAML configuration file in .github/workflows/. For example, check-standard creates .github/workflows/R-CMD-check.yaml.\nThis workflow automatically runs when you:\n\nPush commits to main or master\nOpen or update a pull request\n\nYou can view workflow results in the “Actions” tab of your GitHub repository. A status badge is added to your README showing whether checks are passing.\n\n\n6.7.4 Workflow Files and Security\n\n\n\n\n\n\nWarning\n\n\n\nImportant Security Consideration\nWorkflow files (.github/workflows/*.yaml) have access to repository secrets and can execute code. Always review workflow files carefully before committing them, especially if copied from external sources.\nSee Section 17.4.7 for guidance on working with workflow files using AI tools.\n\n\nThe workflow YAML files in .github/workflows/ are configuration files that tell GitHub Actions:\n\nWhen to run (on push, pull request, schedule, etc.)\nWhat operating systems and R versions to use\nWhat steps to execute (install dependencies, run checks, etc.)\n\n\n\n6.7.5 Troubleshooting Failed Workflows\nWhen workflows fail, check the “Actions” tab in your GitHub repository for detailed logs. Common issues include:\n\nTest failures: Your tests found a bug (this is good! fix the bug)\nPlatform-specific issues: Code works on your machine but not on other platforms\nMissing dependencies: System libraries needed for packages aren’t installed\nLinting errors: Code style issues detected by automated checks\n\nFor help addressing workflow failures, see Section 17.4.5.\n\n\n6.7.6 Additional Resources\n\nGitHub Actions features overview\nr-lib/actions repository - R-specific actions and example workflows\nR Packages book: Continuous Integration\nGitHub Actions documentation\nWhere to find help with r-lib/actions",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-r-qa-checklist",
    "href": "coding-practices.html#sec-r-qa-checklist",
    "title": "6  R Coding Practices",
    "section": "6.8 Quality Assurance Checklist",
    "text": "6.8 Quality Assurance Checklist\nBefore submitting a pull request or finalizing analysis, verify:\n\nAll functions have complete roxygen2 documentation\nAll functions have corresponding tests\ndevtools::document() has been run\ndevtools::test() passes with no failures\ndevtools::check() passes with no errors, warnings, or notes\nlintr::lint_package() shows no issues (or only acceptable ones)\nspelling::spell_check_package() passes\nVersion number has been incremented\nNEWS.md has been updated with changes\nREADME.Rmd has been updated (if needed) and README.md regenerated\npkgdown::build_site() builds successfully\nAll changes committed and pushed to GitHub",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-auto-styling",
    "href": "coding-practices.html#sec-auto-styling",
    "title": "6  R Coding Practices",
    "section": "6.9 Automated Code Styling",
    "text": "6.9 Automated Code Styling\n\n6.9.1 RStudio Built-in Formatting\nUse RStudio’s built-in autoformatter (keyboard shortcut: CMD-Shift-A or Ctrl-Shift-A) to quickly format highlighted code.\n\n\n6.9.2 styler Package\nFor automated styling of entire projects:\n# Install styler\ninstall.packages(\"styler\")\n\n# Style all files in R/ directory\nstyler::style_dir(\"R/\")\n\n# Style entire package\nstyler::style_pkg()\n\n# Note: styler modifies files in-place\n# Always use with version control so you can review changes\n\n\n6.9.3 lintr Package\nFor checking code style without modifying files:\n# Install lintr\ninstall.packages(\"lintr\")\n\n# Lint the entire package\nlintr::lint_package()\n\n# Lint a specific file\nlintr::lint(\"R/my_function.R\")\nThe linter checks for:\n\nUnused variables\nImproper whitespace\nLine length issues\nStyle guide violations\n\nYou can customize linting rules by creating a .lintr file in your project root.\nSee also Section 7.11.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-documenting-code",
    "href": "coding-practices.html#sec-documenting-code",
    "title": "6  R Coding Practices",
    "section": "6.10 Documenting your code",
    "text": "6.10 Documenting your code\n\n6.10.1 Function headers\nEvery function you write must include documentation to describe its purpose, inputs, and outputs. For any reproducible workflows, this is essential, because R is dynamically typed. This means you can pass a string into an argument that is meant to be a data.table, or a list into an argument meant for a tibble. It is the responsibility of a function’s author to document what each argument is meant to do and its basic type.\nWe use {roxygen2} (Wickham et al. 2024) for function documentation. Roxygen2 allows you to describe your functions in special comments next to their definitions, and automatically generates R documentation files (.Rd files) and helps manage your package NAMESPACE. The roxygen2 format uses #' comments placed immediately before the function definition.\nHere is an example of documenting a function using roxygen2:\n#' Calculate flu season means by site\n#'\n#' Make a dataframe with rows for flu season and site\n#' containing the number of patients with an outcome, the total patients,\n#' and the percent of patients with the outcome.\n#'\n#' @param data A data frame with variables flu_season, site, studyID, and yname\n#' @param yname A string for the outcome name\n#' @param silent A boolean specifying whether to suppress console output \n#'   (default: TRUE)\n#'\n#' @returns A dataframe as described above\n#'\n#' @examples\n#' calc_fluseas_mean(my_data, \"hospitalized\", silent = FALSE)\n#'\ncalc_fluseas_mean &lt;- function(data, yname, silent = TRUE) {\n  ### function code here\n  \n}\nThe roxygen2 header tells you what the function does, its various inputs, and how you might use it. Also notice that all optional arguments (i.e. ones with pre-specified defaults) follow arguments that require user input.\nFor more information on roxygen2 syntax and features, see https://roxygen2.r-lib.org/.\n\n\n6.10.2 Using ... (dots) and @inheritDotParams\nThe ... argument (pronounced “dots” or “ellipsis”) is a special R construct that allows functions to accept additional arguments that are passed to other functions. This is particularly useful when creating wrapper functions that call other functions internally.\nWhen to use ...:\n\nYou’re creating a wrapper function that calls another function\nYou want to allow users to pass additional arguments to an internal function\nYou want to provide flexibility without explicitly listing all possible arguments\n\nBasic example with ...:\n#' Plot data with custom ggplot2 styling\n#'\n#' A wrapper function that creates a scatter plot with custom theme settings.\n#' Additional arguments are passed to ggplot2::geom_point().\n#'\n#' @param data A data frame containing the variables to plot\n#' @param x A string specifying the x-axis variable name\n#' @param y A string specifying the y-axis variable name\n#' @param ... Additional arguments passed to ggplot2::geom_point()\n#'\n#' @returns A ggplot2 object\n#'\n#' @examples\n#' # Pass color and size arguments to geom_point\n#' plot_with_style(my_data, \"age\", \"height\", color = \"blue\", size = 3)\n#'\nplot_with_style &lt;- function(data, x, y, ...) {\n  ggplot2::ggplot(data, ggplot2::aes(.data[[x]], .data[[y]])) +\n    ggplot2::geom_point(...) +\n    ggplot2::theme_minimal()  # Apply a minimal theme\n}\nWhile the example above documents ... with a simple description, roxygen2 provides @inheritDotParams to automatically inherit parameter documentation from the function you’re calling. This is more robust and maintainable because it automatically stays synchronized with the target function’s documentation.\nUsing @inheritDotParams:\n#' Plot data with custom ggplot2 styling\n#'\n#' A wrapper function that creates a scatter plot with custom theme settings.\n#'\n#' @param data A data frame containing the variables to plot\n#' @param x A string specifying the x-axis variable name\n#' @param y A string specifying the y-axis variable name\n#' @inheritDotParams ggplot2::geom_point -mapping -data -stat -position\n#'\n#' @returns A ggplot2 object\n#'\n#' @examples\n#' # Pass color and size arguments to geom_point\n#' plot_with_style(my_data, \"age\", \"height\", color = \"blue\", size = 3)\n#'\nplot_with_style &lt;- function(data, x, y, ...) {\n  ggplot2::ggplot(data, ggplot2::aes(.data[[x]], .data[[y]])) +\n    ggplot2::geom_point(...) +\n    ggplot2::theme_minimal()  # Apply a minimal theme\n}\nThe @inheritDotParams tag:\n\nAutomatically imports parameter documentation from ggplot2::geom_point()\nUses -mapping -data -stat -position to exclude parameters that don’t make sense in this context\nKeeps documentation synchronized if the underlying function changes\nMakes it clear which function receives the ... arguments\n\nBest practices for ...:\n\nAlways document what receives the dots: Use @inheritDotParams when passing to a specific function, or clearly describe where the arguments go\nExclude irrelevant parameters: Use the -param_name syntax to exclude parameters that don’t apply\nValidate unexpected arguments: Consider using the {ellipsis} package to catch misspelled argument names:\nmy_function &lt;- function(x, y, ...) {\n  ellipsis::check_dots_used()\n  # function code\n}\nConsider alternatives: If you’re only passing a few specific arguments, it may be clearer to list them explicitly rather than using ...\n\nFor more details on @inheritDotParams, see the roxygen2 documentation on inheriting parameters.\n\n\n\n\n\n\nNote\n\n\n\nAs someone trying to call a function, it is possible to access a function’s documentation (and internal code) by CMD-Left-Clicking the function’s name in RStudio\n\n\n\n\n\n\n\n\nNote\n\n\n\nDepending on how important your function is, the complexity of your function code, and the complexity of different types of data in your project, you can also add “type-checking” to your function with the assertthat::assert_that() function. You can, for example, assert_that(is.data.frame(statistical_input)), which will ensure that collaborators or reviewers of your project attempting to use your function are using it in the way that it is intended by calling it with (at the minimum) the correct type of arguments. You can extend this to ensure that certain assumptions regarding the inputs are fulfilled as well (i.e. that time_column, location_column, value_column, and population_column all exist within the statistical_input tibble).\n\n\n\n\n6.10.3 Script headers\nEvery file in a project that doesn’t have roxygen function documentation should at least have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. You may optionally wish to include the inputs and outputs of the script as well, though the next section makes this significantly less necessary.\n################################################################################\n# @Organization - Example Organization\n# @Project - Example Project\n# @Description - This file is responsible for [...]\n################################################################################\n\n\n\n6.10.4 Sections and subsections\nRstudio (v1.4 or more recent) supports the use of Sections and Subsections. You can easily navigate through longer scripts using the navigation pane in RStudio, as shown on the right below.\n# Section -------\n\n## Subsection -------\n\n### Sub-subsection -------\n\n\n6.10.5 Code folding\nConsider using RStudio’s code folding feature to collapse and expand different sections of your code. Any comment line with at least four trailing dashes (-), equal signs (=), or pound signs (#) automatically creates a code section. For example:\n\n\n6.10.6 Comments in the body of your code\nCommenting your code is an important part of reproducibility and helps document your code for the future. When things change or break, you’ll be thankful for comments. There’s no need to comment excessively or unnecessarily, but a comment describing what a large or complex chunk of code does is always helpful. See this file for an example of how to comment your code and notice that comments are always in the form of:\n# This is a comment -- first letter is capitalized and spaced away from the pound sign\nSee also Section 7.2 for function documentation style guidelines.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#object-naming",
    "href": "coding-practices.html#object-naming",
    "title": "6  R Coding Practices",
    "section": "6.11 Object naming",
    "text": "6.11 Object naming\nGenerally we recommend using nouns for objects and verbs for functions. This is because functions are performing actions, while objects are not.\nTry to make your variable names both more expressive and more explicit. Being a bit more verbose is useful and easy in the age of autocompletion! For example, instead of naming a variable vaxcov_1718, try naming it vaccination_coverage_2017_18. Similarly, flu_res could be named absentee_flu_residuals, making your code more readable and explicit.\n\nFor more help, check out Be Expressive: How to Give Your Variables Better Names\n\nWe recommend you use snake_case.\n\nBase R allows . in variable names and functions (such as read.csv()), but this goes against best practices for variable naming in many other coding languages. For consistency’s sake, snake_case has been adopted across languages, and modern packages and functions typically use it (i.e. readr::read_csv()). As a very general rule of thumb, if a package you’re using doesn’t use snake_case, there may be an updated version or more modern package that does, bringing with it the variety of performance improvements and bug fixes inherent in more mature and modern software.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may also see camelCase throughout the R code you come across. This is okay but not ideal – try to stay consistent across all your code with snake_case.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, it’s also worth noting there’s nothing inherently wrong with using . in variable names, just that it goes against style best practices that are cropping up in data science, so it’s worth getting rid of these bad habits now.\n\n\nSee also Section 7.10.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#function-calls",
    "href": "coding-practices.html#function-calls",
    "title": "6  R Coding Practices",
    "section": "6.12 Function calls",
    "text": "6.12 Function calls\nIn a function call, use “named arguments” and put each argument on a separate line to make your code more readable.\nHere’s an example of what not to do when calling the function a function calc_fluseas_mean (defined above):\nmean_Y = calc_fluseas_mean(flu_data, \"maari_yn\", FALSE)\nAnd here it is again using the best practices we’ve outlined:\nmean_Y &lt;- calc_fluseas_mean(\n  data = flu_data, \n  yname = \"maari_yn\",\n  silent = FALSE\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-here-package-practices",
    "href": "coding-practices.html#sec-here-package-practices",
    "title": "6  R Coding Practices",
    "section": "6.13 The here package",
    "text": "6.13 The here package\nThe here package is one great R package that helps multiple collaborators deal with the mess that is working directories within an R project structure. Let’s say we have an R project at the path /home/oski/Some-R-Project. My collaborator might clone the repository and work with it at some other path, such as /home/bear/R-Code/Some-R-Project. Dealing with working directories and paths explicitly can be a very large pain, and as you might imagine, setting up a Config with paths requires those paths to flexibly work for all contributors to a project. This is where the here package comes in and this a great vignette describing it.\nSee also Section 7.9 for code style guidelines on using the here package.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#readingsaving-data",
    "href": "coding-practices.html#readingsaving-data",
    "title": "6  R Coding Practices",
    "section": "6.14 Reading/Saving Data",
    "text": "6.14 Reading/Saving Data\n\n6.14.1 .RDS vs .RData Files\nOne of the most common ways to load and save data in Base R is with the load() and save() functions to serialize multiple objects in a single .RData file. The biggest problems with this practice include an inability to control the names of things getting loaded in, the inherent confusion this creates in understanding older code, and the inability to load individual elements of a saved file. For this, we recommend using the RDS format to save R objects.\n\n\n\n\n\n\nNote\n\n\n\nIf you have many related R objects you would have otherwise saved all together using the save function, the functional equivalent with RDS would be to create a (named) list containing each of these objects, and saving it.\n\n\n\n\n6.14.2 CSVs\nOnce again, the readr package as part of the Tidvyerse is great, with a much faster read_csv() than Base R’s read.csv(). For massive CSVs (&gt; 5 GB), you’ll find data.table::fread() to be the fastest CSV reader in any data science language out there. For writing CSVs, readr::write_csv() and data.table::fwrite() outclass Base R’s write.csv() by a significant margin as well.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#integrating-box-and-dropbox",
    "href": "coding-practices.html#integrating-box-and-dropbox",
    "title": "6  R Coding Practices",
    "section": "6.15 Integrating Box and Dropbox",
    "text": "6.15 Integrating Box and Dropbox\nBox and Dropbox are cloud-based file sharing systems that are useful when dealing with large files. When our scripts generate large output files, the files can slow down the workflow if they are pushed to GitHub. This makes collaboration difficult when not everyone has a copy of the file, unless we decide to duplicate files and share them manually. The files might also take up a lot of local storage. Box and Dropbox help us avoid these issues by automatically storing the files, reading data, and writing data back to the cloud.\nBox and Dropbox are separate platforms, but we can use either one to store and share files. To use them, we can install the packages that have been created to integrate Box and Dropbox into R. The set-up instructions are detailed below.\nMake sure to authenticate before reading and writing from either Box or Dropbox. The authentication commands should go in the configuration file; it only needs to be done once. This will prompt you to give your login credentials for Box and Dropbox and will allow your application to access your shared folders.\n\n6.15.1 Box\nFollow the instructions in this section to use the boxr package. Note that there are a few setup steps that need to be done on the box website before you can use the boxr package, explained here in the section “Creating an Interactive App.” This gets the authentication keys that must be put in box. Once that is done, add the authentication keys to your code in the configuration file, with box_auth(client_id = \"&lt;your_client_id&gt;\", client_secret = \"&lt;your_client_secret_id&gt;\"). It is also important to set the default working directory so that the code can reference the correct folder in box: box_setwd(&lt;folder_id&gt;). The folder ID is the sequence of digits at the end of the URL.\nFurther details can be found here.\n\n\n6.15.2 Dropbox\nFollow the instructions at this link to use the rdrop2 package. Similar to the boxr package, you must authenticate before reading and writing from Dropbox, which can be done by adding drop_auth() to the configuration file.\nSaving the authentication token is not required, although it may be useful if you plan on using Dropbox frequently. To do so, save the token with the following commands. Tokens are valid until they are manually revoked.\n# first time only\n# save the output of drop_auth to an RDS file\ntoken &lt;- drop_auth()\n# this token only has to be generated once, it is valid until revoked\nsaveRDS(token, \"/path/to/tokenfile.RDS\")\n\n# all future usages\n# to use a stored token, provide the rdstoken argument\ndrop_auth(rdstoken = \"/path/to/tokenfile.RDS\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-tidyverse",
    "href": "coding-practices.html#sec-tidyverse",
    "title": "6  R Coding Practices",
    "section": "6.16 Tidyverse",
    "text": "6.16 Tidyverse\nThroughout this document there have been references to the Tidyverse, but this section is to explicitly show you how to transform your Base R tendencies to Tidyverse (or Data.Table, Tidyverse’s performance-optimized competitor). For most of our work that does not utilize very large datasets, we recommend that you code in Tidyverse rather than Base R. Tidyverse is quickly becoming the gold standard in R data analysis and modern data science packages and code should use Tidyverse style and packages unless there’s a significant reason not to (i.e. big data pipelines that would benefit from Data.Table’s performance optimizations).\nThe package author has published R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023), which leans heavily on many Tidyverse packages and may be worth checking out.\nThe following list is not exhaustive, but is a compact overview to begin to translate Base R into something better:\n\n\n\n\n\n\n\nBase R\nBetter Style, Performance, and Utility\n\n\n\n\n_\n_\n\n\nread.csv()\nreadr::read_csv() or data.table::fread()\n\n\nwrite.csv()\nreadr::write_csv() or data.table::fwrite()\n\n\nreadRDS\nreadr::read_rds()\n\n\nsaveRDS()\nreadr::write_rds()\n\n\n_\n_\n\n\ndata.frame()\ntibble::tibble() or tibble::tribble()\n\n\nrbind()\ndplyr::bind_rows()\n\n\ncbind()\ndplyr::bind_cols()\n\n\ndf$some_column\ndf |&gt; dplyr::pull(some_column)\n\n\ndf$some_column = ...\ndf |&gt; dplyr::mutate(some_column = ...)\n\n\ndf[get_rows_condition,]\ndf |&gt; dplyr::filter(get_rows_condition)\n\n\ndf[,c(col1, col2)]\ndf |&gt; dplyr::select(col1, col2)\n\n\nmerge(df1, df2, by = ..., all.x = ..., all.y = ...)\ndf1 |&gt; dplyr::left_join(df2, by = ...) or dplyr::full_join or dplyr::inner_join or dplyr::right_join\n\n\n_\n_\n\n\nstr()\ndplyr::glimpse()\n\n\ngrep(pattern, x)\nstringr::str_which(string, pattern)\n\n\ngsub(pattern, replacement, x)\nstringr::str_replace(string, pattern, replacement)\n\n\nifelse(test_expression, yes, no)\nif_else(condition, true, false)\n\n\nNested: ifelse(test_expression1, yes1, ifelse(test_expression2, yes2, ifelse(test_expression3, yes3, no)))\ncase_when(test_expression1 ~ yes1,  test_expression2 ~ yes2, test_expression3 ~ yes3, TRUE ~ no)\n\n\nproc.time()\ntictoc::tic() and tictoc::toc()\n\n\nstopifnot()\nassertthat::assert_that() or assertthat::see_if() or assertthat::validate_that()\n\n\n_\n_\n\n\nsessionInfo()\nsessioninfo::session_info()\n\n\n\nFor a more extensive set of syntactical translations to Tidyverse, you can check out this document.\nWorking with Tidyverse within functions can be somewhat of a pain due to non-standard evaluation (NSE) semantics. If you’re an avid function writer, we’d recommend checking out the following resources:\n\nTidy Eval in 5 Minutes (video)\nTidy Evaluation (e-book)\nEvaluation (advanced details)\nData Frame Columns as Arguments to Dplyr Functions (blog)\nStandard Evaluation for *_join (stackoverflow)\nProgramming with dplyr (package vignette)\n\nSee also Section 7.8",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#coding-with-r-and-python",
    "href": "coding-practices.html#coding-with-r-and-python",
    "title": "6  R Coding Practices",
    "section": "6.17 Coding with R and Python",
    "text": "6.17 Coding with R and Python\nIf you’re using both R and Python, you may wish to check out the Feather package for exchanging data between the two languages extremely quickly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#repeating-analyses-with-different-variations",
    "href": "coding-practices.html#repeating-analyses-with-different-variations",
    "title": "6  R Coding Practices",
    "section": "6.18 Repeating analyses with different variations",
    "text": "6.18 Repeating analyses with different variations\nIn many cases, we will need to apply our modeling on different combinations of interests (outcomes, exposures, etc.). We can certainly use a for loop to repeat the execution of a wrapper function, but generally, for loops request high memory usage and produce the results in long computation time.\nFortunately, R has some functions which implement looping in a compact form to help repeating your analyses with different variations (subgroups, outcomes, covariate sets, etc.) with better performances.\n\n6.18.1 lapply() and sapply()\nlapply() is a function in the base R package that applies a function to each element of a list and returns a list. It’s typically faster than for. Here is a simple generic example:\nresult &lt;- lapply(X = mylist, FUN = func)\nThere is another very similar function called sapply(). It also takes a list as its input, but if the output of the func is of the same length for each element in the input list, then sapply() will simplify the output to the simplest data structure possible, which will usually be a vector.\n\n\n6.18.2 mapply() and pmap()\nSometimes, we’d like to employ a wrapper function that takes arguments from multiple different lists/vectors. Then, we can consider using mapply() from the base R package or pmap() from the purrr package.\nPlease see the simple specific example below where the two input lists are of the same length and we are doing a pairwise calculation:\nmylist1 = list(0:3)\nmylist2 = list(6:9)\nmylists = list(mylist1, mylist2)\n\nsquare_sum &lt;- function(x, y) {\n  x^2 + y^2\n}\n\n#Use `mapply()`\nresult1 &lt;- mapply(FUN = square_sum, mylist1, mylist2)\n\n#Use `pmap()`\nlibrary(purrr)\nresult2 &lt;- pmap(.l = mylists, .f = square_sum)\n\n#unlist(as.list(result1)) = result2 = [36 50 68 90]\n\nThere are two major differences between mapply() and pmap(). The first difference is that mapply() takes seperate lists as its input arguments, while pmap() takes a list of list. Secondly, the output of mapply() will be in the form of a matrix or an array, but pmap() produces a list directly.\nHowever, when the input lists are of different lengths AND/OR the wrapper function doesn’t take arguments in pairs, mapply() and pmap() may not give the preferable results.\nBoth mapply() and pmap() will recycle shorter input lists to match the length of the longest input list. Assume that now mylist2 = list(6:12). Then, pmap(mylists, square_sum) will generate [36  50  68  90 100 122 148] where elements 0, 1, and 2 are recycled to match 10, 11, and 12. And it will return an error message that “longer object length is not a multiple of shorter object length.”\nThus, unless the recycling pattern described above is desirable feature for a certain experiment design, when the input lists are of different lengths, the best practice is probably to use lapply() and then combine the results.\nHere is an example where we’d like to find the square_sum for every element combination of mylist1 and mylist2.\nmylist1 &lt;- list(0:3)\nmylist2 &lt;- list(6:12)\n\nsquare_sum &lt;- function(x, y) {\n  x^2 + y^2\n}\n\nresults &lt;- list()\n\nfor (i in seq_along(mylist1[[1]])) {\n  result &lt;- lapply(X = mylist2, FUN = function(y) square_sum(mylist1[[1]][i], y))\n  results[[i]] &lt;- result\n}\n\nThis example doesn’t work in the way that 0 is paired to 6, 1 is paired to 7, and so on. Instead, every element in mylist1 will be paired with every element in mylist2. Thus, the “unlisted” results from the example will have \\(4*7 = 28\\) elements.\nWe can use flatten() or unlist() functions to decrease the depths of our results. If the results are data frames, then we will need to use bind_rows() to combine them.\n\n\n6.18.3 Parallel processing with parallel and future packages\nOne big drawback of lapply() is its long computation time, especially when the list length is long. Fortunately, computers nowadays must have multiple cores which makes parallel processing possible to help make computation much faster.\nAssume you have a list called mylist of length 1000, and lapply(X = mylist, FUN = func) applies the function to each of the 1000 elements one by one in \\(T\\) seconds. If we could execute the func in \\(n\\) processors simultaneously, then ideally, we would shrink the computation time to \\(T/n\\) seconds.\nIn practice, using functions under the parallel and the future packages, we can split mylist into smaller chunks and apply the function to each element of the several chunks in parallel in different cores to significantly reduce the run time.\n\n6.18.3.1 parLapply()\nBelow is a generic example of parLapply():\nlibrary(parallel)\n\n# Set how many processors will be used to process the list and make cluster\nn_cores &lt;- 4\ncl &lt;- makeCluster(n_cores)\n\n#Use parLapply() to apply func to each element in mylist\nresult &lt;- parLapply(cl = cl, x = mylist, FUN = func)\n\n#Stop the parallel processing\nstopCluster(cl)\nLet’s still assume mylist is of length 1000. The parLapply above splits mylist into 4 sub-lists each of length 250 and applies the function to the elements of each sub-list in parallel. To be more specific, first apply the function to element 1, 251, 501, 751; second apply to element 2, 252, 502, 752; so on and so forth. As such, the computation time will be greatly reduced.\nYou can use parallel::detectCores() to test how many cores your machine has and to help decide what to put for n_cores. It would be a good idea to leave at least one core free for the operating system to use.\nWe will always start parLapply() with makeCluster(). stopCluster() is not fully necessary but follows the best practices. If not stopped, the processing will continue in the back end and consuming the computation capacity for other software in your machine. But keep in mind that stopping the cluster is similar quitting R, meaning that you will need to re-load the packages needed when you need to do parallel processing use parLapply() again.\n\n\n6.18.3.2 future.lapply()\nBelow is a generic example of future.lapply():\nlibrary(future)\nlibrary(future.apply)\n\n# First, plan how the future_lapply() will be resolved\nfuture::plan(\n  multisession, workers = future::availableCores() - 1\n)\n\n# Use future_lapply() to apply func to each element in mylist\nfuture_lapply(x = mylist, FUN = func)\nHere, future::availableCores() checks how many cores your machine has. Similar to parLapply() showed above, future_lapply() parallelizes the computation of lapply() by executing the function func simultaneously on different sub-lists of mylist.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#reviewing-code",
    "href": "coding-practices.html#reviewing-code",
    "title": "6  R Coding Practices",
    "section": "6.19 Reviewing Code",
    "text": "6.19 Reviewing Code\nBefore publishing new changes, it is important to ensure that the code has been tested and well-documented. GitHub makes it possible to document all of these changes in a pull request. Pull requests can be used to describe changes in a branch that are ready to be merged with the base branch (more information in the GitHub section).\nThis section provides guidance on both constructing effective pull requests and reviewing code submitted by others. Much of the content in this section is adapted from the Tidyverse code review principles (Tidyverse Team 2023), which provides excellent principles for code review in R package development.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#constructing-pull-requests",
    "href": "coding-practices.html#constructing-pull-requests",
    "title": "6  R Coding Practices",
    "section": "6.20 Constructing Pull Requests",
    "text": "6.20 Constructing Pull Requests\n\n6.20.1 Write Focused PRs\nA focused pull request is one self-contained change that addresses just one thing. Writing focused PRs has several benefits:\n\nFaster reviews: It’s easier for a reviewer to find 5-10 minutes to review a single bug fix than to set aside an hour for one large PR implementing many features.\nMore thorough reviews: Large PRs with many changes can overwhelm reviewers, leading to important points being missed.\nFewer bugs: Smaller changes make it easier to reason about impacts and identify potential issues.\nEasier to merge: Large PRs take longer and are more likely to have merge conflicts.\nLess wasted work: If the overall direction is wrong, you’ve wasted less time on a small PR.\n\nAs a guideline, 100 lines is usually a reasonable size for a PR, and 1000 lines is usually too large. However, the number of files affected also matters—a 200-line change in one file might be fine, but the same change spread across 50 files is usually too large.\n\n\n6.20.2 Writing PR Descriptions\nWhen you submit a pull request, include a detailed PR title and description. A comprehensive description helps your reviewer and provides valuable historical context.\nPR Title: The title should be a short summary (ideally under 72 characters) of what is being done. It should be informative enough that future developers can understand what the PR did without reading the full description.\nPoor titles that lack context:\n\n“Fix bug”\n“Add patch”\n“Moving code from A to B”\n\nBetter titles that summarize the actual change:\n\n“Fix missing value handling in data processing function”\n“Add support for custom date formats in import functions”\n\nPR Description Body: The description should provide context that helps the reviewer understand your PR. Consider including:\n\nA brief description of the problem being solved\nLinks to related issues (e.g., “Closes #123” or “Related to #456”)\nA before/after example showing changed behavior\nPossible shortcomings of the approach being used\nFor complex PRs, a suggested reading order for the reviewer\nThe Files tab of a Pull Request page on GitHub allows you to annotate your pull request with inline comments. These comments are not part of the source files; they only exist in GitHub’s metadata. Use these comments to explain changes whose reasoning might not be self-apparent to a reviewer.\n\n\n\n6.20.3 Add Tests\nFocused PRs should include related test code. A PR that adds or changes logic should be accompanied by new or updated tests for the new behavior. Pure refactoring PRs should also be covered by tests—if tests don’t exist for code you’re refactoring, add them in a separate PR first to validate that behavior is unchanged.\n\n\n6.20.4 Separate Out Refactorings\nIt’s usually best to do refactorings in a separate PR from feature changes or bug fixes. For example, moving and renaming a function should be in a different PR from fixing a bug in that function. This makes it much easier for reviewers to understand the changes introduced by each PR.\nSmall cleanups (like fixing a local variable name) can be included in a feature change or bug fix PR, but large refactorings should be separate.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#reviewing-pull-requests",
    "href": "coding-practices.html#reviewing-pull-requests",
    "title": "6  R Coding Practices",
    "section": "6.21 Reviewing Pull Requests",
    "text": "6.21 Reviewing Pull Requests\n\n6.21.1 Purpose of Code Review\nThe primary purpose of code review is to ensure that the overall code health of our projects improves over time. Reviewers should balance the need to make forward progress with the importance of maintaining code quality.\nKey principle: Reviewers should favor approving a PR once it is in a state where it definitely improves the overall code health of the system, even if the PR isn’t perfect. There is no such thing as “perfect” code—there is only better code. Rather than seeking perfection, seek continuous improvement.\n\n\n6.21.2 Monitoring PRs Awaiting Your Review\nTo ensure timely code reviews, bookmark GitHub’s review-requested page and check it regularly (at least daily):\n\nGeneral bookmark: https://github.com/pulls/review-requested shows all PRs across GitHub where you’ve been requested as a reviewer\nProject-specific bookmark: For frequently-reviewed repositories, you can bookmark project-specific versions using GitHub’s search syntax. For example, to see PRs awaiting your review in this repository: https://github.com/UCD-SERG/lab-manual/pulls/review-requested/YOUR-USERNAME (replace YOUR-USERNAME with your GitHub username)\n\nChecking these pages regularly helps ensure that PRs don’t languish waiting for review, which is important for maintaining team productivity and code quality.\n\n\n6.21.3 Writing Review Comments\nWhen reviewing code, maintain courtesy and respect while being clear and helpful:\n\nComment on the code, not the author\nExplain why you’re making suggestions (reference best practices, design patterns, or how the suggestion improves code health)\nBalance pointing out problems with providing guidance (help authors learn while being constructive)\nHighlight positive aspects too—if you see good practices, comment on those to reinforce them\n\nPoor comment: “Why did you use this approach when there’s obviously a better way?”\nBetter comment: “This approach adds complexity without clear benefits. Consider using [alternative approach] instead, which would simplify the logic and improve readability.”\n\n\n6.21.4 Mentoring Through Review\nCode review is an excellent opportunity for mentoring. As a reviewer:\n\nLeave comments that help authors learn something new\nLink to relevant sections of style guides or best practices documentation\nConsider pair programming for complex reviews—live review sessions can be very effective for teaching\n\n\n\n6.21.5 Giving Constructive Feedback\nIn general, it is the author’s responsibility to fix a PR, not the reviewer’s. Strike a balance between pointing out problems and providing direct guidance. Sometimes pointing out issues and letting the author decide on a solution helps them learn and may result in a better solution since they are closer to the code.\nFor very small tweaks (typos, comment additions), use GitHub’s suggestion feature to allow authors to quickly accept changes directly in the UI.\n\n\n\n\n\n\nFigure 6.1: GitHub’s suggestion feature in a PR review comment\n\n\n\n\n\n6.21.6 Ignoring Auto-Generated Files\nWhen reviewing pull requests in R package repositories, you can typically ignore changes to .Rd files in the man/ directory. These are R documentation files automatically generated by {roxygen2} from special comments in the R source code (see Section 6.10).\nWhy ignore .Rd files?\n\nThey are auto-generated and should never be manually edited\nChanges to .Rd files simply reflect changes already visible in the roxygen2 comments\nReviewing the source roxygen2 comments is more informative and efficient\nThe .Rd files will be regenerated during the package build process\n\nWhat to review instead:\nFocus your review on the roxygen2 documentation comments in the actual R source files (.R files in the R/ directory). These special comments start with #' and appear immediately before function definitions. Any changes to function documentation will be visible there.\nIf the repository has a preview workflow (such as pkgdown for R packages or Quarto for documentation sites), you can also review the rendered documentation in the preview build. The preview workflow should automatically post a comment on the PR containing a link to a preview version of the revised documentation.\n\n\n\n\n\n\nFigure 6.2: Example of an automated PR preview comment posted by GitHub Actions\n\n\n\nGitHub review tip:\nIn GitHub’s pull request “Files changed” view, you can click the three dots (...) next to a file and select “View file” to hide it from the diff view. This helps you focus on the meaningful changes.\n\n\n6.21.7 Reviewing Copilot-Generated Pull Requests\nWhen reviewing pull requests created by GitHub Copilot coding agents, apply the same standards and principles as any other PR, but be aware of some unique considerations:\nWorkflow approval requirements:\n\nYou must manually approve GitHub Actions workflows for Copilot PRs\nThis is a security measure because Copilot can modify any file, including workflow files themselves\nClick the approval button in the Actions tab or on the PR to trigger workflows\nThere is currently no way to bypass this manual approval, even if you are the repository owner\n\nReview focus areas:\n\nVerify the solution addresses the issue: Ensure Copilot understood the requirements correctly\nCheck for over-engineering: Copilot may sometimes add unnecessary complexity or features beyond what was requested\nReview test coverage: Verify that tests are appropriate and comprehensive\nCheck documentation: Ensure documentation is clear and follows project conventions\nLook for edge cases: AI-generated code may miss edge cases or error handling\n\nIterating on Copilot PRs:\nWhen you find issues in a Copilot PR, you have two options:\n\nRequest changes from Copilot: Leave review comments and ask Copilot to address them. This works well for complex changes or when you want to see how Copilot interprets your feedback.\nMake direct changes yourself: Push commits directly to the Copilot PR branch. This is often faster for simple fixes like typos, formatting, or small adjustments.\n\nFor quick fixes, you can often make changes faster than writing review comments and waiting for Copilot to respond.\nBest practices:\n\nDon’t push while Copilot is working: Wait for Copilot to complete its current iteration before pushing your own changes to avoid merge conflicts\nReview incrementally: If a Copilot PR is large, review it in stages as the agent updates it rather than waiting until the end\nTrust but verify: Copilot is a powerful tool, but human review is essential for catching issues and ensuring quality",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#creating-a-pull-request-template",
    "href": "coding-practices.html#creating-a-pull-request-template",
    "title": "6  R Coding Practices",
    "section": "6.22 Creating a Pull Request Template",
    "text": "6.22 Creating a Pull Request Template\nGitHub allows you to create a pull request template in a repository to standardize the information in pull requests. When you add a template, everyone will automatically see its contents in the pull request body.\nFollow these steps to add a pull request template:\n\nOn GitHub, navigate to the main page of the repository.\nAbove the file list, click Create new file.\nName the file pull_request_template.md. GitHub will not recognize this as the template if it is named anything else. The file must be on the default branch.\n\nTo store the file in a hidden directory, name it .github/pull_request_template.md.\n\nIn the body of the new file, add your pull request template.\n\nHere is an example pull request template:\n# Description\n\n## Summary of change\n\nPlease include a summary of the change, including any new functions added and example usage.\n\n## Related Issues\n\nCloses #(issue number)\nRelated to #(issue number)\n\n## Testing\n\nDescribe how this change has been tested.\n\n## Checklist\n\n- [ ] Tests added/updated\n- [ ] Documentation updated\n- [ ] Code follows project style guidelines\n\n## Who should review the pull request?\n\n@username",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-practices.html#sec-r-resources",
    "href": "coding-practices.html#sec-r-resources",
    "title": "6  R Coding Practices",
    "section": "6.23 Additional Resources",
    "text": "6.23 Additional Resources\n\n6.23.1 R Package Development\n\nR Packages (Wickham and Bryan 2023) - comprehensive guide to R package development\nTidyverse design guide (Wickham 2023) - principles for designing R packages and APIs that are intuitive, composable, and consistent with tidyverse philosophy\nusethis documentation - workflow automation for R projects\ndevtools documentation - essential development tools\npkgdown documentation - create package websites\ntestthat documentation - unit testing framework\n\n\n\n6.23.2 General R Programming\n\nR for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) - learn data science with the tidyverse\nAdvanced R (Wickham 2019) - deep dive into R programming and internals\n\n\n\n6.23.3 Shiny Development\n\nMastering Shiny (Wickham 2021) - comprehensive guide to building web applications with Shiny\nEngineering Production-Grade Shiny Apps (Fay et al. 2021) - best practices for production Shiny applications\n\n\n\n6.23.4 Git and Version Control\n\nHappy Git and GitHub for the useR (Bryan 2023) - essential guide to using Git and GitHub with R\n\n\n\n\n\nBryan, Jennifer. 2023. Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\n\nFay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman; Hall/CRC. https://engineering-shiny.org/.\n\n\nTidyverse Team. 2023. Tidyverse Code Review Principles. https://code-review.tidyverse.org/.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Chapman; Hall/CRC. https://adv-r.hadley.nz/.\n\n\n———. 2021. Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/.\n\n\n———. 2023. Tidyverse Design Guide. https://design.tidyverse.org/.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages. 2nd ed. O’Reilly Media. https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster. 2024. Roxygen2: In-Line Documentation for r. https://roxygen2.r-lib.org/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Coding Practices</span>"
    ]
  },
  {
    "objectID": "coding-style.html",
    "href": "coding-style.html",
    "title": "7  R Code Style",
    "section": "",
    "text": "7.1 General Principles\nAdapted by UCD-SeRG team from original by Kunal Mishra, Jade Benjamin-Chung, and Stephanie Djajadi\nFollow these code style guidelines for all R code:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#general-principles",
    "href": "coding-style.html#general-principles",
    "title": "7  R Code Style",
    "section": "",
    "text": "Follow tidyverse style guide: https://style.tidyverse.org\nUse native pipe: |&gt; not %&gt;% (available in R &gt;= 4.1.0)\nNaming: Use snake_case for functions and variables; acronyms may be uppercase (e.g., prep_IDs_data)\nWrite tidy code: Keep code clean, readable, and well-organized\nAvoid redundant logical comparisons: Use logical variables directly in conditional statements (e.g., if (x) instead of if (x == TRUE) or if (x == 1))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#sec-function-docs",
    "href": "coding-style.html#sec-function-docs",
    "title": "7  R Code Style",
    "section": "7.2 Function Structure and Documentation",
    "text": "7.2 Function Structure and Documentation\nEvery function should follow this pattern:\n#' Short Title (One Line)\n#'\n#' Longer description providing details about what the function does,\n#' when to use it, and important considerations.\n#'\n#' @param param1 Description of first parameter, including type and constraints\n#' @param param2 Description of second parameter\n#'\n#' @returns Description of return value, including type and structure\n#'\n#' @examples\n#' # Example usage\n#' result &lt;- my_function(param1 = \"value\", param2 = 10)\n#'\n#' @export\nmy_function &lt;- function(param1, param2) {\n  # Implementation\n}\nSee also Section 6.10 for general code documentation practices.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#comments",
    "href": "coding-style.html#comments",
    "title": "7  R Code Style",
    "section": "7.3 Comments",
    "text": "7.3 Comments\nUse comments to explain why, not what:\n# Good: Explains reasoning\n# Use log scale because distribution is highly skewed\nggplot(data, aes(x = log10(income))) + geom_histogram()\n\n# Bad: States the obvious\n# Create a histogram\nggplot(data, aes(x = income)) + geom_histogram()\nFile headers (for scripts in data-raw/ or inst/analyses/):\n################################################################################\n# @Organization - Example Organization\n# @Project - Example Project\n# @Description - This file is responsible for [...]\n################################################################################\nFile Structure - Just as your data “flows” through your project, data should flow naturally through a script. Very generally, you want to\n\nsource your config =&gt;\nload all your data =&gt;\ndo all your analysis/computation =&gt; save your data.\n\nEach of these sections should be “chunked together” using comments. See this file for a good example of how to cleanly organize a file in a way that follows this “flow” and functionally separate pieces of code that are doing different things.\n\n\n\n\n\n\nNote\n\n\n\nIf your computer isn’t able to handle this workflow due to RAM or requirements, modifying the ordering of your code to accommodate it won’t be ultimately helpful and your code will be fragile, not to mention less readable and messy. You need to look into high-performance computing (HPC) resources in this case.\n\n\nSingle-Line Comments - Commenting your code is an important part of reproducibility and helps document your code for the future. When things change or break, you’ll be thankful for comments. There’s no need to comment excessively or unnecessarily, but a comment describing what a large or complex chunk of code does is always helpful. See this file for an example of how to comment your code and notice that comments are always in the form of:\n# This is a comment -- first letter is capitalized and spaced away from the pound sign\nMulti-Line Comments - Occasionally, multi-line comments are necessary. You should manually insert line breaks to “hard-wrap” code and comments, whenever lines become longer than 80 characters. lintr should object otherwise, even for comments. Try to break lines at semantic boundaries: ends of sentences or phrases. Long lines in source code files make it more difficult to see and comment on diffs in pull requests.\nIn prose text chunks, Quarto ignores single line breaks, so you should also line-break your prose text in .qmd files to keep them under 80 characters.\nYou can configure RStudio’s settings to display the 80-character margin.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#line-breaks-and-formatting",
    "href": "coding-style.html#line-breaks-and-formatting",
    "title": "7  R Code Style",
    "section": "7.4 Line Breaks and Formatting",
    "text": "7.4 Line Breaks and Formatting\nBlank Lines Before Lists\nAlways include a blank line before starting a bullet list or numbered list in markdown/Quarto documents. This ensures proper rendering and readability.\nCorrect:\nHere are the requirements:\n\n- First item\n- Second item\nIncorrect:\nHere are the requirements:\n- First item\n- Second item\nHere’s what happens if you don’t add the blank line:\nHere are the requirements: - First item - Second item\nLine Breaks in Code\n\nFor ggplot calls and dplyr pipelines, do not crowd single lines. Here are some nontrivial examples of “beautiful” pipelines, where beauty is defined by coherence:\n\n# Example 1\nschool_names = list(\n  OUSD_school_names = absentee_all |&gt;\n    filter(dist.n == 1) |&gt;\n    pull(school) |&gt;\n    unique |&gt;\n    sort,\n\n  WCCSD_school_names = absentee_all |&gt;\n    filter(dist.n == 0) |&gt;\n    pull(school) |&gt;\n    unique |&gt;\n    sort\n)\n# Example 2\nabsentee_all = fread(file = raw_data_path) |&gt;\n  mutate(program = case_when(schoolyr %in% pre_program_schoolyrs ~ 0,\n                             schoolyr %in% program_schoolyrs ~ 1)) |&gt;\n  mutate(period = case_when(schoolyr %in% pre_program_schoolyrs ~ 0,\n                            schoolyr %in% LAIV_schoolyrs ~ 1,\n                            schoolyr %in% IIV_schoolyrs ~ 2)) |&gt;\n  filter(schoolyr != \"2017-18\")\nAnd of a complex ggplot call:\n# Example 3\nggplot(data=data) +\n  \n  aes(x=.data[[\"year\"]], y=.data[[\"rd\"]], group=.data[[group]]) +\n\n  geom_point(mapping = aes(col = .data[[group]], shape = .data[[group]]),\n             position=position_dodge(width=0.2),\n             size=2.5) +\n\n  geom_errorbar(mapping = aes(ymin=.data[[\"lb\"]], ymax= .data[[\"ub\"]], col= .data[[group]]),\n                position=position_dodge(width=0.2),\n                width=0.2) +\n\n  geom_point(position=position_dodge(width=0.2),\n             size=2.5) +\n\n  geom_errorbar(mapping=aes(ymin=lb, ymax=ub),\n                position=position_dodge(width=0.2),\n                width=0.1) +\n\n  scale_y_continuous(limits=limits,\n                     breaks=breaks,\n                     labels=breaks) +\n\n  scale_color_manual(std_legend_title,values=cols,labels=legend_label) +\n  scale_shape_manual(std_legend_title,values=shapes, labels=legend_label) +\n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  xlab(\"Program year\") +\n  ylab(yaxis_lab) +\n  theme_complete_bw() +\n  theme(strip.text.x = element_text(size = 14),\n        axis.text.x = element_text(size = 12)) +\n  ggtitle(title)\nImagine (or perhaps mournfully recall) the mess that can occur when you don’t strictly style a complicated ggplot call. Trying to fix bugs and ensure your code is working can be a nightmare. Now imagine trying to do it with the same code 6 months after you’ve written it. Invest the time now and reap the rewards as the code practically explains itself, line by line.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#markdown-and-quarto-formatting",
    "href": "coding-style.html#markdown-and-quarto-formatting",
    "title": "7  R Code Style",
    "section": "7.5 Markdown and Quarto Formatting",
    "text": "7.5 Markdown and Quarto Formatting\n\n7.5.1 Writing about code in Quarto documents\nWhen writing about code in prose sections of quarto documents, use backticks to apply a code style: for example, dplyr::mutate(). When talking about packages, use backticks and curly-braces with a hyperlink to the package website. For example: {dplyr}.\nImportant: Do not use raw HTML (&lt;a href=\"...\"&gt;) in .qmd files. Always use Quarto/markdown link syntax instead.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#messaging-and-user-communication",
    "href": "coding-style.html#messaging-and-user-communication",
    "title": "7  R Code Style",
    "section": "7.6 Messaging and User Communication",
    "text": "7.6 Messaging and User Communication\nUse cli package functions for all user-facing messages in package functions:\n# Good\ncli::cli_inform(\"Analysis complete\")\ncli::cli_warn(\"Missing data detected\")\ncli::cli_abort(\"Invalid input: {x}\")\n\n# Bad - don't use these in package code\nmessage(\"Analysis complete\")\nwarning(\"Missing data detected\")\nstop(\"Invalid input\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#package-code-practices",
    "href": "coding-style.html#package-code-practices",
    "title": "7  R Code Style",
    "section": "7.7 Package Code Practices",
    "text": "7.7 Package Code Practices\n\nNo library() in package code: Use :: notation or declare in DESCRIPTION Imports\nDocument all exports: Use roxygen2 (@title, @description, @param, @returns, @examples)\nAvoid code duplication: Extract repeated logic into helper functions",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#sec-tidyverse-replacements",
    "href": "coding-style.html#sec-tidyverse-replacements",
    "title": "7  R Code Style",
    "section": "7.8 Tidyverse Replacements",
    "text": "7.8 Tidyverse Replacements\nUse modern tidyverse/alternatives for base R functions:\n# Data structures\ntibble::tibble()           # instead of data.frame()\ntibble::tribble()          # instead of manual data.frame creation\n\n# I/O\nreadr::read_csv()          # instead of read.csv()\nreadr::write_csv()         # instead of write.csv()\nreadr::read_rds()          # instead of readRDS()\nreadr::write_rds()         # instead of saveRDS()\n\n# Data manipulation\ndplyr::bind_rows()         # instead of rbind()\ndplyr::bind_cols()         # instead of cbind()\n\n# String operations\nstringr::str_which()       # instead of grep()\nstringr::str_replace()     # instead of gsub()\n\n# Date/time operations\nlubridate::NA_Date_        # instead of as.Date(NA)\n\n# Session info\nsessioninfo::session_info() # instead of sessionInfo()\nSee also Section 6.16.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#sec-here-package-style",
    "href": "coding-style.html#sec-here-package-style",
    "title": "7  R Code Style",
    "section": "7.9 The here Package",
    "text": "7.9 The here Package\nThe here package helps manage file paths in projects by automatically finding the project root and building paths relative to it:\nlibrary(here)\n\n# Automatically finds project root and builds paths\ndata &lt;- readr::read_csv(here(\"data-raw\", \"survey.csv\"))\nsaveRDS(results, here(\"inst\", \"analyses\", \"results.rds\"))\nThis solves the problem of different working directory paths across collaborators. For example, one person might have the project at /home/oski/Some-R-Project while another has it at /home/bear/R-Code/Some-R-Project. The here package handles this automatically.\nThis works regardless of where collaborators clone the repository. For more details, see the here package vignette.\nSee also Section 6.13 for detailed explanation of the here package.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#sec-object-naming",
    "href": "coding-style.html#sec-object-naming",
    "title": "7  R Code Style",
    "section": "7.10 Object Naming",
    "text": "7.10 Object Naming\nUse descriptive names that are both expressive and explicit. Being verbose is useful and easy in the age of autocompletion:\n# Good\nvaccination_coverage_2017_18\nabsentee_flu_residuals\n\n# Less good\nvaxcov_1718\nflu_res\n\nPrefer nouns for objects and verbs for functions:\n# Good\nclean_data &lt;- prep_study_data(raw_data)  # verb for function, noun for object\n\n# Less clear\ndata &lt;- process(input)\nGenerally we recommend using nouns for objects and verbs for functions. This is because functions are performing actions, while objects are not.\n\nUse snake_case for all variable and function names. Avoid using . in names (as in base R’s read.csv()), as this goes against best practices in modern R and other languages. Modern packages like readr::read_csv() follow this convention.\nTry to make your variable names both more expressive and more explicit. Being a bit more verbose is useful and easy in the age of autocompletion! For example, instead of naming a variable vaxcov_1718, try naming it vaccination_coverage_2017_18. Similarly, flu_res could be named absentee_flu_residuals, making your code more readable and explicit.\nBase R allows . in variable names and functions (such as read.csv()), but this goes against best practices for variable naming in many other coding languages. For consistency’s sake, snake_case has been adopted across languages, and modern packages and functions typically use it (i.e. readr::read_csv()). As a very general rule of thumb, if a package you’re using doesn’t use snake_case, there may be an updated version or more modern package that does, bringing with it the variety of performance improvements and bug fixes inherent in more mature and modern software.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may also see camelCase throughout the R code you come across. This is okay but not ideal – try to stay consistent across all your code with snake_case.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, it’s also worth noting there’s nothing inherently wrong with using . in variable names, just that it goes against style best practices that are cropping up in data science, so it’s worth getting rid of these bad habits now.\n\n\n\nFor more help, check out Be Expressive: How to Give Your Variables Better Names",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#sec-style-auto-style",
    "href": "coding-style.html#sec-style-auto-style",
    "title": "7  R Code Style",
    "section": "7.11 Automated Tools for Style and Project Workflow",
    "text": "7.11 Automated Tools for Style and Project Workflow\n\n7.11.1 Styling\n\n7.11.1.1 RStudio shortcuts\n\nCode Autoformatting - RStudio includes a fantastic built-in utility (keyboard shortcut: CMD-Shift-A (Mac) or Ctrl-Shift-A (Windows/Linux)) for autoformatting highlighted chunks of code to fit many of the best practices listed here. It generally makes code more readable and fixes a lot of the small things you may not feel like fixing yourself. Try it out as a “first pass” on some code of yours that doesn’t follow many of these best practices!\nAssignment Aligner - A cool R package allows you to very powerfully format large chunks of assignment code to be much cleaner and much more readable. Follow the linked instructions and create a keyboard shortcut of your choosing (recommendation: CMD-Shift-Z). Here is an example of how assignment aligning can dramatically improve code readability:\n\n# Before\nOUSD_not_found_aliases = list(\n  \"Brookfield Village Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Brookfield\"),\n  \"Carl Munck Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Munck\"),\n  \"Community United Elementary School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Community United\"),\n  \"East Oakland PRIDE Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"East Oakland Pride\"),\n  \"EnCompass Academy\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"EnCompass\"),\n  \"Global Family School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Global\"),\n  \"International Community School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"International Community\"),\n  \"Madison Park Lower Campus\" = \"Madison Park Academy TK-5\",\n  \"Manzanita Community School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Manzanita Community\"),\n  \"Martin Luther King Jr Elementary\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"King\"),\n  \"PLACE @ Prescott\" = \"Preparatory Literary Academy of Cultural Excellence\",\n  \"RISE Community School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Rise Community\")\n)\n# After\nOUSD_not_found_aliases = list(\n  \"Brookfield Village Elementary\"      = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Brookfield\"),\n  \"Carl Munck Elementary\"              = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Munck\"),\n  \"Community United Elementary School\" = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Community United\"),\n  \"East Oakland PRIDE Elementary\"      = str_subset(string = OUSD_school_shapes$schnam, pattern = \"East Oakland Pride\"),\n  \"EnCompass Academy\"                  = str_subset(string = OUSD_school_shapes$schnam, pattern = \"EnCompass\"),\n  \"Global Family School\"               = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Global\"),\n  \"International Community School\"     = str_subset(string = OUSD_school_shapes$schnam, pattern = \"International Community\"),\n  \"Madison Park Lower Campus\"          = \"Madison Park Academy TK-5\",\n  \"Manzanita Community School\"         = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Manzanita Community\"),\n  \"Martin Luther King Jr Elementary\"   = str_subset(string = OUSD_school_shapes$schnam, pattern = \"King\"),\n  \"PLACE @ Prescott\"                   = \"Preparatory Literary Academy of Cultural Excellence\",\n  \"RISE Community School\"              = str_subset(string = OUSD_school_shapes$schnam, pattern = \"Rise Community\")\n)\n\n\n7.11.1.2 {styler}\n{styler} is another cool R package from the Tidyverse that can be powerful and used as a first pass on entire projects that need refactoring. The most useful function of the package is the style_dir function, which will style all files within a given directory. See the function’s documentation and the vignette linked above for more details.\n\n\n\n\n\n\nNote\n\n\n\nThe default Tidyverse styler is subtly different from some of the things we’ve advocated for in this document. Most notably we differ with regards to the assignment operator (&lt;- vs =) and number of spaces before/after “tokens” (i.e. Assignment Aligner add spaces before = signs to align them properly). For this reason, we’d recommend the following: style_dir(path = ..., scope = \"line_breaks\", strict = FALSE). You can also customize {styler} even more if you’re really hardcore.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs is mentioned in the package vignette linked above, {styler} modifies things in-place, meaning it overwrites your existing code and replaces it with the updated, properly styled code. This makes it a good fit on projects with version control, but if you don’t have backups or a good way to revert back to the initial code, I wouldn’t recommend going this route.\n\n\n\n\n\n\n\n\nTipstyler Package\n\n\n\nFor automated styling of entire projects:\n# Install styler\ninstall.packages(\"styler\")\n\n# Style all files in R/ directory\nstyler::style_dir(\"R/\")\n\n# Style entire package\nstyler::style_pkg()\n\n# Note: styler modifies files in-place\n# Always use with version control so you can review changes\n\n\n\n\n7.11.1.3 {lintr}\nLinters are programming tools that check adherence to a given style, syntax errors, and possible semantic issues. The R linter, called lintr, can be found in this package. It helps keep files consistent across different authors and even different organizations. For example, it notifies you if you have unused variables, global variables with no visible binding, not enough or superfluous whitespace, and improper use of parentheses or brackets. A list of its other purposes can be found in this link, and most guidelines are based on the Tidyverse R Style Guide.\n\n\n\n\n\n\nNote\n\n\n\nYou can customize your settings to set defaults or to exclude files. More details can be found here.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lintr package goes hand in hand with the styler package. The styler can be used to automatically fix the problems that the lintr catches.\n\n\n\n\n\n\n\n\nTiplintr package\n\n\n\nFor checking code style without modifying files:\n# Install lintr\ninstall.packages(\"lintr\")\n\n# Lint the entire package\nlintr::lint_package()\n\n# Lint a specific file\nlintr::lint(\"R/my_function.R\")\nThe linter checks for:\n\nUnused variables\nImproper whitespace\nLine length issues\nStyle guide violations\n\nYou can customize linting rules by creating a .lintr or lintr.R file in your project root.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "coding-style.html#sec-r-resources-style",
    "href": "coding-style.html#sec-r-resources-style",
    "title": "7  R Code Style",
    "section": "7.12 Additional Resources",
    "text": "7.12 Additional Resources\n\nTidyverse style guide (Wickham 2023): Detailed coding style conventions for writing clear, consistent R code. Covers naming, syntax, pipes, functions, and more.\n\n\n\n\n\nWickham, Hadley. 2023. The Tidyverse Style Guide. https://style.tidyverse.org/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Code Style</span>"
    ]
  },
  {
    "objectID": "big-data.html",
    "href": "big-data.html",
    "title": "8  Big data",
    "section": "",
    "text": "8.1 The data.table package\nAdapted by UCD-SeRG team from original by Kunal Mishra and Jade Benjamin-Chung\nIt may also be the case that you’re working with very large datasets. Generally I would define this as 10+ million rows. As is outlined in this document, the 3 main players in the data analysis space are Base R, Tidvyerse (more specificially, dplyr), and data.table. For a majority of things, Base R is inferior to both dplyr and data.table, with concise but less clear syntax and less speed. Dplyr is architected for medium and smaller data, and while its very fast for everyday usage, it trades off maximum performance for ease of use and syntax compared to data.table. An overview of the dplyr vs data.table debate can be found in this stackoverflow post and all 3 answers are worth a read.\nYou can also achieve a performance boost by running dplyr commands on data.tables, which I find to be the best of both worlds, given that a data.table is a special type of data.frame and fairly easy to convert with the as.data.table() function. The speedup is due to dplyr’s use of the data.table backend and in the future this coupling should become even more natural.\nIf you want to test whether using a certain coding approach increases speed, consider the tictoc package. Run tic() before a code chunk and toc() after to measure the amount of system time it takes to run the chunk. For example, you might use this to decide if you really need to switch a code chunk from dplyr to data.table.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Big data</span>"
    ]
  },
  {
    "objectID": "big-data.html#using-downsampled-data",
    "href": "big-data.html#using-downsampled-data",
    "title": "8  Big data",
    "section": "8.2 Using downsampled data",
    "text": "8.2 Using downsampled data\nIn our studies with very large datasets, we save “downsampled” data that usually includes a 1% random sample stratified by any important variables, such as year or household id. This allows us to efficiently write and test our code without having to load in large, slow datasets that can cause RStudio to freeze. Be very careful to be sure which dataset you are working with and to label results output accordingly.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Big data</span>"
    ]
  },
  {
    "objectID": "big-data.html#optimal-rstudio-set-up",
    "href": "big-data.html#optimal-rstudio-set-up",
    "title": "8  Big data",
    "section": "8.3 Optimal RStudio set up",
    "text": "8.3 Optimal RStudio set up\nUsing the following settings will help ensure a smooth experience when working with big data. In RStudio, go to the “Tools” menu, then select “Global Options”. Under “General”:\nWorkspace\n\nUncheck Restore RData into workspace at startup\nSave workspace to RData on exit – choose never\n\nHistory\n\nUncheck Always save history\n\nUnfortunately RStudio often gets slow and/or freezes after hours working with big datasets. Sometimes it is much more efficient to just use Terminal / gitbash to run code and make updates in git.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Big data</span>"
    ]
  },
  {
    "objectID": "data-masking.html",
    "href": "data-masking.html",
    "title": "9  Data masking",
    "section": "",
    "text": "9.1 General Overview\nAdapted by UCD-SeRG team from original by Anna Nguyen, Jade Benjamin-Chung, and Gabby Barratt Heitmann\nFor information about UC Davis computing resources for data-intensive work, see Chapter 16.\nThis chapter covers data masking, a unique process in R in which columns are treated as distinct objects within their dataframe’s environment. In our lab, data masking most frequently comes up when writing wrapper functions where arguments to indicate column names are supplied as strings. We often do this when we repeat the same code on multiple columns, and want to apply a function to a vector of strings that correspond to column names in a dataframe. For example, we might want to clean multiple columns using the same function or estimate the same model under different feature sets. Here, we try to break down what data masking is, why this error comes up, and common approaches to solve this problem.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data masking</span>"
    ]
  },
  {
    "objectID": "data-masking.html#general-overview",
    "href": "data-masking.html#general-overview",
    "title": "9  Data masking",
    "section": "",
    "text": "9.1.1 What is Data Masking?\nWithin certain tidyverse operations, columns are called as if they were variables. For example, while running df |&gt; mutate(X = …) R recognizes that X specifically references a column in df without explicitly stating its membership df |&gt; mutate(df$X = …) or calling the column name as a string df |&gt; mutate(\"X\" = …).\n\n\n\n\n\n\nFigure 9.1: Data masking in tidyverse operations\n\n\n\nHowever, this behavior may introduce errors when we attempt to incorporate variables from the global environment within these tidyverse pipelines. In the example shown in Figure 9.1, column_name = \"X\" followed by df |&gt; mutate(X2 = column_name + 1) would yield an error, since column_name is not a column in df and the variable column_name is not defined within the environment of df\n\n\n9.1.2 Using tidy evaluation for data masking\nIn dplyr-based R programming, we make use of tidy evaluation. This allows us to avoid using base R syntax to reference specific columns in a data frame. By leveraging Tidy evaluation-based data masking, we can employ long pipes with several dplyr verbs to manipulate our data using stand-alone variables that store column names as strings.\nFor example, consider a data frame “df” that contains a column called “heavyrain” that we want to manipulate. Suppose we wanted to convert the values of “heavyrain” into a factor.\nUsing base R, which does not mask data, heavyrain must have quotes to be treated as a data-variable:\ndf[[\"outcome\"]] = as.factor(df[[\"heavyrain\"]])\nIn a dplyr pipe, heavyrain is being masked using tidy evaluation and will be correctly interpreted as a column because it is recognized as a data-variable: df |&gt; mutate(outcome = as.factor(heavyrain))\nWith modified data masking, heavyrain is a string that is coerced into being recognized as a data-variable:\nvar_name = \"heavyrain\"\ndf |&gt; mutate(outcome = as.factor(!!sym(var_name)) \nWhile cleaner and often more convenient, the data frame that var_name is in is now “masked” and we refer to the vectors in the dataframe (data-variables) as though it is an object of its own (an environmental-variable). This is why we can just say the variable’s name in the context of a pipe – we treat it as though it’s an object defined in our environment. Within normal scripts, this is usually fine, because the data frame is “held on to” in the pipe. However, it can cause some programming hurdles when writing functions that take strings of variable/column names as arguments. In the next section, we briefly describe how to troubleshoot common errors in data masking, as relevant to our lab’s work.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data masking</span>"
    ]
  },
  {
    "objectID": "data-masking.html#technical-overview",
    "href": "data-masking.html#technical-overview",
    "title": "9  Data masking",
    "section": "9.2 Technical Overview",
    "text": "9.2 Technical Overview\nThis section covers the R functions and tools that we often use in the context of data masking, focusing on the bang bang operator (!!) with symbol coercion (sym()) and the Walrus operator (:=).\nThe combined use of !! and sym() allows us to use strings, rather than data-variables, to reference column names within dplyr. Together, !!sym(\"column_name\") forces dplyr to recognize “column_name” as a data-variable prior to evaluating the rest of the expression, enabling the ability to perform calculations on the column while referring to it as a string. sym() is a function that turns strings into symbols. In the context of a dplyr pipe, these symbols are interpreted as data-variables. The !! (bang bang) operator tells dplyr to evaluate the sym() expression first, e.g. to unquote its expression (e.g. “column_name”) and evaluate it as a pre-existing object, first. This is helpful because often we use sym(\"column_name\") within a larger expression, and dplyr might evaluate other elements of the expression first without !!, causing errors.\nWhen we want to create a new column (via mutate or summarize), the Walrus operator (:=) allows us to specify the new column’s name using a string. For example, while df |&gt; mutate(\"new_column\" = values) would yield an error, df |&gt; mutate(\"new_column\" := values) will correctly create a new column called “new_column”. If we want to use a variable representing a string, we can use !! to force the variable to be evaluated before using := to assign the value of the new column.\ncol_name = \"new_column\"\ndf |&gt; mutate(!!col_name := values)\n\n9.2.1 Example\nSuppose we want to write a function “generate_descriptive_table” to summarize how the prevalence of “outcome” varies under different levels of a “risk_factor” in a data frame “df”\nWe can start by writing the function shell:\ngenerate_descriptive_table &lt;- function (df, outcome, rf) {\noutcome_dist_by_rf &lt;- ….\nreturn(outcome_dist_by_rf)\n}\nNext, we can filter the data frame for only rows in which “rf” and “outcome” are not missing. We can use !! and sym() within filter to evaluate the strings stored in “rf” and “outcome”. Note that defining !!sym(outcome) or !!sym(outcome) in variables outside of the dplyr pipeline will not work.\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df |&gt; \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) |&gt;\n  ….\n  return(outcome_dist_by_rf)\n}\nSimilarly, we use !! and sym() in group_by to evaluate column name, stored as a string in the argument “rf”\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df |&gt; \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) |&gt;\n  ….\n  return(outcome_dist_by_rf)\n}\nFinally, we can use the walrus operator, !! and sym() with “summarize” to create a new column that takes the mean of the column referenced in “rf”. We also use “glue” or “paste” to give the new column an informative name that includes the “outcome” it describes.\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df |&gt; \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) |&gt; \n  group_by(!!sym(rf)) |&gt;\n  summarize(!!(glue::glue(\"{outcome}_prev\")) := mean(!!sym(outcome))) \n  return(outcome_dist_by_rf)\n}\nOR\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  outcome_dist_by_rf &lt;- df |&gt; \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) |&gt; \n  group_by(!!sym(rf)) |&gt;\n  summarize(!!(paste0(outcome, \"_prev\")) := mean(!!sym(outcome)))\n  return(outcome_dist_by_rf)\n}\nOR\ngenerate_descriptive_table &lt;- function (df, outcome, rf,) {\n  new_column_name = paste0(outcome, \"_prev\")\n  outcome_dist_by_rf &lt;- df |&gt; \n  filter(!is.na(!!sym(outcome)), !is.na(!!sym(rf))) |&gt; \n  group_by(!!sym(rf)) |&gt;\n  summarize(!!(new_column_name) := mean(!!sym(outcome))) \n  return(outcome_dist_by_rf)\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data masking</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "10  Quarto",
    "section": "",
    "text": "10.1 Introduction\nQuarto is an open-source scientific and technical publishing system that allows you to create documents, books, websites, presentations, and more. Quarto provides a unified authoring framework for data science, combining your code, its results, and your prose. Quarto documents are fully reproducible and support dozens of output formats, like PDFs, Word files, presentations, and more.\nQuarto files are designed to be used in three ways:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#introduction",
    "href": "quarto.html#introduction",
    "title": "10  Quarto",
    "section": "",
    "text": "For communicating to decision-makers, who want to focus on the conclusions, not the code behind the analysis.\nFor collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them (i.e., the code).\nAs an environment in which to do data science, as a modern-day lab notebook where you can capture not only what you did, but also what you were thinking.\n\n\n10.1.1 Key Features\nMulti-format Output: Quarto documents can be rendered into HTML, PDF, MS Word, ePub, PowerPoint, Revealjs presentations, dashboards, websites, and books from a single source file. This allows authors to maintain one document but publish it in multiple formats without rewriting content.\nRich Markdown Authoring: Content is created in markdown, with support for figures, tables, equations (LaTeX), citations, cross-references, and advanced layout features like tabs, callouts, and panels.\nEmbedded Executable Code: Integrate code chunks (R, Python, Julia, Observable JS) that can be executed and the results rendered directly in the document. This allows for dynamic results, data analysis, plots, and reproducible research workflows.\nInteractivity: Add interactive components such as widgets, tab sets, and collapsible sections for richer communication with readers.\nCustomization: Extensive theming and styling options, including custom CSS and advanced layout controls for polished, publication-quality output.\nProject Management: Organize large projects and integrate with version control tools like Git. Use Quarto projects to group related documents, manage dependencies, and orchestrate rendering.\n\n\n10.1.2 Why Quarto?\nIf you’re an R Markdown user, you might be thinking “Quarto sounds a lot like R Markdown.” You’re not wrong! Quarto unifies the functionality of many packages from the R Markdown ecosystem (rmarkdown, bookdown, distill, xaringan, etc.) into a single consistent system as well as extends it with native support for multiple programming languages like Python and Julia in addition to R. In a way, Quarto reflects everything that was learned from expanding and supporting the R Markdown ecosystem over a decade.\n\n\n10.1.3 Getting Started\nTo get started with Quarto:\n\nInstallation: Quarto CLI is included with RStudio, so if you have a recent version of RStudio, you already have Quarto. Otherwise, visit https://quarto.org/docs/get-started/\nDocumentation: The official Quarto documentation is available at https://quarto.org/docs/guide/\nR4DS Chapter: For an excellent introduction to using Quarto with R, see the Quarto chapter in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-basics",
    "href": "quarto.html#quarto-basics",
    "title": "10  Quarto",
    "section": "10.2 Quarto Basics",
    "text": "10.2 Quarto Basics\nA Quarto document is a plain text file with the extension .qmd. It contains three important types of content:\n\nAn (optional) YAML header surrounded by ---s\nChunks of code surrounded by ```\nText mixed with simple text formatting like # heading and _italics_\n\n\n10.2.1 Creating a New Quarto Document\nIn RStudio, create a new Quarto document using File &gt; New File &gt; Quarto Document… in the menu bar. RStudio will launch a wizard that you can use to pre-populate your file with useful content that reminds you how the key features of Quarto work.\n\n\n10.2.2 Visual vs. Source Editor\nRStudio provides two ways to edit Quarto documents:\nVisual Editor: The Visual editor provides a WYSIWYM interface for authoring Quarto documents. If you’re new to computational documents but have experience using tools like Google Docs or MS Word, the visual editor is the easiest way to get started. In the visual editor you can use the buttons on the menu bar to insert images, tables, cross-references, etc., or you can use the catch-all ⌘ + / or Ctrl + / shortcut to insert just about anything.\nSource Editor: The source editor allows you to edit the raw markdown and code. While the visual editor will feel familiar to those with experience in word processors, the source editor will feel familiar to those with experience writing R scripts or R Markdown documents. The source editor can also be useful for debugging any Quarto syntax errors since it’s often easier to catch these in plain text.\nYou can switch between the visual and source editors at any time using the toggle in the top-left of the editor pane.\n\n\n10.2.3 Rendering Documents\nTo produce a complete report containing all text, code, and results:\n\nClick “Render” in RStudio, or\nPress Cmd/Ctrl + Shift + K, or\nUse quarto::quarto_render(\"document.qmd\") in R, or\nUse quarto render document.qmd in the terminal\n\nWhen you render the document, Quarto sends the .qmd file to knitr, which executes all of the code chunks and creates a new markdown (.md) document which includes the code and its output. The markdown file generated by knitr is then processed by pandoc, which is responsible for creating the finished file in your chosen format (HTML, PDF, Word, etc.).\n\n\n10.2.4 Code Chunks\nTo run code inside a Quarto document, you need to insert a chunk. There are three ways to do so:\n\nThe keyboard shortcut Cmd + Option + I / Ctrl + Alt + I\nThe “Insert” button icon in the editor toolbar\nBy manually typing the chunk delimiters ```{r} and ```\n\nChunks can be given an optional label and various chunk options:\n```{r}\n#| label: simple-addition\n#| echo: false\n1 + 1\n```\nCommon chunk options include:\n\n#| label: - give the chunk a name\n#| echo: false - hide the code but show the output\n#| eval: false - show the code but don’t run it\n#| include: false - run the code but hide both code and output\n#| warning: false - hide warnings\n#| message: false - hide messages\n\n\n\n10.2.5 Format-Specific Settings\nWhen rendering to multiple output formats (HTML, PDF, DOCX, EPUB), you may want different chunk options or behavior for different formats. Use knitr::pandoc_to() with if () statements to detect the output format and set format-specific settings.\nExample: Different figure sizes for different formats\n```{r}\n#| label: example-plot\n#| fig-width: !expr if (knitr::pandoc_to(\"html\")) 8 else 6\n#| fig-height: !expr if (knitr::pandoc_to(\"html\")) 6 else 4\n\nplot(1:10)\n```\nExample: Conditional code execution based on format\n```{r}\nif (knitr::pandoc_to(\"docx\")) {\n  # DOCX-specific code\n  knitr::kable(data, format = \"simple\")\n} else if (knitr::pandoc_to(\"html\")) {\n  # HTML-specific code\n  knitr::kable(data, format = \"html\")\n} else {\n  # PDF or other formats\n  knitr::kable(data, format = \"latex\")\n}\n```\nCommon format detection patterns:\n\nknitr::pandoc_to(\"html\") - returns TRUE for HTML output\nknitr::pandoc_to(\"latex\") - returns TRUE for PDF output\nknitr::pandoc_to(\"docx\") - returns TRUE for Word output\nknitr::pandoc_to(\"epub\") - returns TRUE for EPUB output\n\nThis technique is particularly useful when you need to:\n\nAdjust figure dimensions for different page sizes\nUse different table formatting for different outputs\nInclude or exclude content based on output format\nSet format-specific styling or options\n\n\n\n10.2.6 Text Formatting\nQuarto uses Pandoc’s markdown for text formatting:\n\n*italic* or _italic_ produces italic text\n**bold** or __bold__ produces bold text\n`code` produces code formatting\n# Heading 1, ## Heading 2, ### Heading 3 for headings\nBullet lists start with - or *\nNumbered lists start with 1., 2., etc.\n[link text](url) creates hyperlinks\n![alt text](image.png) inserts images\n\nImportant: Always include a blank line before bullet lists and numbered lists in markdown and Quarto documents.\nFor more details on using Quarto for writing and analysis, see the Quarto chapter in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#building-quarto-books",
    "href": "quarto.html#building-quarto-books",
    "title": "10  Quarto",
    "section": "10.3 Building Quarto Books",
    "text": "10.3 Building Quarto Books\nQuarto books let you author entire books (or course notes, manuals, dissertations, etc.) in markdown. Quarto books are ideal for documentation, tutorials, lab manuals, and other long-form content.\n\n10.3.1 Creating a Quarto Book\nStarting from a template (recommended):\nUsing a template is the fastest way to get started with a Quarto book, as it provides pre-configured settings, example content, and GitHub Actions workflows for automated deployment:\n\nUCD-SeRG Quarto Book Template - Our recommended template with pre-configured settings for lab publications:\n\nRepository: https://github.com/UCD-SERG/quarto-book-template\nClick “Use this template” → “Create a new repository” on GitHub\nClone your new repository and start editing\nIncludes GitHub Actions for automatic deployment to GitHub Pages\n\nCoatless Tutorials Quarto Book Template - Another template with helpful examples:\n\nRepository: https://github.com/coatless-tutorials/quarto-book-template\nIncludes examples of common Quarto book features\n\nDataLab Quarto Template - Template from the UC Davis DataLab and Davis R Users Group:\n\nRepository: https://github.com/d-rug/datalab_template_quarto\nProvides a starting point for DataLab workshop materials and tutorials\n\n\nWhile these templates jumpstart your project with up-to-date configuration and workflow files, you should still come up to speed on what all the config files do (particularly _quarto.yml and any GitHub Actions workflows) so you can modify and debug them as needed. The templates serve as central locations for the most current versions of these files and best practices.\nStarting from scratch:\nIf you prefer to start from scratch, you can create a new Quarto book project using the Quarto CLI:\n# Create a new Quarto book project\nquarto create project book mybook\ncd mybook\nThis will create a basic book structure with:\n\n_quarto.yml - configuration file for your book\nindex.qmd - the home page / preface\nSample chapter files\nreferences.qmd - bibliography/references page\n\n\n\n10.3.2 Building and Previewing\nOnce you have a Quarto book project, you can build and preview it:\n# Render the entire book\nquarto render\n\n# Preview with live reload (recommended during development)\nquarto preview\nThe quarto preview command starts a local web server and automatically refreshes the preview whenever you save changes to your files.\n\n\n10.3.3 Book Structure\nA typical Quarto book is organized as follows:\n_quarto.yml: The main configuration file that defines:\n\nBook metadata (title, author, date)\nChapter order\nOutput formats (HTML, PDF, ePub, etc.)\nStyling and theme\nNavigation options\n\nChapter files: Individual .qmd files for each chapter. These are listed in the chapters section of _quarto.yml.\nParts: You can organize chapters into parts for better structure:\nbook:\n  chapters:\n    - index.qmd\n    - part: \"Getting Started\"\n      chapters:\n        - intro.qmd\n        - basics.qmd\n    - part: \"Advanced Topics\"\n      chapters:\n        - advanced.qmd\n\n\n10.3.4 Book Features\nQuarto books support many advanced features:\nCross-references: Reference figures, tables, equations, and sections throughout your book:\nSee @fig-plot for details.\nAs shown in @tbl-results.\nRefer to @sec-introduction.\nCitations: Include a bibliography and cite sources:\nAccording to @smith2020, the method works well.\nSearch: Automatic full-text search in HTML output.\nDownloads: Offer PDF, ePub, and Word versions alongside HTML.\nNavigation: Automatic table of contents, previous/next chapter buttons, and breadcrumbs.\nCustomization: Custom themes, CSS, and templates for professional appearance.\n\n\n10.3.5 Example: This Lab Manual\nThis lab manual itself is a Quarto book! You can view its source code at https://github.com/UCD-SERG/lab-manual to see how we’ve structured chapters, used includes for modular content, and configured various output formats.\n\n\n10.3.6 Resources\n\nQuarto Books Guide - comprehensive documentation for Quarto books\nQuarto Publishing Guide - how to publish your book online\nR4DS Quarto chapter (Wickham, Çetinkaya-Rundel, and Grolemund 2023) - excellent introduction to Quarto",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#quarto-profiles",
    "href": "quarto.html#quarto-profiles",
    "title": "10  Quarto",
    "section": "10.4 Quarto Profiles",
    "text": "10.4 Quarto Profiles\nQuarto profiles allow you to customize rendering behavior for different purposes. A profile is a named set of configuration options that can be activated when rendering. This is particularly useful when you want to render the same source files in different formats or for different audiences.\n\n10.4.1 What are Profiles?\nProfiles let you maintain multiple _quarto.yml configuration files in the same project. For example, you might have:\n\n_quarto.yml - default book configuration\n_quarto-revealjs.yml - configuration for rendering chapters as slide decks\n_quarto-print.yml - configuration optimized for PDF printing\n\n\n\n10.4.2 Example: Rendering Chapters as Slides\nA excellent example of using Quarto profiles comes from the Regression Models for Epidemiology course materials by D. Morrison.\nThe project includes a _quarto-revealjs.yml profile that allows each chapter to be compiled as a RevealJS slide deck, in addition to being part of the book.\nTo render a single chapter as slides:\nquarto render chapter-name.qmd --profile=revealjs\nTo render all chapters listed in the profile as slides:\nquarto render --profile=revealjs\n\n\n10.4.3 Creating a Profile\nTo create a profile:\n\nCreate a new YAML file named _quarto-{profile-name}.yml\nInclude only the configuration options that differ from your default _quarto.yml\nActivate the profile when rendering using --profile={profile-name}\n\nExample profile structure:\n_quarto-slides.yml:\nproject:\n  type: default\n  output-dir: slides\n\nformat:\n  revealjs:\n    theme: serif\n    slide-number: true\n    preview-links: auto\n\n\n10.4.4 Common Use Cases\nMultiple output formats: Maintain separate configurations for web, print, and presentation versions of your content.\nDifferent audiences: Create versions with or without solutions, technical details, or instructor notes.\nDevelopment vs. production: Use a development profile with faster rendering options during writing, and a production profile with full features for final output.\nCourse materials: Render the same content as both a reference book and lecture slides, as demonstrated in the RME course.\n\n\n10.4.5 Resources\n\nQuarto Profiles Documentation\nRME Example - see the source at https://github.com/d-morrison/rme for a working example",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#advanced-features",
    "href": "quarto.html#advanced-features",
    "title": "10  Quarto",
    "section": "10.5 Advanced Features",
    "text": "10.5 Advanced Features\n\n10.5.1 Cross-References\nQuarto provides a powerful cross-reference system for figures, tables, equations, and sections. Cross-references automatically number your content and create clickable links in HTML and PDF output.\nRequired label prefixes:\n\nFigures: #fig- (e.g., #fig-workflow-diagram)\nTables: #tbl- (e.g., #tbl-summary-stats)\nEquations: #eq- (e.g., #eq-regression-model)\nSections: #sec- (e.g., #sec-introduction)\nTheorems: #thm-, Lemmas: #lem-, Corollaries: #cor-\nPropositions: #prp-, Examples: #exm-, Exercises: #exr-\n\nFor figures (static images):\n![Caption text](path/to/image.png){#fig-label}\nFor code-generated figures:\n```{r}\n#| label: fig-plot-name\n#| fig-cap: \"Caption text describing the plot\"\n\n# R code to generate plot\nggplot(data, aes(x, y)) + geom_point()\n```\nFor tables (markdown tables):\n| Column 1 | Column 2 |\n|----------|----------|\n| Data     | Data     |\n\n: Caption text {#tbl-label}\nFor code-generated tables:\n```{r}\n#| label: tbl-table-name\n#| tbl-cap: \"Caption text\"\n\n# R code to generate table\nknitr::kable(data)\n```\nReferencing in text:\n\nFigures: @fig-label produces “Figure X”\nTables: @tbl-label produces “Table X”\nEquations: @eq-label produces “Equation X”\nSections: @sec-label produces “Section X”\n\nBenefits:\n\nAutomatic numbering of figures, tables, and equations\nAutomatic updates when content is reordered\nClickable cross-references in HTML and PDF output\nConsistent formatting across all output formats\nBetter accessibility for screen readers\n\nFor complete details, see the Quarto Cross-References documentation.\n\n\n10.5.2 Using Includes for Modular Content\nQuarto’s include feature allows you to decompose large documents into smaller, more manageable files. This is particularly useful for books and long documents.\nBasic syntax:\n{{&lt; include path/to/file.qmd &gt;}}\nBenefits:\n\nBetter Git History: When sections are reordered, only the main chapter file changes (moving include statements), making it immediately clear that content was reorganized rather than edited.\nEasier Code Review: Reviewers can see exactly what changed—either the organization (main file) or the content (include file).\nModular Maintenance: Each section lives in its own file, making it easier to find and edit specific content, reuse sections across chapters, and work on different sections simultaneously without merge conflicts.\nClear Structure: The main chapter file becomes a table of contents showing the organization at a glance.\n\nRecommended pattern:\nMain chapter file (e.g., 05-coding-practices.qmd):\n# Coding Practices\n\n## Section Heading\n\n{{&lt; include coding-practices/section-name.qmd &gt;}}\n\n## Another Section\n\n{{&lt; include coding-practices/another-section.qmd &gt;}}\nInclude files (e.g., coding-practices/section-name.qmd):\n\nStored in a subdirectory matching the chapter name\nContains only the content for that section (no heading)\nThe heading stays in the main chapter file\nNamed descriptively using kebab-case\n\nNote: The heading must be in the main file, followed by a blank line, then the include statement. This keeps the document structure clear in the main file.\nFor more details, see the Quarto Includes documentation.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#additional-resources",
    "href": "quarto.html#additional-resources",
    "title": "10  Quarto",
    "section": "10.6 Additional Resources",
    "text": "10.6 Additional Resources\n\n10.6.1 Official Documentation\n\nQuarto Official Guide - comprehensive official documentation\nQuarto Books Guide - documentation specific to creating books\nQuarto Publishing Guide - how to publish your Quarto content online\nQuarto Getting Started - installation and basic usage\n\n\n\n10.6.2 Learning Resources\n\nR for Data Science - Quarto Chapter (Wickham, Çetinkaya-Rundel, and Grolemund 2023) - excellent introduction to using Quarto with R\nRegression Models for Epidemiology - example of a Quarto book with profiles for rendering chapters as slides\nUCD-SeRG Lab Manual Source - this manual’s source code provides examples of:\n\nBook structure and organization\nUsing includes for modular content\n\nConfiguring multiple output formats (HTML, PDF, ePub, Word)\nCross-references for figures, tables, and sections\n\n\n\n\n10.6.3 Templates\n\nUCD-SeRG Quarto Book Template - our recommended template\nCoatless Tutorials Quarto Book Template - another frequently-used template with helpful examples\nDataLab Quarto Template - template from the UC Davis DataLab and Davis R Users Group\n\n\n\n10.6.4 Related Lab Manual Chapters\nFor additional context about using Quarto in our lab:\n\nChapter 6 - R coding practices that apply to code in Quarto documents\nChapter 7 - Code style guidelines including formatting for Quarto documents\nChapter 11 - Version control for Quarto projects\n\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "11  Github",
    "section": "",
    "text": "11.1 Basics\nAdapted by UCD-SeRG team from original by Stephanie Djajadi and Nolan Pokpongkiat",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#basics",
    "href": "github.html#basics",
    "title": "11  Github",
    "section": "",
    "text": "A detailed tutorial of Git can be found here on the CS61B website.\nIf you are already familiar with Git, you can reference the summary at the end of Section B.\nIf you have made a mistake in Git, you can refer to On undoing, fixing, or removing commits in git (Robertson, n.d.) to undo, fix, or remove commits in git.\nFor hands-on Git practice, see the UC Davis DataLab Git Sandbox - a collaborative repository for learning Git workflows.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#github-desktop",
    "href": "github.html#github-desktop",
    "title": "11  Github",
    "section": "11.2 Github Desktop",
    "text": "11.2 Github Desktop\nWhile knowing how to use Git on the command line will always be useful since the full power of Git and its customizations and flexibilty is designed for use with the command line, Github also provides GitHub Desktop (“GitHub Desktop,” n.d.) as an graphical interface to do basic git commands; you can do all of the basic functions of Git using this desktop app. Feel free to use this as an alternative to Git on the command line if you prefer.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#git-branching",
    "href": "github.html#git-branching",
    "title": "11  Github",
    "section": "11.3 Git Branching",
    "text": "11.3 Git Branching\nBranches allow you to keep track of multiple versions of your work simultaneously, and you can easily switch between versions and merge branches together once you’ve finished working on a section and want it to join the rest of your code. Here are some cases when it may be a good idea to branch:\n\nYou may want to make a dramatic change to your existing code (called refactoring) but it will break other parts of your project. But you want to be able to simultaneously work on other parts or you are collaborating with others, and you don’t want to break the code for them.\nYou want to start working on a new part of the project, but you aren’t sure yet if your changes will work and make it to the final product.\nYou are working with others and don’t want to mix up your current work with theirs, even if you want to bring your work together later in the future.\n\nA detailed tutorial on Git Branching can be found here. You can also find instructions on how to handle merge conflicts when joining branches together.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#example-workflow",
    "href": "github.html#example-workflow",
    "title": "11  Github",
    "section": "11.4 Example Workflow",
    "text": "11.4 Example Workflow\nA standard workflow when starting on a new project and contributing code looks like this:\n\n\n\nTable 11.1: Standard Git workflow for new projects\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nSETUP: FIRST TIME ONLY: git clone &lt;url&gt; &lt;directory_name&gt;\nClone the repo. This copies of all the project files in its current state on Github to your local computer.\n\n\n1. git pull origin master\nupdate the state of your files to match the most current version on GitHub\n\n\n2. git checkout -b &lt;new_branch_name&gt;\ncreate new branch that you’ll be working on and go to it\n\n\n3. Make some file changes\nwork on your feature/implementation\n\n\n4. git add -p\nadd changes to stage for commit, going through changes line by line\n\n\n5. git commit -m &lt;commit message&gt;\ncommit files with a message\n\n\n6. git push -u origin &lt;branch_name&gt;\npush branch to remote and set to track (-u only needed if this is first push)\n\n\n7. Repeat step 4-5.\nwork and commit often\n\n\n8. git push\npush work to remote branch for others to view\n\n\n9. Follow the link given from the git push command to submit a pull request (PR) on GitHub online\nPR merges in work from your branch into master\n\n\n(10.) Your changes and PR get approved, your reviewer deletes your remote branch upon merging\n\n\n\n11. git fetch --all --prune\nclean up your local git by untracking deleted remote branches\n\n\n\n\n\n\nOther helpful commands are listed below.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#commonly-used-git-commands",
    "href": "github.html#commonly-used-git-commands",
    "title": "11  Github",
    "section": "11.5 Commonly Used Git Commands",
    "text": "11.5 Commonly Used Git Commands\n\n\n\nTable 11.2: Commonly used Git commands\n\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ngit clone &lt;url&gt; &lt;directory_name&gt;\nclone a repository, only needs to be done the first time\n\n\ngit pull origin master\npull from master before making any changes\n\n\ngit branch\ncheck what branch you are on\n\n\ngit branch -a\ncheck what branch you are on + all remote branches\n\n\ngit checkout -b &lt;new_branch_name&gt;\ncreate new branch and go to it (only necessary when you create a new branch)\n\n\ngit checkout &lt;branch name&gt;\nswitch to branch\n\n\ngit add &lt;file name&gt;\nadd file to stage for commit\n\n\ngit add -p\nadds changes to commit, showing you changes one by one\n\n\ngit commit -m &lt;commit message&gt;\ncommit file with a message\n\n\ngit push -u origin &lt;branch_name&gt;\npush branch to remote and set to track (-u only works if this is first push)\n\n\ngit branch --set-upstream-to origin &lt;branch_name&gt;\nset upstream to origin/&lt;branch_name&gt; (use if you forgot -u on first push)\n\n\ngit push origin &lt;branch_name&gt;\npush work to branch\n\n\ngit checkout &lt;branch_name&gt;  git merge master\nswitch to branch and merge changes from master into &lt;branch_name&gt; (two commands)\n\n\ngit merge &lt;branch_name&gt; master\nswitch to branch and merge changes from master into &lt;branch_name&gt; (one command)\n\n\ngit checkout --track origin/&lt;branch_name&gt;\npulls a remote branch and creates a local branch to track it (use when trying to pull someone else’s branch onto your local computer)\n\n\ngit push --delete &lt;remote_name&gt; &lt;branch_name&gt;\ndelete remote branch\n\n\ngit branch -d &lt;branch_name&gt;\ndeletes local branch, -D to force\n\n\ngit fetch --all --prune\nuntrack deleted remote branches",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#how-often-should-i-commit",
    "href": "github.html#how-often-should-i-commit",
    "title": "11  Github",
    "section": "11.6 How often should I commit?",
    "text": "11.6 How often should I commit?\nIt is good practice to commit every 15 minutes, or every time you make a significant change. It is better to commit more rather than less.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#repeated-amend-workflow",
    "href": "github.html#repeated-amend-workflow",
    "title": "11  Github",
    "section": "11.7 Repeated Amend Workflow",
    "text": "11.7 Repeated Amend Workflow\nWhen working on a complex task, you may want to make frequent incremental commits to protect your progress, but avoid cluttering your Git history with many tiny “work in progress” commits. The Repeated Amend pattern lets you build up a polished commit gradually.\n\n11.7.1 Basic Workflow\nStart with a clean working tree in a functional state. Then:\n\nMake a small change and verify your project still works\nStage and commit with a temporary message like “WIP” (work in progress)\nDo not push yet\nMake another small change and verify it works\nStage and amend the previous commit: git commit --amend --no-edit\nRepeat steps 4-5 as needed\nWhen finished, amend one final time with a proper commit message\nPush your completed work\n\nIn RStudio, you can use the “Amend previous commit” checkbox when committing.\n\n\n11.7.2 Key Points\n\nEach amend replaces the previous commit rather than creating a new one\nThis keeps your history clean while letting you work incrementally\nOnly use this pattern before pushing - never amend commits that others may have pulled\nIf you need to undo changes, use git reset --hard to return to your last commit state\nThink of commits as climbing protection: use them when in uncertain territory\n\nFor more details and troubleshooting scenarios, see the Repeated Amend chapter in Happy Git with R.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "github.html#what-should-be-pushed-to-github",
    "href": "github.html#what-should-be-pushed-to-github",
    "title": "11  Github",
    "section": "11.8 What should be pushed to Github?",
    "text": "11.8 What should be pushed to Github?\nNever push .Rout files! If someone else runs an R script and creates an .Rout file at the same time and both of you try to push to github, it is incredibly difficult to reconcile these two logs. If you run logs, keep them on your own system or (preferably) set up a shared directory where all logs are name and date timestamped.\nThere is a standardized .gitignore for R which you can download and add to your project. This ensures you’re not committing log files or things that would otherwise best be left ignored to GitHub. This is a great discussion of project-oriented workflows, extolling the virtues of a self-contained, portable projects, for your reference.\n\n\n\n\n“GitHub Desktop.” n.d. GitHub. https://desktop.github.com/.\n\n\nRobertson, Seth. n.d. “On Undoing, Fixing, or Removing Commits in Git.” https://sethrobertson.github.io/GitFixUm/fixup.html.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Github</span>"
    ]
  },
  {
    "objectID": "unix.html",
    "href": "unix.html",
    "title": "12  Unix",
    "section": "",
    "text": "12.1 Basics\nAdapted by UCD-SeRG team from original by Stephanie Djajadi, Kunal Mishra, Anna Nguyen, and Jade Benjamin-Chung\nWe typically use Unix commands in Terminal (for Mac users) or Git Bash (for Windows users) to\nOn the computer, there is a desktop with two folders, folder1 and folder2, and a file called file1. Inside folder1, we have a file called file2. Mac users can run these commands on their terminal; it is recommended that Windows users use Git Bash, not Windows PowerShell.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "unix.html#basics",
    "href": "unix.html#basics",
    "title": "12  Unix",
    "section": "",
    "text": "Figure 12.1: Example desktop with folders and files",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "unix.html#syntax-for-both-macwindows",
    "href": "unix.html#syntax-for-both-macwindows",
    "title": "12  Unix",
    "section": "12.2 Syntax for both Mac/Windows",
    "text": "12.2 Syntax for both Mac/Windows\nWhen typing in directories or file names, quotes are necessary if the name includes spaces.\n\n\n\nTable 12.1: Basic Unix commands for Mac and Windows\n\n\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ncd desktop/folder1\nChange directory to folder1\n\n\npwd\nPrint working directory\n\n\nls\nList files in the directory\n\n\ncp \"file2\" \"newfile2\"\nCopy file (remember to include file extensions when typing in file names like .pdf or .R)\n\n\nmv \"newfile2\" \"file3\"\nRename newfile2 to file3\n\n\ncd ..\nGo to parent of the working directory (in this case, desktop)\n\n\nmv \"file1\" folder2\nMove file1 to folder2\n\n\nmkdir folder3\nMake a new folder in folder2\n\n\nrm &lt;filename&gt;\nRemove files\n\n\nrm -rf folder3\nRemove directories (-r will attempt to remove the directory recursively, -rf will force removal of the directory)\n\n\nclear\nClear terminal screen of all previous commands\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: Terminal output after executing basic Unix commands",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "unix.html#running-bash-scripts",
    "href": "unix.html#running-bash-scripts",
    "title": "12  Unix",
    "section": "12.3 Running Bash Scripts",
    "text": "12.3 Running Bash Scripts\n\n\n\nTable 12.2: Commands for running Bash scripts\n\n\n\n\n\n\n\n\n\n\nWindows\nMac / Linux\nDescription\n\n\n\n\nchmod +750 &lt;filename.sh&gt;\nchmod +x &lt;filename.sh&gt;\nChange access permissions for a file (only needs to be done once)\n\n\n./&lt;filename.sh&gt;\n./&lt;filename.sh&gt;\nRun file (./ to run any executable file)\n\n\nbash bash_script_name.sh &\nbash bash_script_name.sh &\nRun shell script in the background",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "unix.html#running-rscripts-in-windows",
    "href": "unix.html#running-rscripts-in-windows",
    "title": "12  Unix",
    "section": "12.4 Running Rscripts in Windows",
    "text": "12.4 Running Rscripts in Windows\nNote: This code seems to work only with Windows Command Prompt, not with Git Bash.\nWhen R is installed, it comes with a utility called Rscript. This allows you to run R commands from the command line. If Rscript is in your PATH, then typing Rscript into the command line, and pressing enter, will not error. Otherwise, to use Rscript, you will either need to add it to your PATH (as an environment variable), or append the full directory of the location of Rscript on your machine. To find the full directory, search for where R is installed your computer. For instance, it may be something like below (this will vary depending on what version of R you have installed):\nC:\\Program Files\\R\\R-3.6.0\\bin\nFor appending the PATH variable, please view this link. I strongly recommend completing this option.\nIf you add the PATH as an environment variable, then you can run this line of code to test: Rscript -e \"cat(‘this is a test’)\", where the -e flag refers to the expression that will be executed.\nIf you do not add the PATH as an environment variable, then you can run this line of code to replicate the results from above: \"C:\\Program Files\\R\\R-3.6.0\\bin\" -e \"cat(‘this is a test’)\"\nTo run an R script from the command line, we can say: Rscript -e \"source(‘C:/path/to/script/some_code.R’)\"\n\n12.4.1 Common Mistakes\n\nRemember to include all of the quotation marks around file paths that have a spaces.\nIf you attempt to run an R script but run into Error: '\\U' used without hex digits in character string starting \"'C:\\U\", try replacing all \\ with \\\\ or /.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "unix.html#checking-tasks-and-killing-jobs",
    "href": "unix.html#checking-tasks-and-killing-jobs",
    "title": "12  Unix",
    "section": "12.5 Checking tasks and killing jobs",
    "text": "12.5 Checking tasks and killing jobs\n\n\n\n\n\n\n\n\nWindows\nMac / Linux\nDescription\n\n\n\n\ntasklist\nps -v\nList all processes on the command line\n\n\n\ntop -o [cpu/rsize]\nList all running processes, sorted by CPU or memory usage\n\n\ntaskkill /F /PID pid_number\nkill &lt;PID_number&gt;\nKill a process by its process ID\n\n\ntaskkill /IM \"process name\" /F\n\nKill a process by its name\n\n\nstart /b program.exe\n\nRuns jobs in the background (exclude /b if you want the program to run in a new console)\n\n\n\nnohup\nPrevents jobs from stopping\n\n\n\ndisown\nKeeps jobs running in the background even if you close R\n\n\ntaskkill /?\n\nHelp, lists out other commands\n\n\n\nTo kill a task in Windows, you can also go to Task Manager &gt; More details &gt; Select your desired app &gt; Click on End Task.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "unix.html#running-big-jobs",
    "href": "unix.html#running-big-jobs",
    "title": "12  Unix",
    "section": "12.6 Running big jobs",
    "text": "12.6 Running big jobs\nFor big data workflows, the concept of “backgrounding” a bash script allows you to start a “job” (i.e. run the script) and leave it overnight to run. At the top level, a bash script (0-run-project.sh) that simply calls the directory-level bash scripts (i.e. 0-prep-data.sh, 0-run-analysis.sh, 0-run-figures.sh, etc.) is a powerful tool to rerun every script in your project. See the included example bash scripts for more details.\n\nRunning Bash Scripts in Background: Running a long bash script is not trivial. Normally you would run a bash script by opening a terminal and typing something like ./run-project.sh. But what if you leave your computer, log out of your server, or close the terminal? Normally, the bash script will exit and fail to complete. To run it in background, type ./run-project.sh &; disown. You can see the job running (and CPU utilization) with the command top or ps -v and check your memory with free -h.\n\nAlternatively, to keep code running in the background even when an SSH connection is broken, you can use tmux. In terminal or gitbash follow the steps below. This site has useful tips on using tmux.\n# create a new tmux session called session_name\ntmux new -ssession_name\n\n# run your job of interest\nR CMD BATCH myjob.R & \n  \n# check that it is running\nps -v\n\n# to exit the tmux session (Mac)\nctrl + b \nd\n\n# to reopen the tmux session to kill the job or \n# start another job\ntmux attach -tsession_name \n\nDeleting Previously Computed Results: One helpful lesson we’ve learned is that your bash scripts should remove previous results (computed and saved by scripts run at a previous time) so that you never mix results from one run with a previous run. This can happen when an R script errors out before saving its result, and can be difficult to catch because your previously saved result exists (leading you to believe everything ran correctly).\nEnsuring Things Ran Correctly: You should check the .Rout files generated by the R scripts run by your bash scripts for errors once things are run. A utility file is include in this repository, called runFileSaveLogs, and is used by the example bash scripts to… run files and save the generated logs. It is an awesome utility and one I definitely recommend using. Before using runFileSaveLogs, it is necessary to put the file in the home working directory. For help and documentation, you can use the command ./runFileSaveLogs -h. See example code and example usage for runFileSaveLogs below.\n\n\n12.6.1 Example code for runfileSaveLogs\n#!/usr/bin/env python3\n# Type \"./runFileSaveLogs -h\" for help\n\nimport os\nimport sys\nimport argparse\nimport getpass\nimport datetime\nimport shutil\nimport glob\nimport pathlib\n\n# Setting working directory to this script's current directory\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\n# Setting up argument parser\nparser = argparse.ArgumentParser(description='Runs the argument R script(s) - in parallel if specified - and moves the subsequent generated .Rout log files to a timestamped directory.')\n\n# Function ensuring that the file is valid\ndef is_valid_file(parser, arg):\n    if not os.path.exists(arg):\n        parser.error(\"The file %s does not exist!\" % arg)\n    else:\n        return arg\n\n# Function ensuring that the directory is valid\ndef is_valid_directory(parser, arg):\n    if not os.path.isdir(arg):\n        parser.error(\"The specified path (%s) is not a directory!\" % arg)\n    else:\n        return arg\n\n# Additional arguments that can be added when running runFileSaveLogs\nparser.add_argument('-p', '--parallel', action='store_true', help=\"Runs the argument R scripts in parallel if specified\")\nparser.add_argument(\"-i\", \"--identifier\", help=\"Adds an identifier to the directory name where this is saved\")\nparser.add_argument('filenames', nargs='+', type=lambda x: is_valid_file(parser, x))\n\nargs = parser.parse_args()\nargs_dict = vars(args)\n\nprint(args_dict)\n\n# Run given R Scripts\nfor filename in args_dict[\"filenames\"]:\n  system_call = \"R CMD BATCH\" + \" \" + filename\n  if args_dict[\"parallel\"]: \n    system_call = \"nohup\" + \" \" + system_call + \" &\"\n\n  os.system(system_call)\n\n# Create the directory (and any parents) of the log files\ncurrentUser = getpass.getuser()\ncurrentTime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\nlogDirPrefix = \"/home/kaiserData/logs/\" # Change to the directory where the logs should be saved\nlogDir = logDirPrefix + currentTime + \"-\" + currentUser \n\n# If specified, adds the identifier to the filename of the log\nif args.identifier is not None:\n  logDir += \"-\" + args.identifier\n\nlogDir += \"/\"\n\npathlib.Path(logDir).mkdir(parents=True, exist_ok=True)\n\n# Find and move all logs to this new directory\ncurrentLogPaths = glob.glob('./*.Rout')\n\nfor currentLogPath in currentLogPaths:\n  filename = currentLogPath.split(\"/\")[-1]\n  shutil.move(currentLogPath, logDir + filename)\n\n\n12.6.2 Example usage for runfileSaveLogs\nThis example bash script runs files and generates logs for five scripts in the kaiserflu/3-figures folder. Note that the -i flag is used as an identifier to add figures to the filename of each log.\n#!/bin/bash\n\n# Copy utility run script into this folder for concision in call\ncp ~/kaiserflu/runFileSaveLogs ~/kaiserflu/3-figures/\n\n# Run folder scripts and produce output\ncd ~/kaiserflu/3-figures/\n./runFileSaveLogs -i \"figures\" \\\nfig-mean-season-age.R \\\nfig-monthly-rate.R \\\nfig-point-estimates-combined.R \\\nfig-point-estimates.R \\\nfig-weekly-rate.R\n\n# Remove copied utility run script\nrm runFileSaveLogs",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Unix</span>"
    ]
  },
  {
    "objectID": "reproducible-environments.html",
    "href": "reproducible-environments.html",
    "title": "13  Reproducible Environments",
    "section": "",
    "text": "13.1 Package Version Control with renv\nAdapted by UCD-SeRG team from original by Anna Nguyen",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reproducible Environments</span>"
    ]
  },
  {
    "objectID": "reproducible-environments.html#package-version-control-with-renv",
    "href": "reproducible-environments.html#package-version-control-with-renv",
    "title": "13  Reproducible Environments",
    "section": "",
    "text": "13.1.1 Introduction\nReplicable code should produce the same results, regardless of when or where it’s run. However, our analyses often leverage open-source R packages that are developed by other teams. These packages continue to be developed after research projects are completed, which may include changes to analysis functions that could impact how code runs for both other team members and external replicators.\nFor example, suppose we had used a function that took in one argument, such that our code contained example_function(arg_a = \"a\"). A few months after we publish our code, the package developers update the function to take in another mandatory argument arg_b. If someone runs our code, but has the most recent version of the package, they’ll receive an error message that the argument arg_b is missing and will not be able to full reproduce our results.\nTo ensure that the right functions are used in replication efforts, it is important for us to keep track of package versions used in each project.\nrenv can be to promote reproducible environments within R projects. renv creates individual package libraries for each project instead of having all projects, which may use different versions of the same package, share the same package library. However, for projects that use many packages, this process can be memory intensive and increase the time needed for a new users to start running code.\nIn this lab manual chapter, we provide a quick tutorial for integrating renv into research workflows. For more detailed instructions, please refer to the renv package vignette.\n\n\n13.1.2 Implementing renv in projects\nIdeally, renv should be initiated at the start of projects and updated continuously when new packages are introduced in the codebase. However, this process can be initated at any point in a project\nTo add renv to your workflow, follow these steps:\n\nInstall the renv package by running install.packages(\"renv\")\nCreate an RProject file and ensure that your working directory is set to the correct folder\nIn the R console, run renv::init() to intiialize renv in your R Project\nThis will create the following files: renv.lock, .Rprofile, renv/settings.json and renv/activate.R. Commit and push these files to GitHub so that they’re accessible to other users.\nAs you write code, update the project’s R library by running renv::snapshot() in the R console\nAdd renv::restore() to the head of your config file, to make sure that all users that run your code are on the same package versions.\n\n\n\n13.1.3 Using projects with renv\nIf you’re starting to work on an ongoing project that already has renv set up, follow these steps to ensure that you’re using the same project versions.\n\nInstall the renv package by running install.packages(\"renv\")\nPull the most updated version of the project from GitHub\nOpen the project’s RProject file\nRun renv::restore(). In our lab’s projects, this is often already found at the top of the config file, so you can just run scripts as is.\nThis will pull up a list of the project’s packages that need to be updated for you to be consistent with the project. The console will ask if you want to proceed with updating these packages - type “Y” to continue.\nWait for the correct versions of each package to install/update. This may take some time, depending on how many packages the project uses.\nYour R environment should now be using the same package versions as specified in the renv lock file. You should now be able to replicate the code.\nIf you make edits to the code and introduce new/updated packages, see the section above for instructions on how to make updates.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reproducible Environments</span>"
    ]
  },
  {
    "objectID": "code-publication.html",
    "href": "code-publication.html",
    "title": "14  Code Publication",
    "section": "",
    "text": "14.1 Checklist overview\nAdapted by UCD-SeRG team from original by Nolan Pokpongkiat",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#checklist-overview",
    "href": "code-publication.html#checklist-overview",
    "title": "14  Code Publication",
    "section": "",
    "text": "Fill out file headers\nClean up comments\nDocument functions\nRemove deprecated filepaths\nEnsure project runs via bash\nComplete the README\nClean up feature branches\nCreate Github release",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#fill-out-file-headers",
    "href": "code-publication.html#fill-out-file-headers",
    "title": "14  Code Publication",
    "section": "14.2 Fill out file headers",
    "text": "14.2 Fill out file headers\nEvery file in a project should have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. See template here.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#clean-up-comments",
    "href": "code-publication.html#clean-up-comments",
    "title": "14  Code Publication",
    "section": "14.3 Clean up comments",
    "text": "14.3 Clean up comments\nMake sure comments in the code are for code documentation purposes only. Do not leave comments to self in the final script files.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#document-functions",
    "href": "code-publication.html#document-functions",
    "title": "14  Code Publication",
    "section": "14.4 Document functions",
    "text": "14.4 Document functions\nEvery function you write must include a header to document its purpose, inputs, and outputs. See template for the function documentation here.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#remove-deprecated-filepaths",
    "href": "code-publication.html#remove-deprecated-filepaths",
    "title": "14  Code Publication",
    "section": "14.5 Remove deprecated filepaths",
    "text": "14.5 Remove deprecated filepaths\nAll file paths should be defined in 0-config.R, and should be set relative to the project working directory. All absolute file paths from your local computer should be removed, and replaced with a relative path. If a third party were to re-run this analysis, if they need to download data from a separate source and change a filepath in the 0-config.R to match, make sure to specify in the README which line of 0-config.R needs to be substituted.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#ensure-project-runs-via-bash",
    "href": "code-publication.html#ensure-project-runs-via-bash",
    "title": "14  Code Publication",
    "section": "14.6 Ensure project runs via bash",
    "text": "14.6 Ensure project runs via bash\nThe project should be configured to be entirely reproducible by running a master bash script, run-project.sh, which should live at the top directory. This bash script can call other bash scripts in subfolders, if necessary. Bash scripts should use the runFileSaveLogs utility script, which is a wrapper around the Rscript command, allowing you to specify where .Rout log files are moved after the R scripts are run.\nSee usage and documentation here.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#complete-the-readme",
    "href": "code-publication.html#complete-the-readme",
    "title": "14  Code Publication",
    "section": "14.7 Complete the README",
    "text": "14.7 Complete the README\nA README.md should live at the top directory of the project. This usually includes a Project Overview and a Directory Structure, along with the names of the contributors and the Creative Commons License. See below for a template:\n\nOverview\nTo date, coronavirus testing in the US has been extremely limited. Confirmed COVID-19 case counts underestimate the total number of infections in the population. We estimated the total COVID-19 infections – both symptomatic and asymptomatic – in the US in March 2020. We used a semi-Bayesian approach to correct for bias due to incomplete testing and imperfect test performance.\nDirectory structure\n\n0-config.R: configuration file that sets data directories, sources base functions, and loads required libraries\n0-base-functions: folder containing scripts with functions used in the analysis\n\n0-base-functions.R: R script containing general functions used across the analysis\n0-bias-corr-functions.R: R script containing functions used in bias correction\n0-bias-corr-functions-undertesting.R: R script containing functions used in bias correction to estimate the percentage of underestimation due to incomplete testing vs. imperfect test accuracy\n0-prior-functions.R: R script containing functions to generate priors\n\n1-data: folder containing data processing scripts NOTE: some scripts are deprecated\n2-analysis: folder containing analysis scripts. To rerun all scripts in this subdirectory, run the bash script 0-run-analysis.sh.\n\n1-obtain-priors-state.R: obtain priors for each state\n2-est-expected-cases-state.R: estimate expected cases in each state\n3-est-expected-cases-state-perf-testing.R: estimate expected cases in each state, estimate the percentage of underestimation due to incomplete testing vs. imperfect test accuracy\n4-obtain-testing-protocols.R: find testing protocols for each state.\n5-summarize-results.R: summarize results; obtain results for in text numerical results.\n\n3-figure-table-scripts: folder containing figure scripts. To rerun all scripts in this subdirectory, run the bash script 0-run-figs.sh.\n\n1-fig-testing.R: creates plot of testing patterns by state over time\n2-fig-cases-usa-state-bar.R: creates bar plot of confirmed vs. estimated infections by state\n3a-fig-map-usa-state.R: creates map of confirmed vs. estimated infections by state\n3b-fig-map-usa-state-shiny.R: creates map of confirmed vs. estimated infections by state with search functionality by state\n4-fig-priors.R: creates figure with priors for US as a whole\n5-fig-density-usa.R: creates figure of distribution of estimated cases in the US\n6-table-data-quality.R: creates table of data quality grading from COVID Tracking Project\n7-fig-testpos.R: creates figure of the probability of testing positive among those tested by state\n8-fig-percent-undertesting-state.R: creates figure of the percentage of under estimation due to incomplete testing\n\n4-figures: folder containing figure files.\n5-results: folder containing analysis results objects.\n6-sensitivity: folder containing scripts to run the sensitivity analyses\n\nContributors: UCD-SeRG team (adapted from original contributors: Jade Benjamin-Chung, Sean L. Wu, Anna Nguyen, Stephanie Djajadi, Nolan N. Pokpongkiat, Anmol Seth, Andrew Mertens)\nWu SL, Mertens A, Crider YS, Nguyen A, Pokpongkiat NN, Djajadi S, et al. Substantial underestimation of SARS-CoV-2 infection in the United States due to incomplete testing and imperfect test accuracy. medRxiv. 2020; 2020.05.12.20091744. doi:10.1101/2020.05.12.20091744\n\nWhen possible, also include a description of the RDS results that are generated, detailing what data sources were used, where the script lives that creates it, and what information the RDS results hold.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#clean-up-feature-branches",
    "href": "code-publication.html#clean-up-feature-branches",
    "title": "14  Code Publication",
    "section": "14.8 Clean up feature branches",
    "text": "14.8 Clean up feature branches\nIn the remote repository on Github, all feature branches aside from master should be merged in and deleted. All outstanding PRs should be closed.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "code-publication.html#create-github-release",
    "href": "code-publication.html#create-github-release",
    "title": "14  Code Publication",
    "section": "14.9 Create Github release",
    "text": "14.9 Create Github release\nOnce all of these items are verified, create a tag to make a Github release, which will tag the repository, creating a marker at this specific point in time.\nDetailed instructions here.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Code Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html",
    "href": "data-publication.html",
    "title": "15  Data Publication",
    "section": "",
    "text": "15.1 Overview\nAdapted from Fanice Nyatigo and Ben Arnold’s chapter in the Proctor-UCSF Lab Manual\nIf you are releasing data into the public domain, then consider making available at minimum a .csv file and a codebook of the same name (note: you should have a codebook for internal data as well). We often also make available .rds files as well. For example, your mystudy/data/public directory could include three files for a single dataset, two with the actual data in .rds and .csv formats, and a third that describes their contents:\nIn general, datasets are usually too big to save on GitHub, but occasionally they are small. Here is an example of where we actually pushed the data directly to GitHub: https://github.com/ben-arnold/enterics-seroepi/tree/master/data .\nIf the data are bigger, then maintaining them under version control in your git repository can be unwieldy. Instead, we recommend using another stable repository that has version control, such as the Open Science Framework (“Open Science Framework,” n.d.). For example, all of the data from the WASH Benefits trials (led by investigators at Berkeley, icddr,b, IPA-Kenya and others) are all stored through data components nested within in OSF projects: https://osf.io/tprw2/. Another good option is Dryad Digital Repository (“Dryad Digital Repository,” n.d.) or institutional digital repositories.\nWe recommend cross-linking public files in GitHub (scripts/notebooks only) and OSF/Dryad/institutional digital repositories.\nBelow are the main steps to making data public, after finalizing the analysis datasets and scripts:\n1. Remove Protected Health Information (PHI)\n2. Create public IDs or join already created public IDs to the data\n3. Create an OSF repository and/or Dryad/institutional digital repository\n4. Edit analysis scripts to run using the public datasets and test (optional)\n5. Create a public github page for analysis scripts and link to OSF and/or Dryad/Zenodo\n6. Go live",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#overview",
    "href": "data-publication.html#overview",
    "title": "15  Data Publication",
    "section": "",
    "text": "Warning!  NEVER push a dataset into the public domain (e.g., GitHub, OSF) without first checking with lab leadership to ensure that it is appropriately de-identified and we have approval from the sponsor and/or human subjects review board to do so. For example, we will need to re-code participant IDs (even if they contain no identifying information) before making data public to completely break the link between IDs and identifiable information stored on our servers. \n\n\n\nanalysis_data_public.csv\nanalysis_data_public.rds\nanalysis_data_public_codebook.txt",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#removing-phi",
    "href": "data-publication.html#removing-phi",
    "title": "15  Data Publication",
    "section": "15.2 Removing PHI",
    "text": "15.2 Removing PHI\nOnce the data is finalized for analysis, the first step is to strip it of Protected Health Information (PHI), or any other data that could be used to link back to specific participants, such as names, birth dates, or GPS coordinates at the village/neighborhood level or below. PHI includes, but is not limited to:\n\n15.2.1 Personal information\nThese are identifiers that directly point to specific individuals, such as:\n- Names, addresses, photographs, date of birth\n- A combination of age, sex, and geographic location (below population 20,000) is considered identifiable\n\n\n15.2.2 Dates\nAny specific dates (e.g., study visit dates, birth dates, treatment dates) are usually problematic.\n- If a dataset requires high resolution temporal information, coarsen visit or measurement dates to be two variables: year and week of the year (1-52).\n- If a dataset requires age, provide that information without a birth date (typically month resolution is sufficient)\n\n\nCaution! If making changes to the format of dates or ages, make sure your analysis code runs on these modified versions of the data (step 3)! \n\n\n\n\n15.2.3 Geographic information\nDo not include GPS coordinates (longitude, latitude) except in special circumstances where they have been obfuscated/shifted. Reach out to lab leadership before doing this because it can be complicated.\nDo not include place names or codes (e.g., US Zip Codes) if the place contains &lt;20,000 people. For villages or neighborhoods, code them with uninformative IDs. For sub-districts or districts, names are fine.\nIf an analysis requires GPS locations (e.g., to make a map), then typically we include a disclaimer in the article’s data availability statement that explains we cannot make GPS locations public to protect participant confidentiality. As a middle ground, we typically make our code public that runs on the geo-located data for transparency, even if independent researchers can’t actually run that code (although please be careful to ensure the code itself does not in any way include geographic identifiers).\nFor more examples of what constitutes PHI, please refer to this link: https://cphs.berkeley.edu/hipaa/hipaa18.html\nFor learning about working with geospatial data, see the UC Davis DataLab GIS workshops, including resources on QGIS and spatial SQL.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#create-public-ids",
    "href": "data-publication.html#create-public-ids",
    "title": "15  Data Publication",
    "section": "15.3 Create public IDs",
    "text": "15.3 Create public IDs\n\n15.3.1 Rationale\nThe UC Davis IRB requires that public datasets not include the original study IDs to identify participants or other units in the study (such as village IDs). The reason is that those IDs are linked in our private datasets to PHI. By creating a new set of public IDs, the public dataset is one step further removed from the potential to link to PHI.\n\n\n15.3.2 A single set of public IDs for each study\nFor each study, it is ideal to create a single set of public IDs whenever possible. We could create a new set of public IDs for every public dataset, but the downside is that independent researchers could no longer link data that might be related. By creating a single set of public IDs associated with each internal study ID, public files retain the link.\nMaintaining a single set of public IDs requires a shared “bridge” dataset, that includes a row for each study ID and has the associated public ID. For studies with multiple levels of ID, we would typically have separate bridge datasets for each type of ID (e.g,. cluster ID, participant ID, etc.)\nCreate a public ID that can be used to uniquely identify participants and that can internally be linked to the original study IDs. We recommend creating a subdirectory in the study’s shared data directory to store the public IDs. The shared location enables multiple projects to use the same IDs. Create the IDs using a script that reads in the study IDs, creates a unique (uninformative) public ID for the study IDs, and then saves the bridge dataset. The script should be saved in the same directory as the public ID files.\n\n\nCaution! Note that small differences may arise if the new public IDs do not necessarily order participants in the same way as the internal IDs. The small differences are all in estimates that rely on resampling, such as Bootstrap CIs, permutation P-values, and TMLE, as the resampling process may lead tp slightly different re-samples. The key here, to ensure the results are consistent irrespective of the dataset used, is simply to not assign public IDs randomly. Use rank() on the internal ID instead of row_number() to ensure that the order is always the same. \n\n\n\n\n15.3.3 Example scripts\nWe have created a self-contained and reproducible example that you can run and replicate when making data public for your projects. It contains the following files and folders:\n\ndata/final/- folder containing the projects final data in both csv and rds formats\n\ncode/DEMO_generate_public_IDs.R- creates randomly generated public IDs that can be matched to the trial’s assigned patient IDs.\n\ndata/make_public/DEMO_internal_to_publicID.csv- the output from step #2, a bridge dataset with two variables- the new public ID and the patient’s assigned ID.\n\ncode/DEMO_create_public_datasets.R- joins the public IDs to the trial’s full dataset, and strips it of the assigned patient ID.\ndata/public/- folder containing the output from step #3- de-identified public dataset, in csv and rds formats, with uniquely identifying public IDs that cannot be easily linked back to the patient’s ID.\n\nThe example workflow is accessible via GitHub: https://github.com/proctor-ucsf/dcc-handbook/tree/master/templates/making-data-public",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#create-a-data-repository",
    "href": "data-publication.html#create-a-data-repository",
    "title": "15  Data Publication",
    "section": "15.4 Create a data repository",
    "text": "15.4 Create a data repository\nFirst, ensure that you create a codebook and metadata file for each public dataset See the DCC guide on Documenting datasets. Use the same name as the datasets, but with “-codebook.txt” / “-codebook.html” / “-codebook.csv” at the end (depending on the file format for the codebook). One nice option is the R codebook package, which also generates JSON output that is machine-readable.\nFor additional guidance on data documentation best practices, see the UC Davis DataLab workshop on data documentation.\n\n15.4.1 Steps for creating an Open Science Framework (OSF) repository:\n\nCreate a new OSF project per these instructions: https://help.osf.io/article/252-create-a-project\nCreate a data component and upload the datasets in .csv and .rds format along with the codebooks. The primary format for public dissemination is .csv but we make the .rds files available too as auxiliary files for convenience.\nCreate a notebook component and upload the final .html files (which will not be on github… but see optional item below)\nOn the OSF landing Wiki, provide some context. Here is a recent example: https://osf.io/954bt/\nCreate a Digital Object Identifier (DOI) for the repository. A DOI is a unique identifier that provides a persistent link to content, such as a dataset in this case. Learn more about DOIs\nOptional: Complete the software checklist and system requirement guide for the analysis to guide others. Include it on the GitHub README for the project: https://github.com/proctor-ucsf/mordor-antibody",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#edit-and-test-analysis-scripts",
    "href": "data-publication.html#edit-and-test-analysis-scripts",
    "title": "15  Data Publication",
    "section": "15.5 Edit and test analysis scripts",
    "text": "15.5 Edit and test analysis scripts\nMake minor changes to the analysis scripts so that they run on public data. If using version control in GitHub, the most straight-forward way is to create a branch from the main git branch that reads in the public files, and then renames the new public ID variable, e.g., “id_public” to the internally recognized ID variable name, e.g. “recordID”, when reading in the public data. Re-run all the analysis scripts to ensure that they still work with the public version of the dataset.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#create-a-public-github-page-for-public-scripts",
    "href": "data-publication.html#create-a-public-github-page-for-public-scripts",
    "title": "15  Data Publication",
    "section": "15.6 Create a public GitHub page for public scripts",
    "text": "15.6 Create a public GitHub page for public scripts\nAt minimum, we should include all of the scripts required to run the analyses. IMPORTANT: ensure you have taken a snapshot and saved your computing environment using the renv package (renv).\nSee examples:\n- ACTION - https://github.com/proctor-ucsf/ACTION-public\n- NAITRE - https://github.com/proctor-ucsf/NAITRE-primary\n\n\nCaution! Read through the scripts carefully to ensure there is no PHI in the code itself \n\n\nOnce a public GitHub page exists, you can create a new component on an OSF project (step 3, above) and link it to the public version of the GitHub repo.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "data-publication.html#go-live",
    "href": "data-publication.html#go-live",
    "title": "15  Data Publication",
    "section": "15.7 Go live",
    "text": "15.7 Go live\nOn GitHub, it is useful to create an official “release” version to freeze the repository, where you can have “associated files” with each version. Include the .html notebook output as additional files — since they aren’t tracked in GitHub, it does provide a way of freezing / saving the HTML output for us and others. OSF examples of a studies from UCSF’s Proctor Foundation:\n- ACTION - https://osf.io/ca3pe/\n- NAITRE - https://osf.io/ujeyb/\n- MORDOR Niger antibody study - https://osf.io/dgsq3/\nFurther reading on end-to-end data management: How to Store and Manage Your Data - PLOS (“How to Store and Manage Your Data,” n.d.)\n\n\n\n\n“Dryad Digital Repository.” n.d. Dryad. https://datadryad.org/.\n\n\n“How to Store and Manage Your Data.” n.d. PLOS. https://plos.org/resource/how-to-store-and-manage-your-data/.\n\n\n“Open Science Framework.” n.d. Center for Open Science. https://osf.io/.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Publication</span>"
    ]
  },
  {
    "objectID": "slurm.html",
    "href": "slurm.html",
    "title": "16  High-performance computing (HPC)",
    "section": "",
    "text": "16.1 UC Davis Computing Resources\nAdapted by UCD-SeRG team from original by Anna Nguyen, Jade Benjamin-Chung, and Gabby Barratt Heitmann\nWhen you need to run a script that requires a large amount of RAM, large files, or that uses parallelization, UC Davis provides several high-performance computing (HPC) resources.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#uc-davis-computing-resources",
    "href": "slurm.html#uc-davis-computing-resources",
    "title": "16  High-performance computing (HPC)",
    "section": "",
    "text": "16.1.1 Available Resources\nUC Davis HPC Clusters: - Farm Cluster (hpc.ucdavis.edu): UC Davis’s primary HPC cluster providing shared computing resources for research\nPHS Shared Compute Environments: For lab members affiliated with the School of Public Health Sciences (PHS), additional shared computing environments are available. These environments provide secure, HIPAA-compliant computing resources suitable for working with sensitive health data.\n\nShiva (shiva.ucdavis.edu): SLURM-based cluster for computational work\nMercury (mercury.ucdavis.edu): RStudio GUI computing environment\n\nFor detailed information about PHS shared compute environments, including access procedures, security guidelines, and usage policies, please refer to the PHS Shared Compute Environments Guide.\nContact lab leadership for assistance with: - Requesting access to computing resources - Choosing the appropriate computing environment for your project - Setting up your computing environment",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#getting-started-with-slurm-clusters",
    "href": "slurm.html#getting-started-with-slurm-clusters",
    "title": "16  High-performance computing (HPC)",
    "section": "16.2 Getting started with SLURM clusters",
    "text": "16.2 Getting started with SLURM clusters\nTo access a UC Davis HPC cluster, in terminal, log in using SSH. For example, to access shiva:\nssh USERNAME@shiva.ucdavis.edu\nYou will be prompted to enter your UC Davis credentials and may need to complete two-factor authentication.\nOnce you log in, you can view the contents of your home directory in command line by entering cd $HOME. You can create subfolders within this directory using the mkdir command. For example, you could make a “code” subdirectory and clone a Github repository there using the following code:\ncd $HOME\nmkdir code\ngit clone https://github.com/jadebc/covid19-infections.git\n\n16.2.1 One-Time System Set-Up\nTo keep the install packages consistent across different nodes, you will need to explicitly set the pathway to your R library directory.\nOpen your ~/.Renviron file (vi ~/.Renviron) and append the following line:\nNote: Once you open the file using vi [file_name], you must press i (on Mac OS) or Insert (on Windows) to make edits. After you finish, hit Esc to exit editing mode and type :wq to save and close the file.\nR_LIBS=~/R/x86_64-pc-linux-gnu-library/4.0.2\nAlternatively, run an R script with the following code on the cluster:\nr_environ_file_path = file.path(Sys.getenv(\"HOME\"), \".Renviron\")\nif (!file.exists(r_environ_file_path)) file.create(r_environ_file_path)\n\ncat(\"\\nR_LIBS=~/R/x86_64-pc-linux-gnu-library/4.0.2\",\n    file = r_environ_file_path, sep = \"\\n\", append = TRUE)\nTo load packages that run off of C++, you’ll need to set the correct compiler options in your R environment.\nOpen the Makevars file (vi ~/.R/Makevars) and append the following lines\nCXX14FLAGS=-O3 -march=native -mtune=native -fPIC\nCXX14=g++\nAlternatively, create an R script with the following code, and run it on the cluster:\ndotR = file.path(Sys.getenv(\"HOME\"), \".R\")\nif (!file.exists(dotR)) dir.create(dotR)\n\nM = file.path(dotR, \"Makevars\")\nif (!file.exists(M)) file.create(M)\n\ncat(\"\\nCXX14FLAGS=-O3 -march=native -mtune=native -fPIC\",\n    \"CXX14=g++\",\n    file = M, sep = \"\\n\", append = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#moving-files-to-the-cluster",
    "href": "slurm.html#moving-files-to-the-cluster",
    "title": "16  High-performance computing (HPC)",
    "section": "16.3 Moving files to the cluster",
    "text": "16.3 Moving files to the cluster\nThe $HOME directory is a good place to store code and small test files. Save large files to the $SCRATCH directory or other designated storage areas. Check with the UC Davis HPC documentation for specific quotas and retention policies. It’s best to create a bash script that records the file transfer process for a given project. See example code below:\n# note: the following steps should be done from your local \n# (not after ssh-ing into the cluster)\n\n# securely transfer folders from Box to cluster home directory\n# note: the -r option is for folders and is not needed for files\nscp -r \"Box/project-folder/folder-1/\" USERNAME@shiva.ucdavis.edu:/home/users/USERNAME/\n\n# securely transfer folders from Box to your cluster scratch directory\nscp -r \"Box/project-folder/folder-2/\" USERNAME@shiva.ucdavis.edu:/scratch/users/USERNAME/\n\n# securely transfer folders from Box to shared scratch directory\nscp -r \"Box/project-folder/folder-3/\" USERNAME@shiva.ucdavis.edu:/scratch/group/GROUPNAME/",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#installing-packages-on-the-cluster",
    "href": "slurm.html#installing-packages-on-the-cluster",
    "title": "16  High-performance computing (HPC)",
    "section": "16.4 Installing packages on the cluster",
    "text": "16.4 Installing packages on the cluster\nWhen you begin working on a cluster, you will most likely encounter problems with installing packages. To install packages, login to the cluster on the command line and open a development node. Do not attempt to do this in RStudio Server, as you will have to re-do it for every new session you open.\nssh USERNAME@shiva.ucdavis.edu\n\nsdev\nYou should only have to install packages once. The cluster may require that you specify the repository where the package is downloaded from. You may also need to add an additional argument to install.packages to prevent the packages from locking after installation:\ninstall.packages(&lt;PACKAGE NAME&gt;, repos=\"https://cran.r-project.org\", \n                  INSTALL_opts = \"--no-lock\")\nIn order for some R packages to work on clusters, it is necessary to load specific software modules before running R. These must be loaded each time you want to use the package in R. For example, for spatial and random effects analyses, you may need the modules/packages below. These modules must also be loaded on the command line prior to opening R in order for package installation to work.\nmodule --force purge # remove any previously loaded modules, including math and devel\nmodule load math\nmodule load math gmp/6.1.2\nmodule load devel\nmodule load gcc/10\nmodule load system\nmodule load json-glib/1.4.4\nmodule load curl/7.81.0\nmodule load physics\nmodule load physics udunits geos\nmodule load physics gdal/2.2.1 # for R/4.0.2\nmodule load physics proj/4.9.3 # for R/4.0.2\nmodule load pandoc/2.7.3\n\nmodule load R/4.0.2\n\nR # Open R in the Shell window to install individual packages or test code\nRscript install-packages.R # Alternatively, run a package installation script in the Shell window\nFiguring out the issues with some packages will require some trial and error. If you are still encountering problems installing a package, you may have to install other dependencies manually by reading through the error messages. If you try to install a dependency from CRAN and it isn’t working, it may be a module. You can search for it using the module spider command:\nmodule spider DEPENDENCY NAME\nYou can also reach out to UC Davis HPC support for help. Visit hpc.ucdavis.edu for support information.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#testing-your-code",
    "href": "slurm.html#testing-your-code",
    "title": "16  High-performance computing (HPC)",
    "section": "16.5 Testing your code",
    "text": "16.5 Testing your code\nBoth of the following ways to test code on a cluster are recommended for making small changes, such as editing file paths and making sure the packages and source files load. You should write and test the functionality of your script locally, only testing on the cluster once major bugs are out.\n\n16.5.1 The command line\nThere are two main ways to explore and test code on computing clusters. The first way is best for users who are comfortable working on the command line and editing code in base R. Even if you are not comfortable yet, this is probably the better way because these commands will transfer between different cluster computers using Slurm.\nTypically, you will want to initially test your scripts by initiating a development node using the command sdev. This will allocate a small amount of computing resources for 1 hour. You can access R via command line using the following code.\n# open development node\nsdev\n\n# Load all the modules required by the packages you are using\nmodule load MODULE NAME  \n\n# Load R (default version)*\nmodule load R \n\n# initiate R in command line\nR\n*Note: for collaboration purposes, it’s best for everyone to work with one version of R. Check what version is being used for the project you are working on. Some packages only work with some versions of R, so it’s best to keep it consistent.\n\n\n16.5.2 RStudio Server\nFor RStudio GUI computing, UC Davis provides mercury.ucdavis.edu. This is accessed through a web browser and provides an RStudio interface. You will be prompted to authenticate with your UC Davis credentials. This is the best way to work with R for people who are not comfortable accessing & editing in base R in a Shell application.\nNote that mercury does not have SLURM, so it’s best suited for interactive work and smaller computations. For large-scale computations requiring SLURM job scheduling, use shiva.ucdavis.edu instead.\nWhen using RStudio Server, you can test your code interactively. However, do NOT use the RStudio Server’s Terminal to install packages and configure your environment for SLURM-based clusters, as you will likely need to re-do it for every session/project. For SLURM clusters, use the command line approach described earlier.\n\n\n16.5.3 Filepaths & configuration on the cluster\nIn most cases, you will want to test that the file paths work correctly on the cluster. You will likely need to add code to the configuration file in the project repository that specifies cluster-specific file paths. Here is an example:\n# set cluster-specific file paths\nif(Sys.getenv(\"LMOD_SYSHOST\")!=\"\"){\n  \n  cluster_path = paste0(Sys.getenv(\"HOME\"), \"/project-name/\")\n  \n  data_path = paste0(cluster_path, \"data/\")\n  results_path = paste0(cluster_path, \"results/\")\n}",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#storage-group-storage-access",
    "href": "slurm.html#storage-group-storage-access",
    "title": "16  High-performance computing (HPC)",
    "section": "16.6 Storage & group storage access",
    "text": "16.6 Storage & group storage access\n\n16.6.1 Individual storage\nThere are multiple places to store your files on computing clusters. Each user has their own $HOME directory as well as a $SCRATCH directory. These are directories that can be accessed via the command line once you’ve logged in to the cluster:\ncd $HOME \ncd /home/users/USERNAME # Alternatively, use the full path\n\ncd $SCRATCH\ncd /scratch/users/USERNAME # Full path\nYou can also navigate to these using the File Explorer if available through a web interface.\n$HOME typically has a volume quota (e.g., 15 GB). $SCRATCH typically has a larger volume quota (e.g., 100 TB), but files here may get deleted after a certain period of inactivity. Thus, use $SCRATCH for test files, exploratory analyses, and temporary storage. Use $HOME for long-term storage of important files and more finalized analyses.\nCheck with the UC Davis HPC documentation for specific storage options and quotas.\n\n\n16.6.2 Group storage\nThe lab may have shared $GROUP_HOME and $GROUP_SCRATCH directories to store files for collaborative use. These typically have larger quotas and may have different retention policies. You can access these via the command line or navigate to them using the File Explorer:\ncd $GROUP_HOME\ncd /home/groups/GROUPNAME\n\ncd $GROUP_SCRATCH\ncd /scratch/groups/GROUPNAME\nHowever, saving files to group storage can be tricky. You can try using the scp command in the section “Moving files to the cluster” to see if you have permission to add files to group directories. Read the next section to ensure any directories you create have the right permissions.\n\n\n16.6.3 Folder permissions\nGenerally, when we put folders in $GROUP_HOME or $GROUP_SCRATCH, it is so that we can collaborate on an analysis within the research group, so multiple people need to be able to access the folders. If you create a new folder in $GROUP_HOME or $GROUP_SCRATCH, please check the folder’s permissions to ensure that other group members are able to access its contents. To check the permissions of a folder, navigate to the level above it, and enter ls -l. You will see output like this:\ndrwxrwxrwx 2 jadebc jadebc  2204 Jun 17 13:12 myfolder\nPlease review this website to learn how to interpret the code on the left side of this output. The website also tells you how to change folder permissions. In order to ensure that all users and group members are able to access a folder’s contents, you can use the following command:\nchmod ugo+rwx FOLDER_NAME",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "slurm.html#running-big-jobs",
    "href": "slurm.html#running-big-jobs",
    "title": "16  High-performance computing (HPC)",
    "section": "16.7 Running big jobs",
    "text": "16.7 Running big jobs\nOnce your test scripts run successfully, you can submit an sbatch script for larger jobs. These are text files with a .sh suffix. Use a text editor like Sublime to create such a script. Documentation on sbatch options is available from Slurm Workload Manager (“Slurm Workload Manager: Sbatch Documentation,” n.d.). Here is an example of an sbatch script with the following options:\n\njob-name=run_inc: Job name that will show up in the SLURM system\nbegin=now: Requests to start the job as soon as the requested resources are available\ndependency=singleton: Jobs can begin after all previously launched jobs with the same name and user have ended.\nmail-type=ALL: Receive all types of email notification (e.g., when job starts, fails, ends)\ncpus-per-task=16: Request 16 processors per task. The default is one processor per task.\nmem=64G: Request 64 GB memory per node.\noutput=00-run_inc_log.out: Create a log file called 00-run_inc_log.out that contains information about the Slurm session\ntime=47:59:00: Set maximum run time to 47 hours and 59 minutes. If you don’t include this option, the cluster will automatically exit scripts after 2 hours of run time (default may vary by cluster).\n\nThe file analysis.out will contain the log file for the R script analysis.R.\n#!/bin/bash\n\n#SBATCH --job-name=run_inc\n#SBATCH --begin=now\n#SBATCH --dependency=singleton\n#SBATCH --mail-type=ALL\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=64G\n#SBATCH --mem=64G\n#SBATCH --output=00-run_inc_log.out\n#SBATCH --time=47:59:00\n\ncd $HOME/project-code-repo/2-analysis/\n\nmodule purge \n\n# load R version 4.0.2 (required for certain packages)\nmodule load R/4.0.2\n\n# load gcc, a C++ compiler (required for certain packages)\nmodule load gcc/10\n\n# load software required for spatial analyses in R\nmodule load physics gdal\nmodule load physics proj\n\nR CMD BATCH --no-save analysis.R analysis.out\nTo submit this job, save the code in the chunk above in a script called myjob.sh and then enter the following command into terminal:\nsbatch myjob.sh \nTo check on the status of your job, enter the following code into terminal:\nsqueue -u $USERNAME\n\n\n\n\n“Slurm Workload Manager: Sbatch Documentation.” n.d. SchedMD. https://slurm.schedmd.com/sbatch.html.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>High-performance computing (HPC)</span>"
    ]
  },
  {
    "objectID": "ai-tools.html",
    "href": "ai-tools.html",
    "title": "17  Working with AI",
    "section": "",
    "text": "17.1 Responsibility for validation\nAI-powered coding assistants can dramatically accelerate and improve your work, but they require careful and responsible use. Lab members who use AI tools must adhere to the following guidelines:\nYou are fully responsible for checking and validating all AI-generated code and content. AI tools can make mistakes, generate insecure code, produce incorrect logic, or suggest approaches that are inappropriate for our specific research context. Before using any AI-generated code:",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "ai-tools.html#responsibility-for-validation",
    "href": "ai-tools.html#responsibility-for-validation",
    "title": "17  Working with AI",
    "section": "",
    "text": "Carefully review the code to ensure you understand what it does\nTest the code thoroughly to verify it works as expected\nVerify that the logic is appropriate for your specific use case\nCheck that the code follows our lab’s coding standards and best practices\nEnsure the code does not introduce security vulnerabilities or data privacy issues\n\n\n\n\n\n\n\nWarning\n\n\n\nNever blindly use AI-generated code without fully understanding it. If you don’t completely understand what the AI has suggested, take the time to learn or ask a colleague for help.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "ai-tools.html#disclosure-of-ai-use",
    "href": "ai-tools.html#disclosure-of-ai-use",
    "title": "17  Working with AI",
    "section": "17.2 Disclosure of AI use",
    "text": "17.2 Disclosure of AI use\nYou must clearly state whenever you have used AI tools in your work. This is essential for transparency and reproducibility. Specifically:\n\nIn code comments, note when AI tools were used to generate or significantly modify code\nIn commit messages, mention if AI tools assisted with the changes\nIn manuscripts and reports, acknowledge AI tool usage in the methods or acknowledgments section\nIn presentations, disclose AI assistance when relevant\n\nExample code comment:\n# The following function was generated with assistance from GitHub Copilot\n# and has been reviewed and tested to ensure correctness",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "ai-tools.html#attribution-of-sources",
    "href": "ai-tools.html#attribution-of-sources",
    "title": "17  Working with AI",
    "section": "17.3 Attribution of sources",
    "text": "17.3 Attribution of sources\nWhen using AI tools to generate content that borrows from or adapts existing sources, you must ensure proper attribution. AI tools sometimes paraphrase or adapt content from documentation, guides, or other resources without clearly indicating the original source. It is your responsibility to:\n\nAsk the AI tool to identify and properly cite sources when it borrows or adapts content\nVerify that any content the AI generates includes appropriate citations\nAdd citations yourself if the AI fails to do so\nFollow appropriate attribution practices for the type of content (code comments, documentation, academic writing, etc.)\n\nWhen instructing AI tools to create documentation or written content, explicitly request that they provide proper attribution for any borrowed or adapted material. For example: “Please quote from and paraphrase [source], with proper attribution” rather than simply asking it to summarize information on a topic.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "ai-tools.html#coding-agents",
    "href": "ai-tools.html#coding-agents",
    "title": "17  Working with AI",
    "section": "17.4 Coding Agents",
    "text": "17.4 Coding Agents\nWe recommend working with AI coding agents to help you code.\n\n17.4.1 What are AI coding agents?\nAI coding agents are AI agents specialized for coding. They differ from other AI coding tools in important ways:\nCompared to inline coding assistants (like traditional autocomplete), coding agents work autonomously rather than providing suggestions as you type. They can navigate entire codebases, execute commands, and complete multi-step tasks without constant human guidance.\nCompared to AI chatbots (like ChatGPT or Claude), coding agents don’t just generate code snippets in conversation—they actively interact with your development environment. While chatbots require you to copy code from a chat window and manually integrate it into your project, coding agents directly read your codebase, make changes to files, run tests and build commands, and create pull requests with their proposed changes. Chatbots are conversational assistants; coding agents are autonomous development tools.\nCoding agents are autonomous software programs that can:\n\nUnderstand and execute complex tasks: Coding agents can interpret natural language instructions and break them down into actionable development tasks\nNavigate and modify codebases: They can read, understand, and edit multiple files across a repository to implement features or fix bugs\nRun tools and commands: Coding agents can execute build commands, run tests, use linters, and interact with development tools\nMake decisions autonomously: They can plan their approach, make technical decisions, and adjust their strategy based on results\nWork iteratively: Coding agents can test their changes, identify issues, and refine their solutions through multiple iterations\nCreate comprehensive solutions: They can implement complete features that span multiple files, including code, tests, and documentation\n\nCoding agents operate in isolated environments where they can safely experiment and validate changes before proposing them. This allows them to work more independently than inline coding assistants, which require step-by-step human direction. The agent workflow typically involves analyzing requirements, planning an implementation, making changes, testing those changes, and creating a pull request with the results.\nWhile coding agents can handle substantial development tasks, they still require human oversight and review. The human developer remains responsible for:\n\nReviewing the agent’s work\nEnsuring the solution meets requirements\nVerifying code quality and security\nMaking the final decision to merge changes\n\n\n\n17.4.2 Relative Advantages of AI and Humans\nAI coding agents and human coders have complementary strengths. Understanding these differences helps you decide when to delegate work to agents and when to handle tasks yourself.\n\n17.4.2.1 Comparative Strengths: Humans vs. AI Agents\nThe table below summarizes the relative advantages of human coders and AI coding agents across different types of tasks:\n\n\n\nTable 17.1: Relative advantages of humans and AI coding agents\n\n\n\n\n\n\n\n\n\n\nTask Type\nHumans 😊\nAI Agents 🤖\n\n\n\n\nCreative thinking\n😊 Humans excel at understanding context, handling ambiguous requirements, and thinking creatively about novel problems\n😞 AI agents struggle with ambiguous requirements and creative problem-solving in unfamiliar domains\n\n\nAlgorithmic thinking\n😞 Humans make mistakes when following repetitive instructions and may introduce inconsistencies\n😊 AI agents excel at executing well-defined, repetitive tasks with precision and consistency\n\n\n\n\n\n\nOr, if you prefer a more visual representation:\n\n\n\nTable 17.2: Relative advantages of humans and Agents\n\n\n\n\n\n\n\n\n\n\n\nHumans\nAI agents\n\n\n\n\nCreative thinking\n\n\n\n\nAlgorithmic thinking\n\n\n\n\n\n\n\n\nThis pattern mirrors the evolution of programming itself. Just as almost no one writes machine code anymore because higher-level languages and compilers handle those details, most developers will increasingly spend less time writing low-level code. Instead, you’ll describe what the system needs to do as clearly as possible, and AI agents will handle many of the computational and coding details.\nFor most tasks, you won’t need to step in and manipulate code yourself. However, you’ll still need strong coding skills to:\n\nSupervise and validate AI-generated code\nHandle edge cases that agents struggle with\nMake creative decisions about architecture and design\nUnderstand when agent suggestions are incorrect or suboptimal\n\n\n\n17.4.2.2 Future Developments: World Models\nAs AI technology advances, the distinction between these strengths may shift. Yann LeCun, 2019 Turing Award winner and AI researcher at Meta and NYU, advocates for developing “world models”—AI systems that understand and reason about the physical world, not just language patterns (LeCun 2022).\nWorld models aim to give AI systems:\n\nPersistent memory and reasoning: Understanding that persists across interactions\nPhysical world understanding: Reasoning about how things work in reality, not just in text\nBetter handling of ambiguity: Using world knowledge to interpret unclear requirements\n\nAs these technologies mature, AI agents may become better at tasks requiring contextual understanding and creative problem-solving. This makes it even more important to develop strong supervision and validation skills now, so you can effectively work with increasingly capable AI systems.\n\n\n\n17.4.3 How to Work with Coding Agents\nGitHub Copilot coding agents can be used in several ways to automate development tasks:\n\n17.4.3.1 Assigning Issues to Copilot\nYou can assign GitHub Issues directly to @copilot just like you would assign to a human collaborator:\n\nOn GitHub.com: Navigate to an issue and assign it to Copilot in the assignees section\nIn VS Code: In the GitHub Pull Requests or Issues view, right-click an issue and select “Assign to Copilot”\nFrom Copilot Chat: Delegate tasks to Copilot directly from the chat interface in supported editors\n\n\n\n17.4.3.2 The Agent Workflow\nOnce assigned an issue, the coding agent follows an autonomous workflow:\n\nAnalysis: Reviews the issue description, related discussions, repository instructions, and codebase context\nPlanning: Determines what changes are needed and creates a work plan\nDevelopment: Works in an isolated GitHub Actions environment, modifies code, runs tests and linters, and validates changes\nPull Request Creation: Creates a draft pull request with implemented changes, audit logs, and a summary of modifications\nReview and Iteration: You review the PR and can request changes; the agent will iterate based on your feedback\n\n\n\n17.4.3.3 Example: This Document\nThis very section you’re reading was created through the coding agent workflow:\n\nIssue created: Issue #42 requested adding discussion about benefits and hazards of coding agents, including a Matrix film connection and best practices\nAgent assigned: The issue was assigned to @copilot\nWork completed: The agent analyzed the requirements, reviewed the repository structure, and implemented the changes across multiple files\nPull request: PR #50 was created with comprehensive content about coding agents, including this “How to Work with Coding Agents” section, benefits and hazards discussion, best practices, and firewall configuration details\nIteration: The PR received feedback comments requesting additional links, improved wording, and this example section—all of which the agent addressed through follow-up commits\n\nThis demonstrates the full lifecycle of working with a coding agent on a real documentation task.\n\n\n17.4.3.4 Collaborating with Coding Agents\nBetween iterations of asking coding agents to extend a PR, human collaborators can also push changes directly to the PR branch. This allows for a collaborative workflow where both humans and agents contribute:\n\nHuman contributions: You can make quick fixes, add content, or refine the agent’s work by pushing commits to the same branch\nAgent iterations: After your changes, you can ask the agent to continue working on additional requirements\n\nImportant: Try to avoid pushing changes while the coding agent is actively working. Simultaneous edits can produce conflicting diffs that:\n\nNeed to be manually resolved\nMay confuse both human and AI collaborators\nCould result in lost work or merge conflicts\n\nBest practice: Wait for the agent to complete its current iteration (indicated by the PR being updated) before pushing your own changes to the branch. Then assign new work to the agent for the next iteration.\n\n\n17.4.3.5 Directly Prompting for Pull Requests\nYou can also prompt Copilot to create pull requests without first creating an issue:\n\nUse Copilot Chat in your editor to describe the changes you want\nThe agent will analyze your request and create a pull request\nThis is useful for quick fixes or well-defined tasks\n\n\n\n17.4.3.6 Important Safeguards\n\nHuman approval required: Coding agents cannot merge their own changes\nBranch restrictions: Agents can only push to their own branches (e.g., copilot/*)\nFull transparency: All agent actions are logged and visible in the PR\n\n\n\n17.4.3.7 Workflow Approval Requirements\nWhen GitHub Copilot creates or updates a pull request, it cannot automatically trigger GitHub Actions workflows. You must manually approve each workflow run by clicking the approval button in the Actions tab or on the PR.\nThis manual approval requirement is a security measure that prevents potentially malicious or unintended code execution. Because Copilot can modify any file in the repository—including workflow files themselves or scripts called by workflows—allowing automatic workflow execution could create security vulnerabilities.\nKey points:\n\nNo automatic approval: There is currently no way to bypass manual workflow approval for Copilot PRs, even if you are the repository owner\nSecurity reasoning: Copilot could modify workflow files (.github/workflows/*.yml) or scripts they execute, potentially injecting malicious code\nImpact on workflow: This means you need to actively monitor and approve workflow runs as Copilot iterates on your issue, which can slow down the development cycle\n\nWorkaround considerations:\nSome users have discussed using Personal Access Tokens (PATs) to allow Copilot to trigger workflows on your behalf, but this approach has security implications and should be carefully evaluated before implementation.\nFor more details and community discussion about this limitation, see:\n\nGitHub Community Discussion #162826: Discussion about workflow approval requirements\nGitHub Community Discussion #183966: Product feedback on this topic\n\nFor detailed instructions, see GitHub Copilot coding agent documentation.\n\n\n\n17.4.4 Useful Prompt Formats\nWhen working with coding agents, using clear and specific prompts helps achieve better results. Here are some useful prompt formats that you can use when requesting assistance from coding agents:\n\n17.4.4.1 Common Task Patterns\nTidying up code:\n\n“tidy up [file, function, module, whole project]”\nUseful for improving code organization, consistency, and readability\nExample: “tidy up the data processing module”\n\nAddressing failing workflows:\n\n“address failing workflows”\nHelps fix continuous integration (CI) failures, build errors, or test failures\nExample: “address failing workflows in the GitHub Actions pipeline”\n\nDecomposing code:\n\n“decompose [function, quarto-file, etc]”\nBreaks down large or complex code into smaller, more manageable pieces\nExample: “decompose this analysis function into separate helper functions”\n\nUpdating content:\n\n“update [links, content, etc]”\nRefreshes outdated information, fixes broken links, or modernizes code\nExample: “update all package URLs in the documentation”\n\nExpanding documentation:\n\n“expand [a section in a document]”\nAdds more detail, examples, or explanation to existing content\nExample: “expand the section on data validation with practical examples”\n\nCondensing content:\n\n“condense [a section in a document]”\nReduces verbosity while preserving essential information\nExample: “condense the installation instructions to be more concise”\n\nClarifying content:\n\n“clarify [a section in a document]”\nImproves clarity, removes ambiguity, or simplifies complex explanations\nExample: “clarify the explanation of the analysis workflow”\n\n\n\n17.4.4.2 Tips for Effective Prompts\n\nBe specific: Include file names, function names, or specific sections when possible\nProvide context: Explain what you want to achieve and why\nSet boundaries: Specify what should or shouldn’t change\nRequest validation: Ask the agent to test or verify its changes when appropriate\n\n\n\n\n17.4.5 Addressing Failing GitHub Actions Workflows\nWhen GitHub Actions workflows fail, you can use Copilot to help diagnose and fix the issues. However, it’s important to use the right prompts depending on whether the problem is in your code or in the workflow configuration itself.\n\n17.4.5.1 Scenario 1: Code Issues Found by Workflows (Most Common)\nWhen to use: The workflow is functioning correctly, but it’s detecting problems in your code (e.g., failing tests, linting errors, build failures).\nWhat you want: Fix the code issues without modifying the workflow files themselves.\nRecommended prompts:\n\n“fix the code issues found by the failing workflows”\n“address the linting errors reported in the GitHub Actions checks”\n“fix the test failures in the CI pipeline”\n“resolve the build errors shown in the workflow logs”\n\nExample: If your R package has failing tests detected by usethis::use_github_action(\"check-standard\"), you want Copilot to fix the test failures in your R code, not modify the workflow YAML file.\nWhy this matters: These prompts make it clear that you want code changes, not workflow changes. This helps prevent the agent from unnecessarily modifying your carefully-configured CI/CD pipeline.\n\n\n17.4.5.2 Scenario 2: Issues with Workflow Files Themselves\nWhen to use: The workflow configuration itself has problems (e.g., syntax errors in YAML, incorrect job definitions, outdated actions).\nWhat you want: Fix the workflow files, but with extreme caution due to security implications.\nRecommended prompts:\n\n“fix the syntax error in the GitHub Actions workflow file at line X”\n“update the workflow to use the latest version of action Y”\n“correct the job configuration in .github/workflows/check-standard.yaml”\n\nImportant considerations:\n\n\n\n\n\n\nWarning\n\n\n\nSecurity Warning\nWorkflow files have access to repository secrets and can execute arbitrary code. Before accepting any changes to workflow files:\n\nReview every line of the proposed changes\nVerify the changes only address the specific issue\nCheck that no new secret access or command execution has been added\nTest in a safe environment if possible\n\nSee Section 17.4.7 for more details on workflow file security.\n\n\nWhen to do it yourself: Workflow syntax errors and configuration issues are often faster to fix manually than with Copilot, especially if you’re familiar with GitHub Actions. See Section 17.4.10 for more guidance.\n\n\n17.4.5.3 Scenario 3: Uncertain Which Scenario Applies\nWhen to use: You’re not sure whether the failure is due to code issues or workflow configuration problems.\nRecommended approach:\n\nFirst, examine the workflow logs:\n\nLook at the error messages in the GitHub Actions tab\nIdentify whether the error is in your code or the workflow itself\nCommon code issues: test failures, linting errors, compilation errors\nCommon workflow issues: YAML syntax errors, missing actions, permission errors\n\nUse a diagnostic prompt:\n\n“examine the failing workflow logs and identify whether the issue is in the code or the workflow configuration”\n“diagnose the root cause of the workflow failure”\n\nThen use the appropriate scenario above: Once you understand the issue, use the specific prompts from Scenario 1 or 2.\n\nExample workflow:\n1. Prompt: \"examine the failing workflow logs and identify the issue\"\n2. Copilot responds: \"The workflow is failing because of linting errors\n   in src/analysis.R\"\n3. Prompt: \"fix the linting errors in src/analysis.R\"\n\n\n17.4.5.4 Additional Resources\n\nSee Section 6.7 for setting up GitHub Actions workflows\nSee Section 17.4.7 and Section 17.4.6 for security considerations with workflow files\nSee Section 17.4.10 for guidance on when to use Copilot vs. fixing issues yourself\nSee the GitHub Actions documentation for workflow syntax and troubleshooting\n\n\n\n\n17.4.6 Benefits and Hazards\nCoding agents are powerful programs that can work autonomously. They create pull requests that propose changes to the code in our repositories, potentially including their own configuration files and our automated workflows. They can work powerfully on our behalf, but they require careful oversight and control to ensure they serve our interests and that we understand the consequences of their actions.\nCoding agents offer several advantages:\n\nBuilt-in transparency: Coding agents create a clear record of their role in your work through commit history and code suggestions\nContext-aware suggestions: Coding agents understand your codebase and can make contextually relevant suggestions\nIntegration with version control: Using coding agents within GitHub ensures that AI-assisted changes are tracked alongside all other code changes\nInteractive workflow: Coding agents’ interactive nature encourages you to review and modify suggestions rather than blindly accepting them\nAccelerated development: Coding agents can help you write boilerplate code, refactor existing code, and implement common patterns more quickly\nLearning opportunities: Coding agents can suggest approaches or techniques you may not have considered, helping you expand your coding knowledge\n\nHowever, coding agents also come with significant hazards:\n\nOver-reliance: Depending too heavily on coding agents can atrophy your coding skills and understanding\nSubtle bugs: AI-generated code may contain logic errors that are not immediately obvious\nSecurity vulnerabilities: Coding agents may introduce insecure patterns or fail to follow security best practices\nInappropriate solutions: AI may suggest solutions that work but are not optimal for your specific research context or constraints\nHidden biases: Coding agents may perpetuate coding patterns or approaches that reflect biases in their training data\nFalse confidence: Well-formatted, professional-looking code from AI can mask underlying problems and reduce critical review\nWorkflow manipulation risks: Coding agents that modify CI/CD workflows (.github/workflows/*.yml) or setup configurations can inadvertently or maliciously compromise repository security, expose secrets, or execute harmful commands\n\n\n17.4.6.1 Further reading/viewing\n\nI Robot (Asimov 1950)\nDune (Herbert 1965)\n“2001: A Space Odyssey” (1968)\n“Terminator 3: Rise of the Machines” (2003)\n“The Matrix” (1999)\n“Blade Runner” (1982)\n“WarGames” (1983)\nBattlestar Galactica (2004) (“Battlestar Galactica” 2004)\nEnder’s Game (Card 1985)\n“The Humans are Dead” (Flight of the Conchords 2007)\n\n\n\n\nAgents\n\n\n\n\n\n17.4.7 Best Practices for Safe and Successful Use\nTo work with coding agents safely and successfully:\n\nMaintain active supervision: Never assume AI-generated code is correct. Review every line critically.\nUnderstand before accepting: If you don’t understand what the code does, don’t use it. Take time to learn or ask a colleague.\nTest thoroughly: AI-generated code must be tested as rigorously as code you write yourself. Don’t skip testing because “the AI wrote it.”\nStart small: Begin with small, well-defined tasks to build confidence and understanding of the agent’s capabilities and limitations.\nVerify logic and assumptions: Check that the AI hasn’t made incorrect assumptions about your data, requirements, or scientific context.\nReview for security: Explicitly check for security issues, especially when handling sensitive data or user input.\nIterate and refine: Use coding agents as a starting point, not an endpoint. Refine and improve the generated code.\nMaintain coding practice: Regularly write code yourself to maintain and develop your skills. Don’t let the agent do everything.\n\n\n\n\n\n\n\nWarningCritical: Exercise Extreme Caution with Workflow Files\n\n\n\nBe especially careful when allowing coding agents to edit GitHub Actions workflows or CI/CD configurations. These files control automated processes that can:\n\nAccess secrets and credentials\nDeploy code to production\nExecute arbitrary commands in your repository\n\nNever allow a coding agent to edit workflow files (especially .github/workflows/*.yml or copilot-setup-steps.yml) without thorough manual review. Before approving any workflow run, always check if the workflow files themselves have been modified. Malicious or erroneous changes to workflows can compromise your entire repository and its secrets.\n\n\nWhen using coding agents, work interactively with the AI suggestions: review, modify, and test them rather than accepting them wholesale. This interactive approach helps ensure code quality and deepens your understanding of the code.\nRemember: AI tools are assistants, not replacements for your expertise and judgment. The quality and correctness of your work remains your responsibility.\n\n\n17.4.8 Firewall and Network Configuration\nCoding agents require specific network access to function properly. If a coding agent is running behind a corporate firewall or on a restricted network, you may need to configure allowlists to enable coding agent functionality.\n\n17.4.8.1 Built-in Agent Firewall\nCoding agents run in a GitHub Actions environment with a built-in firewall that limits internet access by default. This firewall helps protect against:\n\nData exfiltration\nAccidental leaks of sensitive information\nExecution of malicious instructions\n\nBy default, the agent’s firewall allows access to:\n\nCommon OS package repositories (Debian, Ubuntu, Red Hat, etc.)\nPopular container registries (Docker Hub, Azure Container Registry, AWS ECR, etc.)\nLanguage-specific package registries (npm, PyPI, Maven, RubyGems, etc.)\nCommon certificate authorities for SSL validation\n\nFor the complete list of allowed hosts, see the Copilot allowlist reference.\n\n\n17.4.8.2 Customizing Agent Firewall Settings\nIn your repository’s “Coding agent” settings page, you can:\n\nAdd custom hosts to the allowlist (for internal dependencies or additional registries)\nOpt out of the default recommended allowlist for stricter security\nDisable the firewall entirely (not recommended)\n\nIf a coding agent’s request is blocked by the firewall, a warning will be added to the pull request or comment, detailing the blocked address and the command that triggered it.\nFor more information, see Customizing or disabling the firewall for GitHub Copilot coding agent.\n\n\n17.4.8.3 Recommended URLs for Data Science Repositories\nFor data science and R-focused repositories, we recommend adding the following URLs to your Copilot allowlist. These sites are safe, reputable sources of documentation and packages that coding agents may need to access:\nR Package Documentation and Ecosystems:\n\ntidyverse.org - {tidyverse} package documentation and learning resources\nr-lib.org - Core R infrastructure packages ({devtools}, {testthat}, {usethis}, etc.)\nggplot2.tidyverse.org - {ggplot2} visualization package\ndplyr.tidyverse.org - {dplyr} data manipulation package\ntidyr.tidyverse.org - {tidyr} data tidying package\npurrr.tidyverse.org - {purrr} functional programming package\nreadr.tidyverse.org - {readr} data reading package\nstringr.tidyverse.org - {stringr} string manipulation package\nforcats.tidyverse.org - {forcats} categorical data package\n\nR Package Repositories:\n\ncran.r-project.org - The Comprehensive R Archive Network\ncloud.r-project.org - CRAN mirror (cloud-based)\ndocs.ropensci.org - rOpenSci package documentation (e.g., {targets})\nrdatatable.gitlab.io - {data.table} package documentation\nrstudio.github.io - RStudio-maintained packages (e.g., {renv})\n\nCode Style and Quality Tools:\n\nstyler.r-lib.org - {styler} code formatting package\nlintr.r-lib.org - {lintr} code linting package\nroxygen2.r-lib.org - {roxygen2} documentation package\nstyle.tidyverse.org - Tidyverse style guide\n\nGeneral Documentation and Reference:\n\nen.wikipedia.org - General reference and technical documentation\nr-project.org - Official R project website\nquarto.org - Quarto publishing system documentation\npandoc.org - Pandoc document converter documentation\n\nGitHub Organizations (for package repositories):\n\ngithub.com/tidyverse/* - Tidyverse package source code\ngithub.com/r-lib/* - R-lib package source code\ngithub.com/rstudio/* - RStudio package source code\ngithub.com/ropensci/* - rOpenSci package source code\n\n\n\n\n\n\n\nTipWhen to Add These URLs\n\n\n\nAdd these URLs to your repository’s allowlist if:\n\nCoding agents report blocked access to these sites\nYou’re working on R or data science projects that use these packages\nYou want agents to access current documentation during code generation\n\nYou can add URLs selectively based on your project’s specific dependencies rather than adding all URLs at once.\n\n\n\n\n\n\n\n\nNoteSafety of These URLs\n\n\n\nAll URLs listed here are:\n\nMaintained by reputable organizations (Tidyverse, RStudio/Posit, R Core Team, rOpenSci)\nWidely used in the R community\nFocused on documentation and package distribution\nSafe for coding agents to access\n\nThese sites do not host user-generated content or allow arbitrary code execution, making them appropriate for inclusion in your allowlist.\n\n\n\n\n\n17.4.9 Configuring the Agent Environment\nThe .github/workflows/copilot-setup-steps.yml file allows you to customize the development environment in which the GitHub Copilot coding agent operates. This file preinstalls tools and dependencies so that Copilot can build, test, and lint your code more reliably.\n\n17.4.9.1 Why Configure the Environment?\nWhile Copilot can discover and install dependencies through trial and error, this can be slow and unreliable. Additionally, Copilot may be unable to access private dependencies. Preconfiguring the environment ensures:\n\nFaster agent startup and execution\nMore reliable builds and tests\nAccess to private or authenticated dependencies\nConsistent development environment across all agent sessions\n\n\n\n17.4.9.2 File Location and Structure\nThe workflow file must be located at .github/workflows/copilot-setup-steps.yml in your repository’s default branch. It follows GitHub Actions workflow syntax but must contain a single job named copilot-setup-steps.\n\n\n17.4.9.3 Basic Configuration Example\nSee Appendix: Copilot Setup Steps File for the configuration used in this repository (adapted for R and Quarto projects).\n\n\n17.4.9.4 Configurable Options\nYou can customize only these specific settings in the copilot-setup-steps job:\n\nsteps: Setup commands and actions to run\npermissions: Access permissions (typically contents: read)\nruns-on: Runner type (Ubuntu x64 Linux only)\nservices: Database or service containers\nsnapshot: Save environment state\ntimeout-minutes: Maximum 59 minutes\n\nAll other workflow settings are ignored by Copilot.\n\n\n17.4.9.5 Common Setup Tasks\nFor Node.js/TypeScript projects:\n- name: Set up Node.js\n  uses: actions/setup-node@v4\n  with:\n    node-version: \"20\"\n    cache: \"npm\"\n\n- name: Install dependencies\n  run: npm ci\nFor Python projects:\n- name: Set up Python\n  uses: actions/setup-python@v5\n  with:\n    python-version: \"3.11\"\n\n- name: Install dependencies\n  run: pip install -r requirements.txt\nFor R projects:\n- name: Set up R\n  uses: r-lib/actions/setup-r@v2\n  with:\n    r-version: 'release'\n\n- name: Install R dependencies\n  uses: r-lib/actions/setup-renv@v2\n\n\n17.4.9.6 Environment Variables and Secrets\nTo set environment variables for Copilot:\n\nNavigate to your repository’s Settings\nGo to Environments\nSelect or create the copilot environment\nAdd environment variables or secrets as needed\n\nUse secrets for sensitive values like API keys or passwords.\n\n\n17.4.9.7 Testing Your Configuration\nThe workflow runs automatically when you modify copilot-setup-steps.yml, allowing you to validate changes in pull requests. You can also manually trigger the workflow from the repository’s Actions tab.\nSetup logs appear in the agent session logs when Copilot starts working. If a step fails, Copilot will skip remaining steps and begin working with the current environment state.\n\n\n17.4.9.8 Advanced Configuration\nLarger runners: For projects requiring more resources, you can use larger GitHub-hosted runners:\njobs:\n  copilot-setup-steps:\n    runs-on: ubuntu-4-core\nSelf-hosted runners (ARC): For access to internal resources or private registries, use Actions Runner Controller (ARC) self-hosted runners:\njobs:\n  copilot-setup-steps:\n    runs-on: arc-scale-set-name\nNote: When using self-hosted runners, you must disable Copilot’s integrated firewall in repository settings and configure appropriate network security controls.\nGit Large File Storage (LFS): If your repository uses Git LFS:\n- uses: actions/checkout@v4\n  with:\n    lfs: true\n\n\n17.4.9.9 Further Reading\nFor complete details, see Customizing the development environment for GitHub Copilot coding agent.\n\n\n\n17.4.10 When to use a coding agent\nCoding agent sessions are currently1 considered “premium requests”, which are limited resources; see https://github.com/features/copilot/plans for details. So, use coding agents sparingly. Use them for complex changes that would be difficult or time-consuming for you to complete by hand. Coding agents also take time to get configured for work, every time you make a request. See https://docs.github.com/en/copilot/how-tos/use-copilot-agents/coding-agent/customize-the-agent-environment#preinstalling-tools-or-dependencies-in-copilots-environment for ways to reduce that startup time, but it will never be 0. If you can complete the task faster than the coding agent can, you should probably do it yourself. For example, when you have errors in the spell-check or lint workflows, you can often fix them faster than Copilot can. Similarly, when reviewing Copilot’s PRs, you can often make direct changes to the branch faster than you could write clear review comments and get Copilot to address them.\nAlso, the less we practice, the weaker our skills get, and the harder it is for us to supervise the agents and make sure they are actually doing what we want them to do, the way we want them to do it. You should exercise your own coding skills regularly, just like you would for any other skill you want to maintain.\n\n\n17.4.11 Editing with .docx files\nGitHub Copilot coding agents can read Microsoft Word (.docx) files, including tracked changes and comments. This enables a hybrid editing workflow where:\n\nLab members can export Quarto content to Word format for review\nReviewers can make edits, add tracked changes, and insert comments in Word\nCoding agents can read the .docx file and translate the edits back to Quarto format\n\nWhen using this workflow, make sure to explicitly instruct the coding agent to:\n\nExamine and apply all tracked changes in the .docx file\nRead and address all comments in the .docx file\nTranslate edits from Word formatting to appropriate Quarto/markdown syntax\n\nThis approach makes it easier for collaborators who are more comfortable with Word to contribute to the lab manual while maintaining the source files in Quarto format.\n\n\n17.4.12 Copilot Instructions for this Repository\nThe .github/copilot-instructions.md file in this repository contains specific instructions and guidelines for GitHub Copilot coding agents when working with the lab manual. This file helps ensure that AI-generated contributions follow the lab’s formatting standards, coding conventions, and documentation practices.\nThe copilot instructions file specifies:\n\nMarkdown and Quarto formatting rules (e.g., blank lines before lists, line breaks in prose)\nR code style guidelines (e.g., using native pipe |&gt;, following tidyverse style)\nFile organization patterns (e.g., using Quarto includes for modular content)\nHow to work with DOCX files for hybrid editing workflows\nRepository-specific best practices\n\nBy having these instructions in .github/copilot-instructions.md, we ensure that coding agents produce consistent, high-quality contributions that align with the lab’s established practices. This reduces the review burden and helps maintain consistency across all contributions to the lab manual, whether made by humans or AI assistants.\nSee Appendix: Copilot Instructions File for the complete file.\n\n\n\n\n“2001: A Space Odyssey.” 1968. Film. https://en.wikipedia.org/wiki/2001:_A_Space_Odyssey_(film).\n\n\nAsimov, Isaac. 1950. “I, Robot.” New York: Novel; Gnome Press. https://search.library.ucdavis.edu/permalink/01UCD_INST/9fle3i/alma990000226350403126.\n\n\n“Battlestar Galactica.” 2004. Television Series. https://en.wikipedia.org/wiki/Battlestar_Galactica_(2004_TV_series).\n\n\n“Blade Runner.” 1982. Film. https://en.wikipedia.org/wiki/Blade_Runner.\n\n\nCard, Orson Scott. 1985. “Ender’s Game.” Novel; Tor Books. https://en.wikipedia.org/wiki/Ender%27s_Game.\n\n\nFlight of the Conchords. 2007. “The Humans Are Dead.” Music Video. https://www.youtube.com/watch?v=B1BdQcJ2ZYY.\n\n\nHerbert, Frank. 1965. “Dune.” Novel. https://en.wikipedia.org/wiki/Organizations_of_the_Dune_universe#Thinking_machines.\n\n\nLeCun, Yann. 2022. “A Path Towards Autonomous Machine Intelligence.” Meta AI Research; New York University; Technical Report. https://openreview.net/forum?id=BZ5a1r-kVsf.\n\n\n“Terminator 3: Rise of the Machines.” 2003. Film. https://en.wikipedia.org/wiki/Terminator_3:_Rise_of_the_Machines.\n\n\n“The Matrix.” 1999. Film. https://en.wikipedia.org/wiki/The_Matrix.\n\n\n“WarGames.” 1983. Film. https://en.wikipedia.org/wiki/WarGames.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "ai-tools.html#footnotes",
    "href": "ai-tools.html#footnotes",
    "title": "17  Working with AI",
    "section": "",
    "text": "2026-01-10↩︎",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Working with AI</span>"
    ]
  },
  {
    "objectID": "checklists.html",
    "href": "checklists.html",
    "title": "18  Checklists",
    "section": "",
    "text": "18.1 Pre-analysis plan checklist\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "checklists.html#pre-analysis-plan-checklist",
    "href": "checklists.html#pre-analysis-plan-checklist",
    "title": "18  Checklists",
    "section": "",
    "text": "Brief background on the study (a condensed version of the introduction section of the paper)\nHypotheses / objectives\nStudy design\nDescription of data\nDefinition of outcomes\nDefinition of interventions / exposures\nDefinition of covariates\nStatistical power calculation\nStatistical model description\nCovariate selection / screening\nStandard error estimation method\nMissing data analysis\nAssessment of effect modification / subgroup analyses\nSensitivity analyses\nNegative control analyses",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "checklists.html#code-checklist",
    "href": "checklists.html#code-checklist",
    "title": "18  Checklists",
    "section": "18.2 Code checklist",
    "text": "18.2 Code checklist\n\nDoes the script run without errors?\nIs code self-contained within repo and/or associated Box folder?\nIs all commented out code / remarks removed?\nDoes the header accurately describe the process completed in the script?\nIs the script pushed to its github repository?\nDoes the code adhere to the coding style guide?\nAre all warnings ignorable? Should any warnings be intentionally suppressed or addressed?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "checklists.html#manuscript-checklist",
    "href": "checklists.html#manuscript-checklist",
    "title": "18  Checklists",
    "section": "18.3 Manuscript checklist",
    "text": "18.3 Manuscript checklist\nThis is adapted in part from How to tackle the reproducibility crisis in ten steps (Baker 2019).\n\nHave you completed the relevant reporting checklist, if applicable? (See EQUATOR Network (“EQUATOR Network: Enhancing the QUAlity and Transparency of Health Research,” n.d.) for a collection of checklists)\nAre the study results within the manuscript replicable (i.e., if you rerun the code in the study’s repository, the tables and figures will be exactly replicated?)\nIs a target journal selected?\nIs the title declarative, in other words, does it state the object/findings rather than suggest them?\nIs the word count of the manuscript close to the target journal’s allowance?\nDoes the manuscript adhere to the formatting guide of the target journal?\nDoes the manuscript use a consistent voice (passive or active – usually active is preferred … pun intended)?\nIs each figure and table (including supplementary material) referenced in the main text?\nIs there a caption for each figure and table (including supplementary material)?\nAre tables/figures and supplementary material numbered in accordance with their appearance in the main text?\nDoes the text use past tense if it is reporting research findings or future tense if it is a study protocol?\nDoes the text avoid subjective wording (e.g., “interesting”, “dramatic”)?\nDoes the text use minimal abbreviations, and are all abbreviations defined at first use?\nDoes the text avoid directionless words? (e.g., instead of writing, ‘Precipitation influences disease risk’, write, ‘Precipitation was associated with increased disease risk’).\nDoes the text avoid making causal claims that are not supported by the study design? Be careful about the words “effect”, “increase”, and “decrease”, which are often interpreted as causal.\nDoes the text avoid describing results with the word “significant”, which can easily be confused with statistical significance? (see references on this topic here)\nHave you drafted author contributions? Do they follow the CRediT: Contributor Roles Taxonomy (“CRediT: Contributor Roles Taxonomy,” n.d.) for author contributions?",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "checklists.html#figure-checklist",
    "href": "checklists.html#figure-checklist",
    "title": "18  Checklists",
    "section": "18.4 Figure checklist",
    "text": "18.4 Figure checklist\n\nAre the x-axis and y-axis labeled?\nIf the figure includes panels, is each panel labeled?\nAre there sufficient numerical / text labels and breaks on the x-axis and y-axis?\nIs the font size appropriate (i.e., large enough to read, not so large that it distracts from the data presented in the figure?)\nAre the colors used colorblind friendly? See a colorblind-friendly palette here, a neat palette generator with colorblind options here, and an article on why this matters: The misuse of colour in science communication (Crameri, Shephard, and Heron 2020)\nAre colors/shapes/line types defined in a legend?\nAre the legends and other labels easy to understand with minimal abbreviations?\nIf there is overplotting, is transparency used to show overlapping data?\nAre 95% confidence intervals or other measures of precision shown, if applicable?\n\n\n\n\n\nBaker, Monya. 2019. “How to Tackle the Reproducibility Crisis in Ten Steps.” Nature. https://doi.org/10.1038/d41586-019-01431-z.\n\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The Misuse of Colour in Science Communication.” Nature Communications 11. https://doi.org/10.1038/s41467-020-19160-7.\n\n\n“CRediT: Contributor Roles Taxonomy.” n.d. PLOS ONE. https://journals.plos.org/plosone/s/authorship.\n\n\n“EQUATOR Network: Enhancing the QUAlity and Transparency of Health Research.” n.d. EQUATOR Network. https://www.equator-network.org/.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Checklists</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "19  Resources",
    "section": "",
    "text": "19.1 Resources for R\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung and Kunal Mishra",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#resources-for-r",
    "href": "resources.html#resources-for-r",
    "title": "19  Resources",
    "section": "",
    "text": "19.1.1 Books and Comprehensive Guides\n\nR for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) - comprehensive introduction to doing data science with R\nR Packages (Wickham and Bryan 2023) - complete guide to R package development\nAdvanced R (Wickham 2019) - deep dive into R programming and internals\nMastering Shiny (Wickham 2021) - comprehensive guide to building web applications with Shiny\nEngineering Production-Grade Shiny Apps (Fay et al. 2021) - best practices for production Shiny applications\nHappy Git and GitHub for the useR (Bryan 2023) - guide to using Git and GitHub with R\nJade’s R-for-epi course\n\n\n\n19.1.2 UC Davis DataLab Workshops and Tutorials\nThe UC Davis DataLab provides extensive workshops and learning materials for data science:\n\nWorkshop Index - comprehensive catalog of all DataLab workshops\nR Basics Workshop - foundational R programming for beginners\nResearch Toolkits - in-depth guides for research tools and methods\nInstall Guides - setup instructions for data science software\n\n\n\n19.1.3 Cheat Sheets\n\ndplyr and tidyr cheat sheet\nggplot cheat sheet\ndata table cheat sheet\nRMarkdown cheat sheet\n\n\n\n19.1.4 Style and Best Practices\n\nHadley Wickham’s R Style Guide\n\n\n\n19.1.5 Tidy Evaluation Resources\n\nTidy Eval in 5 Minutes (video)\nTidy Evaluation (e-book)\nData Frame Columns as Arguments to Dplyr Functions (blog)\nStandard Evaluation for *_join (stackoverflow)\nProgramming with dplyr (package vignette)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#resources-for-git-github",
    "href": "resources.html#resources-for-git-github",
    "title": "19  Resources",
    "section": "19.2 Resources for Git & Github",
    "text": "19.2 Resources for Git & Github\n\nHappy Git and GitHub for the useR (Bryan 2023) - comprehensive guide to using Git and GitHub with R\nGitHub Skills: Introduction to GitHub\nUC Davis DataLab Git Sandbox - hands-on Git practice repository",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#resources-for-python",
    "href": "resources.html#resources-for-python",
    "title": "19  Resources",
    "section": "19.3 Resources for Python",
    "text": "19.3 Resources for Python\n\nUC Davis DataLab Python Basics Workshop - foundational Python programming\nNatural Language Processing with Python - text analysis and NLP techniques",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#resources-for-julia",
    "href": "resources.html#resources-for-julia",
    "title": "19  Resources",
    "section": "19.4 Resources for Julia",
    "text": "19.4 Resources for Julia\n\nUC Davis Julia Users Group Julia Basics Workshop - foundational Julia programming",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#scientific-figures",
    "href": "resources.html#scientific-figures",
    "title": "19  Resources",
    "section": "19.5 Scientific figures",
    "text": "19.5 Scientific figures\n\nTen Simple Rules for Better Figures (Rougier, Droettboom, and Bourne 2014)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#writing",
    "href": "resources.html#writing",
    "title": "19  Resources",
    "section": "19.6 Writing",
    "text": "19.6 Writing\n\nUnpacking the Scientific Toolbox (Silbiger and Stubler 2019)\nICMJE Definition of authorship (International Committee of Medical Journal Editors, n.d.)\nComputational science: …why scientific programming does not compute (Merali and Giles 2010)\nThe Pathway to Publishing: A Guide to Quantitative Writing in the Health Sciences\nSecret, actionable writing tips",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#presentations",
    "href": "resources.html#presentations",
    "title": "19  Resources",
    "section": "19.7 Presentations",
    "text": "19.7 Presentations\n\nHow to tell a compelling story in scientific presentations (Van Noorden 2021)\nHow to give a killer narratively-driven scientific talk\nHow to make a better poster\nHow to make an even better poster",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#professional-advice",
    "href": "resources.html#professional-advice",
    "title": "19  Resources",
    "section": "19.8 Professional advice",
    "text": "19.8 Professional advice\n\nProfessional advice, especially for your first job\nTeam Public Health Substack",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#funding",
    "href": "resources.html#funding",
    "title": "19  Resources",
    "section": "19.9 Funding",
    "text": "19.9 Funding\n\nBuilding Your Funding Train\nNIH Grant Writing Resources",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "resources.html#ethics-and-global-health-research",
    "href": "resources.html#ethics-and-global-health-research",
    "title": "19  Resources",
    "section": "19.10 Ethics and global health research",
    "text": "19.10 Ethics and global health research\n\nGlobal Code of Conduct For Research in Resource-Poor Settings\nAddressing power asymmetries in global health (Abimbola et al. 2022)\nTransforming Global Health Partnerships\n\n\n\n\n\nAbimbola, Seye, Sheena Asthana, Cristina Montenegro, et al. 2022. “Addressing Power Asymmetries in Global Health: Imperatives in the Wake of the COVID-19 Pandemic.” PLOS Global Public Health 2 (10). https://doi.org/10.1371/journal.pgph.0002269.\n\n\nBryan, Jennifer. 2023. Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\n\nFay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman; Hall/CRC. https://engineering-shiny.org/.\n\n\nInternational Committee of Medical Journal Editors. n.d. “Defining the Role of Authors and Contributors.” ICMJE. http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html.\n\n\nMerali, Zeeya, and Jim Giles. 2010. “Computational Science: Error ... Why Scientific Programming Does Not Compute.” Nature 467: 775–77. https://doi.org/10.1038/467775a.\n\n\nRougier, Nicolas P., Michael Droettboom, and Philip E. Bourne. 2014. “Ten Simple Rules for Better Figures.” PLOS Computational Biology 10 (9). https://doi.org/10.1371/journal.pcbi.1003833.\n\n\nSilbiger, Nyssa J., and Ariel D. Stubler. 2019. “Unpacking the Scientific Toolbox: Five Skills for the Modern Scientist.” Nature. https://doi.org/10.1038/d41586-019-02918-5.\n\n\nVan Noorden, Richard. 2021. “Scientists and Science Communicators Swap Tips on How to Tell Compelling Stories.” Nature. https://doi.org/10.1038/d41586-021-03603-2.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Chapman; Hall/CRC. https://adv-r.hadley.nz/.\n\n\n———. 2021. Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages. 2nd ed. O’Reilly Media. https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "professional-development.html",
    "href": "professional-development.html",
    "title": "20  Professional Development",
    "section": "",
    "text": "20.1 Mentoring Philosophy\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung\nWe believe in individualized mentoring that supports each person’s unique career goals. Effective mentoring requires:",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#mentoring-philosophy",
    "href": "professional-development.html#mentoring-philosophy",
    "title": "20  Professional Development",
    "section": "",
    "text": "Regular, open communication between mentees and mentors\nMutual respect and trust\nClear expectations and goals\nConstructive feedback\nSupport for both research and career development",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#individual-development-plans",
    "href": "professional-development.html#individual-development-plans",
    "title": "20  Professional Development",
    "section": "20.2 Individual Development Plans",
    "text": "20.2 Individual Development Plans\nAll graduate students and postdocs should maintain an Individual Development Plan (IDP) that outlines:\n\nShort-term and long-term career goals\nSkills to develop\nTraining needs and opportunities\nTimeline and milestones\nProgress toward goals\n\nUpdate your IDP at least annually and discuss it with your PI. Useful resources:\n\nmyIDP - Individual Development Plan tool\nNIH OITE Career Resources",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#presentations-and-conferences",
    "href": "professional-development.html#presentations-and-conferences",
    "title": "20  Professional Development",
    "section": "20.3 Presentations and Conferences",
    "text": "20.3 Presentations and Conferences\nWe encourage lab members to present their work at conferences and seminars:\n\nDiscuss conference opportunities with PIs early\nSubmit abstracts with PI approval\nPractice presentations in lab meeting before the conference\nFunding for conferences depends on availability and should be discussed in advance\n\nResources for effective presentations:\n\nHow to tell a compelling story in scientific presentations\nHow to give a killer narratively-driven scientific talk\nHow to make a better poster\nHow to make an even better poster",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#scientific-figures",
    "href": "professional-development.html#scientific-figures",
    "title": "20  Professional Development",
    "section": "20.4 Scientific Figures",
    "text": "20.4 Scientific Figures\nCreating clear, effective figures is essential for communicating research findings:\n\nLabel x-axis and y-axis clearly\nUse panel labels when including multiple subplots\nInclude sufficient tick marks and labels\nUse appropriate font sizes (readable but not distracting)\nUse colorblind-friendly palettes (see ColorBrewer or iwanthue)\nDefine colors, shapes, and line types in legends\nMinimize abbreviations in labels and legends\nUse transparency to show overlapping data\nShow measures of precision (e.g., 95% confidence intervals)\n\nResources:\n\nTen Simple Rules for Better Figures",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#grant-writing",
    "href": "professional-development.html#grant-writing",
    "title": "20  Professional Development",
    "section": "20.5 Grant Writing",
    "text": "20.5 Grant Writing\n\nGraduate students and postdocs are encouraged to apply for fellowships (e.g., NIH F31, NSF GRFP, K awards)\nPIs will support fellowship applications with feedback, letters of support, and mentoring\nStart planning fellowship applications well in advance of deadlines (typically 3-6 months)\nAttend grant writing workshops and seek feedback from multiple sources\n\nResources:\n\nBuilding Your Funding Train\nNIH Funding Opportunities\nNIH Grant Writing Resources",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#sec-dissertation-requirements",
    "href": "professional-development.html#sec-dissertation-requirements",
    "title": "20  Professional Development",
    "section": "20.6 PhD Dissertation Requirements",
    "text": "20.6 PhD Dissertation Requirements\nUnderstanding what constitutes a sufficient PhD dissertation is crucial for setting realistic expectations and timelines. The dissertation represents an important milestone, but it doesn’t need to be your magnum opus.\n\n20.6.1 Review Previous Dissertations\nBefore setting your dissertation goals, read previous dissertations from students in your program. This helps you:\n\nUnderstand the typical scope and depth expected\nSee different approaches to structure and presentation\nCalibrate your expectations based on successful examples\nIdentify common patterns and standards in your field\n\nMost universities maintain electronic dissertation repositories, making it easy to access recent examples from your program.\n\n\n20.6.2 Publication Requirements\nThree first-author papers typically suffice for a dissertation in public health and biomedical sciences. If academic peers in reputable journals have approved your work through peer review, this demonstrates that your research meets professional standards. Your dissertation committee should recognize this external validation.\nThe specific publication requirements may vary by program and institution, so consult your program’s guidelines and discuss expectations with your committee early. However, three substantial first-author publications generally demonstrate:\n\nIndependent research capability\nAbility to communicate findings effectively\nContribution to the scientific literature\nReadiness for an academic or research career\n\n\n\n20.6.3 External Validation and Fast-Tracking\nIf you have a job offer waiting, you can usually get fast-tracked through the dissertation process. The reasoning is straightforward: your work has been externally validated as worthwhile by prospective employers.\nSince most post-PhD positions offer better compensation than graduate stipends, it’s difficult to justify prolonging your graduation when you’ve already demonstrated professional competence. This applies whether the job offer is in academia, industry, government, or nonprofit sectors.\n\n\n20.6.4 Historical Context: The Masterpiece Tradition\nThe dissertation is a spiritual successor to an apprentice’s masterpiece in craft guilds. Historically, a masterpiece was the piece of work that demonstrated an apprentice had achieved sufficient skill to join the guild as a master craftsperson. It was not meant to be the best work they would ever produce—it was meant to prove they were ready to work independently.\nSimilarly, your dissertation should demonstrate that you’re ready to conduct independent research. It’s your first professional-level work, not your career highlight. This perspective helps set appropriate expectations:\n\nThe dissertation proves you can conduct rigorous research\nIt doesn’t need to solve every problem in your field\nIt doesn’t need to be flawless\nIt doesn’t need to be all-encompassing\nIt just needs to constitute incremental progress in your field\n\n\n\n20.6.5 Setting Realistic Expectations\nMany PhD students struggle with perfectionism or “scope creep” in their dissertations. Remember:\n\nDone is better than perfect when it comes to dissertations\nYou’ll have your entire career to refine and expand on these ideas\nThe goal is to finish and move forward, not to write the definitive work on your topic\nYour committee wants to see you succeed and graduate\n\nFocus on making a solid, incremental contribution to knowledge in your field. That’s what a dissertation is meant to be—no more, no less.\n\n\n20.6.6 Resources\nDissertation writing tools:\n\nquarto-thesis - Quarto extension for creating masters or PhD theses with professional LaTeX formatting",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#teaching-and-outreach",
    "href": "professional-development.html#teaching-and-outreach",
    "title": "20  Professional Development",
    "section": "20.7 Teaching and Outreach",
    "text": "20.7 Teaching and Outreach\nTeaching and outreach are valuable professional development opportunities:\n\nGraduate students are encouraged to gain teaching experience\nWe support science communication and outreach activities\nDiscuss opportunities with PIs\n\nThe UC Davis DataLab offers various workshops and learning materials that can support your teaching and professional development. Their workshop index provides a comprehensive catalog of available resources.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "professional-development.html#networking",
    "href": "professional-development.html#networking",
    "title": "20  Professional Development",
    "section": "20.8 Networking",
    "text": "20.8 Networking\nBuilding a professional network is important for your career:\n\nAttend seminars and departmental events\nConnect with researchers in your field\nJoin professional societies\nUse professional social media platforms to share research and engage with the scientific community",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Professional Development</span>"
    ]
  },
  {
    "objectID": "manuscript-preparation.html",
    "href": "manuscript-preparation.html",
    "title": "21  Manuscript Preparation and Publication",
    "section": "",
    "text": "21.1 Publication Process\nAdapted by UCD-SeRG team from original by Jade Benjamin-Chung\nThe typical workflow for manuscript preparation and publication:",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manuscript Preparation and Publication</span>"
    ]
  },
  {
    "objectID": "manuscript-preparation.html#publication-process",
    "href": "manuscript-preparation.html#publication-process",
    "title": "21  Manuscript Preparation and Publication",
    "section": "",
    "text": "Planning: Discuss target journals, outline, and timeline with PIs\nDrafting: Lead author prepares initial draft\nInternal review: Co-authors review and provide feedback\nRevision: Lead author incorporates feedback iteratively\nPI approval: Obtain final approval from PIs before submission\nSubmission: Submit to journal\nRevisions: Lead author coordinates response to peer reviewers\nPublication: Celebrate and share!",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manuscript Preparation and Publication</span>"
    ]
  },
  {
    "objectID": "manuscript-preparation.html#preprints-and-open-access",
    "href": "manuscript-preparation.html#preprints-and-open-access",
    "title": "21  Manuscript Preparation and Publication",
    "section": "21.2 Preprints and Open Access",
    "text": "21.2 Preprints and Open Access\n\nWe encourage posting preprints prior to or during peer review on platforms like medRxiv or bioRxiv\nPreprints allow rapid dissemination of findings and can be cited in grant applications\nWe support publishing in open access journals when possible to maximize accessibility\nMany funders, including NIH, have public access policies that require making publications freely available\n\nA preprint is a scientific manuscript that has not been peer reviewed. Preprint servers create digital object identifiers (DOIs) and can be cited in other articles and in grant applications. Because the peer review process can take many months, publishing preprints prior to or during peer review enables other scientists to immediately learn from and build on your work. Importantly, NIH allows applicants to include preprint citations in their biosketches. In most cases, we publish preprints on medRxiv.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manuscript Preparation and Publication</span>"
    ]
  },
  {
    "objectID": "manuscript-preparation.html#reporting-checklists",
    "href": "manuscript-preparation.html#reporting-checklists",
    "title": "21  Manuscript Preparation and Publication",
    "section": "21.3 Reporting Checklists",
    "text": "21.3 Reporting Checklists\nUsing reporting checklists ensures that publications contain information needed for readers to assess validity and reproducibility. We use checklists appropriate to study design:\n\nCONSORT for randomized trials\nSTROBE for observational studies\nPRISMA for systematic reviews\nOthers as appropriate (see EQUATOR Network)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manuscript Preparation and Publication</span>"
    ]
  },
  {
    "objectID": "manuscript-preparation.html#manuscript-checklist",
    "href": "manuscript-preparation.html#manuscript-checklist",
    "title": "21  Manuscript Preparation and Publication",
    "section": "21.4 Manuscript Checklist",
    "text": "21.4 Manuscript Checklist\nBefore submitting a manuscript:\n\nCompleted relevant reporting checklist\nResults are reproducible (rerunning code replicates tables/figures exactly)\nTarget journal selected\nTitle is declarative and states findings clearly\nWord count meets journal requirements\nManuscript follows journal formatting guidelines\nConsistent voice throughout (typically active voice)\nAll figures and tables referenced in main text\nCaptions for all figures and tables\nTables/figures numbered by order of appearance\nAbbreviations defined at first use and used sparingly\nAvoid subjective wording (e.g., “interesting”, “dramatic”)\nAvoid directionless statements (specify direction of associations)\nCausal language only when supported by study design\nAvoid “significant” (easily confused with statistical significance)\nAuthor contributions drafted using CRediT Taxonomy",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manuscript Preparation and Publication</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "PREVIEW_BANNER_PLACEHOLDER\n\n\n\n“2001: A Space Odyssey.” 1968. Film. https://en.wikipedia.org/wiki/2001:_A_Space_Odyssey_(film).\n\n\nAbimbola, Seye, Sheena Asthana, Cristina Montenegro, et al. 2022.\n“Addressing Power Asymmetries in Global Health: Imperatives in the\nWake of the COVID-19 Pandemic.” PLOS Global Public\nHealth 2 (10). https://doi.org/10.1371/journal.pgph.0002269.\n\n\nAsimov, Isaac. 1950. “I, Robot.” New York: Novel; Gnome\nPress. https://search.library.ucdavis.edu/permalink/01UCD_INST/9fle3i/alma990000226350403126.\n\n\nBaker, Monya. 2019. “How to Tackle the Reproducibility Crisis in\nTen Steps.” Nature. https://doi.org/10.1038/d41586-019-01431-z.\n\n\n“Battlestar Galactica.” 2004. Television Series. https://en.wikipedia.org/wiki/Battlestar_Galactica_(2004_TV_series).\n\n\nBenjamin-Chung, Jade, Kunal Mishra, Stephanie Djajadi, Nolan\nPokpongkiat, Anna Nguyen, Iris Tong, and Gabby Barratt Heitmann. 2024.\n“Benjamin-Chung Lab Manual.” https://jadebc.github.io/lab-manual/.\n\n\n“Blade Runner.” 1982. Film. https://en.wikipedia.org/wiki/Blade_Runner.\n\n\nBryan, Jennifer. 2023. Happy Git and GitHub for the useR. https://happygitwithr.com/.\n\n\nCard, Orson Scott. 1985. “Ender’s Game.” Novel; Tor Books.\nhttps://en.wikipedia.org/wiki/Ender%27s_Game.\n\n\nCrameri, Fabio, Grace E. Shephard, and Philip J. Heron. 2020. “The\nMisuse of Colour in Science Communication.” Nature\nCommunications 11. https://doi.org/10.1038/s41467-020-19160-7.\n\n\n“Creative Commons Attribution-NonCommercial 4.0 International\nLicense.” n.d. Creative Commons. http://creativecommons.org/licenses/by-nc/4.0/.\n\n\n“CRediT: Contributor Roles Taxonomy.” n.d. PLOS ONE. https://journals.plos.org/plosone/s/authorship.\n\n\n“Dryad Digital Repository.” n.d. Dryad. https://datadryad.org/.\n\n\n“EQUATOR Network: Enhancing the QUAlity and Transparency of Health\nResearch.” n.d. EQUATOR Network. https://www.equator-network.org/.\n\n\nFay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard.\n2021. Engineering Production-Grade Shiny Apps. Chapman;\nHall/CRC. https://engineering-shiny.org/.\n\n\nFlight of the Conchords. 2007. “The Humans Are Dead.” Music\nVideo. https://www.youtube.com/watch?v=B1BdQcJ2ZYY.\n\n\n“GitHub Desktop.” n.d. GitHub. https://desktop.github.com/.\n\n\nHerbert, Frank. 1965. “Dune.” Novel. https://en.wikipedia.org/wiki/Organizations_of_the_Dune_universe#Thinking_machines.\n\n\n“How to Store and Manage Your Data.” n.d. PLOS. https://plos.org/resource/how-to-store-and-manage-your-data/.\n\n\nInternational Committee of Medical Journal Editors. n.d. “Defining\nthe Role of Authors and Contributors.” ICMJE. http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html.\n\n\nLeCun, Yann. 2022. “A Path Towards Autonomous Machine\nIntelligence.” Meta AI Research; New York University; Technical\nReport. https://openreview.net/forum?id=BZ5a1r-kVsf.\n\n\n“medRxiv: The Preprint Server for Health Sciences.” n.d.\nCold Spring Harbor Laboratory. https://www.medrxiv.org/.\n\n\nMerali, Zeeya, and Jim Giles. 2010. “Computational Science: Error\n... Why Scientific Programming Does Not Compute.” Nature\n467: 775–77. https://doi.org/10.1038/467775a.\n\n\nMunafò, Marcus R., Brian A. Nosek, Dorothy V. M. Bishop, Katherine S.\nButton, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn,\nEric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017.\n“A Manifesto for Reproducible Science.” Nature Human\nBehaviour 1. https://doi.org/10.1038/s41562-016-0021.\n\n\nNuzzo, Regina. 2015. “How Scientists Fool Themselves – and How\nThey Can Stop.” Nature 526: 182–85. https://doi.org/10.1038/526182a.\n\n\n“Open Science Framework.” n.d. Center for Open Science. https://osf.io/.\n\n\nRobertson, Seth. n.d. “On Undoing, Fixing, or Removing Commits in\nGit.” https://sethrobertson.github.io/GitFixUm/fixup.html.\n\n\nRougier, Nicolas P., Michael Droettboom, and Philip E. Bourne. 2014.\n“Ten Simple Rules for Better Figures.” PLOS\nComputational Biology 10 (9). https://doi.org/10.1371/journal.pcbi.1003833.\n\n\nSilbiger, Nyssa J., and Ariel D. Stubler. 2019. “Unpacking the\nScientific Toolbox: Five Skills for the Modern Scientist.”\nNature. https://doi.org/10.1038/d41586-019-02918-5.\n\n\n“Slurm Workload Manager: Sbatch Documentation.” n.d.\nSchedMD. https://slurm.schedmd.com/sbatch.html.\n\n\nStoddart, Charlotte. 2019. “Is There a Reproducibility Crisis in\nScience?” Nature. https://doi.org/10.1038/d41586-019-00067-3.\n\n\n“Terminator 3: Rise of the Machines.” 2003. Film. https://en.wikipedia.org/wiki/Terminator_3:_Rise_of_the_Machines.\n\n\n“The Matrix.” 1999. Film. https://en.wikipedia.org/wiki/The_Matrix.\n\n\nTidyverse Team. 2023. Tidyverse Code Review Principles. https://code-review.tidyverse.org/.\n\n\nVan Noorden, Richard. 2021. “Scientists and Science Communicators\nSwap Tips on How to Tell Compelling Stories.” Nature. https://doi.org/10.1038/d41586-021-03603-2.\n\n\n“WarGames.” 1983. Film. https://en.wikipedia.org/wiki/WarGames.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Chapman; Hall/CRC.\nhttps://adv-r.hadley.nz/.\n\n\n———. 2021. Mastering Shiny. O’Reilly Media. https://mastering-shiny.org/.\n\n\n———. 2023a. The Tidyverse Style Guide. https://style.tidyverse.org/.\n\n\n———. 2023b. Tidyverse Design Guide. https://design.tidyverse.org/.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages. 2nd ed.\nO’Reilly Media. https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. 2nd ed. O’Reilly Media. https://r4ds.hadley.nz/.\n\n\nWickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster.\n2024. Roxygen2: In-Line Documentation for r. https://roxygen2.r-lib.org/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix-copilot-instructions.html",
    "href": "appendix-copilot-instructions.html",
    "title": "Copilot Instructions File",
    "section": "",
    "text": "This appendix contains the complete .github/copilot-instructions.md file used in this repository to guide GitHub Copilot coding agents.",
    "crumbs": [
      "Appendices",
      "Copilot Instructions File"
    ]
  },
  {
    "objectID": "appendix-copilot-instructions.html#markdown-and-quarto-formatting",
    "href": "appendix-copilot-instructions.html#markdown-and-quarto-formatting",
    "title": "Copilot Instructions File",
    "section": "Markdown and Quarto Formatting",
    "text": "Markdown and Quarto Formatting\n\nTalking about code\nWhen talking about code in prose sections, use backticks to apply code formatting: for example, dplyr::mutate()\nWhen talking about packages in prose, use backticks and curly-braces with a hyperlink to the package website. For example: {dplyr}\nDo not use raw HTML (&lt;a href=\"...\"&gt;) in .qmd files. Always use Quarto/markdown link syntax instead.\nCommon package URLs:\n\n{dplyr}\n{ggplot2}\n{tidyr}\n{readr}\n{purrr}\n{tibble}\n{stringr}\n{forcats}\n{styler}\n{lintr}\n{roxygen2}\n{testthat}\n{usethis}\n{devtools}\n{renv}\n{targets}\n{data.table}\n{assertthat}\n{lubridate}\n\n\n\nBlank Lines Before Lists\nALWAYS include a blank line before bullet lists and numbered lists in markdown and Quarto (.qmd) files.\nCorrect:\nHere are the key points:\n\n- First item\n- Second item\n- Third item\nIncorrect:\nHere are the key points:\n- First item\n- Second item\n- Third item\nThis applies to:\n\nBullet lists (starting with - or *)\nNumbered lists (starting with 1., 2., etc.)\nLists in all .qmd files throughout the repository\n\n\n\nLine Breaks in Plain Text\nALWAYS line-break at the ends of sentences and long phrases in plain-text paragraphs in .qmd files to avoid long lines.\nCorrect:\nWhen talking about code in prose sections,\nuse backticks to apply code formatting.\nThis helps maintain readability in source files\nand makes diffs easier to review.\nIncorrect:\nWhen talking about code in prose sections, use backticks to apply code formatting. This helps maintain readability in source files and makes diffs easier to review.\nBenefits:\n\nImproves readability of source .qmd files\nMakes git diffs clearer and easier to review\nHelps identify specific changes in version control\nPrevents horizontal scrolling when editing\nFollows semantic line breaks best practice\n\nGuidelines:\n\nBreak after complete sentences (at periods)\nBreak after long phrases or clauses (at commas or conjunctions)\nBreak after approximately 60-80 characters when appropriate\nKeep related short phrases together on one line\nDon’t break in the middle of inline code, links, or formatting\n\n\n\nWhy This Matters\n\nEnsures consistent markdown rendering across different platforms\nImproves readability in both source and rendered forms\nPrevents rendering issues in Quarto books\nFollows markdown best practices\n\n\n\nCross-References for Figures and Tables\nALWAYS use Quarto’s cross-reference system for figures, tables, and other captioned content. See Quarto Cross-References documentation for complete details.\nRequired label prefixes:\n\nFigures: #fig- (e.g., #fig-data-masking, #fig-workflow-diagram)\nTables: #tbl- (e.g., #tbl-git-commands, #tbl-summary-stats)\nEquations: #eq- (e.g., #eq-regression-model)\nSections: #sec- (e.g., #sec-introduction) - already in use throughout manual\nTheorems: #thm- (e.g., #thm-central-limit)\nLemmas: #lem- (e.g., #lem-auxiliary-result)\nCorollaries: #cor- (e.g., #cor-special-case)\nPropositions: #prp- (e.g., #prp-main-result)\nExamples: #exm- (e.g., #exm-simple-case)\nExercises: #exr- (e.g., #exr-practice-problem)\n\nFor figures (images):\n![Caption text](path/to/image.png){#fig-label}\nFor tables (markdown tables):\n| Column 1 | Column 2 |\n|----------|----------|\n| Data     | Data     |\n\n: Caption text {#tbl-label}\nFor code-generated figures:\n```{r}\n#| label: fig-plot-name\n#| fig-cap: \"Caption text\"\n\n# R code to generate plot\n```\nFor code-generated tables:\n```{r}\n#| label: tbl-table-name\n#| tbl-cap: \"Caption text\"\n\n# R code to generate table\n```\nReferencing in text:\n\nFigures: @fig-label produces “Figure X”\nTables: @tbl-label produces “Table X”\nEquations: @eq-label produces “Equation X”\nSections: @sec-label produces “Section X”\n\nImportant: Always use cross-references for sections\nWhen referring to other sections within the manual, always use the Quarto cross-reference system (@sec-label) instead of plain text references like “the section above” or “see the X section”.\nCorrect:\nSee @sec-r-ci for setting up GitHub Actions workflows.\nSee @sec-ai-best-practices for security considerations.\nIncorrect:\nSee the \"Continuous Integration\" section above.\nSee the \"Best Practices\" section for more details.\nBenefits of using cross-references:\n\nAutomatically generates proper section titles and numbers\nCreates clickable links in HTML output\nUpdates automatically if section titles change\nWorks correctly across all output formats (HTML, PDF, DOCX, EPUB)\nQuarto will warn you if a reference is broken\n\nBenefits:\n\nAutomatic numbering of figures, tables, and equations\nAutomatic updates when content is reordered\nClickable cross-references in HTML and PDF output\nConsistent formatting across all output formats\nBetter accessibility for screen readers",
    "crumbs": [
      "Appendices",
      "Copilot Instructions File"
    ]
  },
  {
    "objectID": "appendix-copilot-instructions.html#r-code-style",
    "href": "appendix-copilot-instructions.html#r-code-style",
    "title": "Copilot Instructions File",
    "section": "R Code Style",
    "text": "R Code Style\n\nFollow the tidyverse style guide: https://style.tidyverse.org\nUse native pipe |&gt; instead of %&gt;%\nUse snake_case for variable and function names\nUse .qmd files exclusively (not .Rmd)\nAll R projects should use R package structure\nAvoid redundant logical comparisons: Use logical variables directly in conditional statements (e.g., if (x) instead of if (x == TRUE) or if (x == 1))\nUse lubridate::NA_Date_ instead of as.Date(NA) for missing date values",
    "crumbs": [
      "Appendices",
      "Copilot Instructions File"
    ]
  },
  {
    "objectID": "appendix-copilot-instructions.html#file-organization",
    "href": "appendix-copilot-instructions.html#file-organization",
    "title": "Copilot Instructions File",
    "section": "File Organization",
    "text": "File Organization\n\nUsing Quarto Includes for Modular Content\nAll chapters should use Quarto includes to decompose content into separate files. This modular approach provides significant benefits for version control, collaboration, and content management.\n\nWhy Use Includes?\n\nBetter Git History: When sections are reordered, only the main chapter file changes (moving include statements), making it immediately clear that content was reorganized rather than edited. When content is edited, only the specific include file changes. This makes reviews focused and precise.\nEasier Code Review: Reviewers can see exactly what changed—either the organization (main file) or the content (include file)—without having to parse through large diffs.\nModular Maintenance: Each section lives in its own file, making it easier to:\n\nFind and edit specific content\nReuse sections across chapters if needed\nWork on different sections simultaneously without merge conflicts\nTest and preview individual sections\n\nClear Structure: The main chapter file becomes a table of contents showing the organization at a glance.\n\n\n\nStructure Pattern\nMain chapter file (e.g., 05-coding-practices.qmd):\n\nContains the chapter title and introduction\nContains section headings (##, ###, etc.)\nUses the include shortcode to pull in content (see https://quarto.org/docs/authoring/includes.html for details)\nShows the organization/outline of the chapter\n\nInclude files (e.g., 05-coding-practices/lab-protocols-for-code-and-data.qmd):\n\nStored in a subdirectory matching the chapter name\nContains only the content for that section (no heading)\nThe heading stays in the main chapter file\nNamed descriptively using kebab-case\n\n\n\nRequired Pattern\nAlways follow this pattern:\n## Section Heading\n\n{{&lt; include folder/section-name.qmd &gt;}}\n\nCorrect example:\n## Section heading\n\n{{&lt; include folder/section-name.qmd &gt;}}\n\nIncorrect (don’t do this):\n{{&lt; include folder/section-name.qmd &gt;}}\n\nThe heading must be in the main file, followed by a blank line, then the include statement.\n\n\nFile Naming Conventions\n\nMain chapter files: ##-chapter-name.qmd (e.g., 05-coding-practices.qmd)\nSubdirectory: ##-chapter-name/ (matches the main file name)\nInclude files: descriptive-section-name.qmd using kebab-case\nUse descriptive names that clearly indicate the content\nPrefix with underscore _ for partial/helper files not directly included (e.g., _lintr-summary.qmd)\n\n\n\nGit History Benefits Example\nWhen reordering sections:\n-## Object naming\n+## Function calls\n \n-{{&lt; include demo-folder/section-name.qmd &gt;}}\n+{{&lt; include demo-folder/section-2.qmd &gt;}}\n \n-## Function calls\n+## Object naming\n \n-{{&lt; include demo-folder/section-2.qmd &gt;}}\n+{{&lt; include demo-folder/section-name.qmd &gt;}}\nThis diff clearly shows a reordering (swapping two sections) with no content changes—only the main chapter file changes.\nWhen editing content: Only the specific include file (e.g., 05-coding-practices/function-calls.qmd) appears in the git diff, making it easy to review the actual content changes without distraction.\n\n\nWhen to Create a New Include File\nCreate a new include file when:\n\nAdding a new section to a chapter\nA section becomes long enough to benefit from being in its own file (&gt;20-30 lines)\nContent might be reused elsewhere\nYou want to work on a section independently\n\n\n\nMigration Strategy\nWhen working with chapters that don’t yet use includes:\n\nCreate a subdirectory matching the chapter name\nExtract each section into its own include file\nUpdate the main chapter file to use includes\nKeep headings in the main file\nEnsure blank lines before include statements\nTest that rendering still works correctly\n\n\n\nUsing Includes for Code Examples and Reusable Content\nPrefer using Quarto’s include shortcode over copy-pasting content whenever feasible. This applies to code examples, configuration files, and any content that exists elsewhere in the repository.\nBenefits:\n\nSingle source of truth: Changes to the original file automatically propagate\nReduces maintenance burden and sync issues\nEnsures examples stay current and accurate\nBetter git history (changes appear in one place)\n\nFor including code files:\nUse the include shortcode inside a code fence with the appropriate language. For example, to include a YAML workflow file:\n```{.yaml filename=\"demo-folder/yml.yml\"}\n{{&lt; include demo-folder/yml.yml &gt;}}\n\n\n```\nWhen you need to show the include shortcode syntax itself in documentation (without it being processed), add an extra pair of curly braces: {{&lt; include path/to/file &gt;}}. This prevents Quarto from recognizing it as a shortcode, allowing the literal syntax to appear in the rendered output.\nWhen to copy-paste instead:\nOnly copy-paste when:\n\nThe content is a simplified example that doesn’t exist elsewhere\nYou need to show a partial excerpt with modifications\nThe source file contains content that shouldn’t be fully shown\nYou need to demonstrate different variations of similar code\n\nFile naming for included code:\n\nPrefix standalone code files with _ so Quarto doesn’t try to render them (e.g., _helper-functions.R)\nUse descriptive names that indicate the purpose\nKeep included files in appropriate subdirectories",
    "crumbs": [
      "Appendices",
      "Copilot Instructions File"
    ]
  },
  {
    "objectID": "appendix-copilot-instructions.html#working-with-docx-files",
    "href": "appendix-copilot-instructions.html#working-with-docx-files",
    "title": "Copilot Instructions File",
    "section": "Working with DOCX Files",
    "text": "Working with DOCX Files\nGitHub Copilot can read and process Microsoft Word (.docx) files, which is useful for translating edits made in Word back to Quarto format.\nWhen working with DOCX files:\n\nCheck git metadata first: DOCX files generated from this repository include a “Document Generation Metadata” section at the end with the branch name, commit hash, and commit date. Use this information to:\n\nIdentify which commit generated the original DOCX\nSet up the resulting PR correctly with the appropriate base branch\nAccount for any commits that have been added since the DOCX was generated\nUnderstand the state of the repository when the DOCX was created\n\nAlways examine tracked changes: Use the view tool to read DOCX files and pay special attention to any tracked changes (insertions, deletions, formatting changes)\nReview comments: Look for and address any comments in the DOCX file that may provide context or instructions for edits\nTranslate edits to Quarto: When edits have been made in a DOCX file, apply the equivalent changes to the corresponding .qmd files\nPreserve formatting: Ensure that formatting, citations, and cross-references are properly converted to Quarto/markdown syntax\nVerify completeness: Check that all edits, including those in tracked changes and comments, have been addressed\n\nThis workflow enables a hybrid editing process where collaborators can make edits in familiar Word format, and Copilot can translate those edits back to the Quarto source files.",
    "crumbs": [
      "Appendices",
      "Copilot Instructions File"
    ]
  },
  {
    "objectID": "appendix-copilot-instructions.html#additional-guidelines",
    "href": "appendix-copilot-instructions.html#additional-guidelines",
    "title": "Copilot Instructions File",
    "section": "Additional Guidelines",
    "text": "Additional Guidelines\n\nMaintain consistency with existing code style\nPreserve all existing content when refactoring\nAdd blank lines before all lists\nFollow the lab’s R package development workflow (as described throughout this repo)\n\n\nTesting and Validation\nALWAYS render the full Quarto book before requesting code review or finalizing your work.\nRun quarto render to ensure the book builds successfully in all output formats (HTML, PDF, DOCX, EPUB). This validates that:\n\nAll cross-references are valid\nAll images can be properly converted for PDF output (use PNG format for images, not SVG)\nAll code chunks execute without errors\nThe book structure is correct\n\nIf the render fails, fix the issues before committing or requesting review. Common issues include:\n\nSVG images that cannot be converted to PDF (use PNG instead)\nInvalid cross-references\nMissing or incorrect file paths\nSyntax errors in code chunks",
    "crumbs": [
      "Appendices",
      "Copilot Instructions File"
    ]
  },
  {
    "objectID": "appendix-copilot-setup-steps.html",
    "href": "appendix-copilot-setup-steps.html",
    "title": "Copilot Setup Steps File",
    "section": "",
    "text": "This appendix contains the complete .github/workflows/copilot-setup-steps.yml file used to configure the GitHub Copilot coding agent’s environment.\n\n\n\n\n\n\nNoteView .github/workflows/copilot-setup-steps.yml\n\n\n\n\n\n\n\n.github/workflows/copilot-setup-steps.yml\n\n# GitHub Copilot Setup Steps for lab-manual\n#\n# This workflow configures the GitHub Copilot coding agent's environment\n# by preinstalling R, Quarto and TinyTeX for rendering the lab manual.\n#\n# See: https://docs.github.com/en/copilot/how-tos/use-copilot-agents/coding-agent/customize-the-agent-environment\n#\n# This workflow sets up:\n# - R for executing R code chunks in Quarto documents\n# - Quarto CLI for rendering\n# - TinyTeX for PDF output\n\nname: \"Copilot Setup Steps\"\n\n# Automatically run the setup steps when they are changed to allow for easy validation,\n# and allow manual testing through the repository's \"Actions\" tab\non:\n  workflow_dispatch:\n  push:\n    paths:\n      - .github/workflows/copilot-setup-steps.yml\n  pull_request:\n    paths:\n      - .github/workflows/copilot-setup-steps.yml\n\njobs:\n  # The job MUST be called `copilot-setup-steps` or it will not be picked up by Copilot.\n  copilot-setup-steps:\n    runs-on: ubuntu-latest\n\n    # Set the permissions to the lowest permissions possible needed for your steps.\n    # Copilot will be given its own token for its operations.\n    permissions:\n      contents: read\n\n    # Timeout after 55 minutes (max is 59 for copilot-setup-steps)\n    timeout-minutes: 55\n\n    steps:\n      # Checkout code - Copilot will do this automatically if we don't,\n      # but we include it for completeness\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      # R and renv setup steps disabled for now - we don't have any R code to run yet\n      # When R code is needed, uncomment the following steps:\n\n      # Install system dependencies required for R packages\n      - name: Install system dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y \\\n            libcurl4-openssl-dev \\\n            libssl-dev \\\n            libxml2-dev \\\n            libfontconfig1-dev \\\n            libharfbuzz-dev \\\n            libfribidi-dev \\\n            libfreetype6-dev \\\n            libpng-dev \\\n            libtiff5-dev \\\n            libjpeg-dev\n\n      # Set up pandoc for documentation\n      - name: Set up Pandoc\n        uses: r-lib/actions/setup-pandoc@v2\n\n      # Set up R using the standard GitHub Actions setup\n      - name: Set up R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 'release'\n          use-public-rspm: true\n\n      - name: Install system dependencies\n        run: |\n          sudo apt-get update\n          sudo apt-get install -y jags libcurl4-openssl-dev libpng-dev libfontconfig1-dev libjpeg-dev\n\n      - uses: r-lib/actions/setup-renv@v2\n\n      # Set up Quarto - required for rendering the website\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          tinytex: true\n\n      # # Install R dependencies using renv (disabled for now - no renv.lock file yet)\n      # - name: Install R dependencies via renv\n      #   uses: r-lib/actions/setup-renv@v2\n      #   with:\n      #     cache-version: 1\n\n      # Verify development environment\n      - name: Verify development environment\n        run: |\n          echo \"=== Development Environment Status ===\"\n\n          # Verify R is installed and working\n          echo \"\"\n          echo \"=== R Status ===\"\n          R --version\n\n          # Verify Quarto is installed and working\n          echo \"\"\n          echo \"=== Quarto Status ===\"\n          quarto --version\n          quarto list tools\n\n          echo \"\"\n          echo \"Development environment setup complete!\"",
    "crumbs": [
      "Appendices",
      "Copilot Setup Steps File"
    ]
  },
  {
    "objectID": "appendix-document-metadata.html",
    "href": "appendix-document-metadata.html",
    "title": "Document Generation Metadata",
    "section": "",
    "text": "PREVIEW_BANNER_PLACEHOLDER\n\nThis document was generated from the following git commit:\n\nBranch: HEAD\nCommit: fdac01f\nFull commit hash: fdac01ffb900f41c0307a8783ea2ca3b5ebb5a25\nCommit date: 2026-01-19 22:32:34 +0000\n\nWhen transferring edits from this DOCX file back to the Quarto source files, use this commit information to set up the PR correctly and account for any commits that have been added since this document was generated.",
    "crumbs": [
      "Appendices",
      "Document Generation Metadata"
    ]
  }
]