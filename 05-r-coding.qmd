# R Coding {#sec-r-coding}

## Lab Protocols for Code and Data

Just as wet labs have strict safety protocols to ensure reproducible results and prevent contamination, our computational lab has protocols for coding and data management. These protocols are not suggestions—they are essential practices that:

- **Ensure reproducibility**: Others (including your future self) can recreate your analysis
- **Prevent errors**: Systematic approaches reduce the risk of mistakes
- **Enable collaboration**: Consistent practices allow team members to work together efficiently
- **Maintain data integrity**: Proper handling prevents data corruption and loss
- **Support publication**: Well-documented, reproducible code is increasingly required for publication

**Violating these protocols can have serious consequences**, including invalid results, wasted time, inability to publish, and damage to scientific credibility. Treat coding and data management protocols with the same seriousness as you would safety protocols in a wet lab.

## R Package Structure for All Projects {#sec-r-package-structure}

**All R projects in our lab should be structured as R packages**, even if they are primarily analysis projects and not intended for distribution on CRAN or Bioconductor. This standardized structure provides numerous benefits:

### Why Use R Package Structure?

1. **Organized code**: Clear separation of functions (`R/`), documentation (`man/`), tests (`tests/`), data (`data/`), and vignettes/analyses
2. **Dependency management**: `DESCRIPTION` file explicitly declares all package dependencies and versions
3. **Automatic documentation**: `roxygen2` generates help files from inline comments
4. **Built-in testing**: `testthat` framework integrates seamlessly with package structure
5. **Code quality**: Tools like `devtools::check()` and `lintr` enforce best practices
6. **Reproducibility**: Package structure makes it easy to share and reproduce analyses
7. **Reusable functions**: Decompose complex analyses into well-documented, testable functions
8. **Version control**: Track changes to code, documentation, and data together

### Basic Package Structure

```
myproject/
├── DESCRIPTION          # Package metadata and dependencies
├── NAMESPACE            # Auto-generated, don't edit manually
├── R/                   # All R functions (reusable code)
│   ├── analysis_functions.R
│   ├── data_prep.R
│   └── plotting.R
├── man/                 # Auto-generated documentation
├── tests/              
│   └── testthat/       # Unit tests
├── data/               # Processed data objects (.rda files)
├── data-raw/           # Raw data and data processing scripts
├── vignettes/          # Analysis workbooks and tutorials
│   ├── 01-data-preparation.Rmd
│   ├── 02-primary-analysis.Rmd
│   └── 03-sensitivity-analysis.Rmd
├── inst/               # Additional files to include in package
│   ├── extdata/        # External data files
│   └── analyses/       # Analyses using restricted data (see below)
└── .Rproj              # RStudio project file
```

### Where to Place Analysis Files {#sec-analysis-file-placement}

#### Public Analyses (vignettes/)

Use `vignettes/` for analysis workbooks that:
- Use publicly available data
- Can be shared with collaborators
- Should be built into the package documentation
- Will be rendered by `pkgdown::build_site()`

These vignettes become part of your package's documentation website.

#### Analyses with Restricted Data (inst/analyses/)

For analyses that rely on **private, sensitive, or restricted data**, place `.qmd` or `.Rmd` files in `inst/analyses/`:

```
myproject/
├── inst/
│   ├── analyses/
│   │   ├── 01-confidential-data-analysis.qmd
│   │   ├── 02-unpublished-results.qmd
│   │   └── README.md  # Document data access requirements
│   └── extdata/
└── vignettes/
    ├── 01-public-analysis.Rmd
    └── 02-demo-with-simulated-data.Rmd
```

**Benefits of this approach:**

- Analyses with restricted data are included in version control alongside your code
- They're clearly separated from public documentation
- `inst/analyses/` is **excluded from `pkgdown` builds** and package documentation
- Collaborators with data access can still run these analyses
- You maintain a complete record of all project work

**Note on privacy:** Files in `inst/analyses/` are not inherently private—they will be visible if your repository is public. Use this folder for analyses that rely on restricted data that is stored separately, not for storing the restricted data itself. If you need to keep the analysis code private, use a private repository.

**Best practices for analyses with restricted data:**

1. **Document data requirements**: Include a README.md in `inst/analyses/` explaining:
   - What data is required
   - Where to obtain it (if permissible)
   - Data access restrictions
   - How to set up data paths

2. **Use relative paths carefully**: Structure your code so data paths can be configured:
   ```r
   # In inst/analyses/01-analysis.qmd
   # Users should set this based on their local setup
   data_dir <- Sys.getenv("MYPROJECT_DATA", 
                          default = "~/restricted_data/myproject")
   raw_data <- readr::read_csv(file.path(data_dir, "sensitive.csv"))
   ```

3. **Create public alternatives**: When possible, create companion vignettes in `vignettes/` using:
   - Simulated data that mimics the structure
   - Publicly available datasets
   - Aggregated/de-identified summaries

4. **Add to .Rbuildignore**: Ensure `inst/analyses/` doesn't cause package checks to fail:
   ```
   # In .Rbuildignore
   ^inst/analyses$
   ```

### Keep Analysis Workbooks Tidy

**Decompose reusable functions** from your analysis notebooks into the `R/` directory. Your vignettes should:

- Be clean, readable narratives of your analysis
- Call well-documented functions from your package
- Focus on the "what" and "why" rather than implementation details
- Be reproducible by others with a single click (or with documented data access for private analyses)

**Example of what NOT to do** (all code in vignette):

```r
# Bad: 100 lines of data manipulation in vignette
raw_data <- read_csv("data.csv")
# ... 100 lines of cleaning, transforming, reshaping ...
cleaned_data <- final_result
```

**Example of what TO do** (functions in R/, simple calls in vignette):

```r
# Good: Clean vignette calling documented functions
raw_data <- read_csv("data.csv")
cleaned_data <- prep_study_data(raw_data)  # Function in R/data_prep.R
```

## Essential R Package Development Tools {#sec-r-package-tools}

The following tools are essential for R package development in our lab:

### usethis: Package Setup and Management

`usethis` automates common package development tasks:

```r
# Install usethis
install.packages("usethis")

# Create a new package
usethis::create_package("~/myproject")

# Add common components
usethis::use_mit_license()          # Add a license
usethis::use_git()                  # Initialize git
usethis::use_github()               # Connect to GitHub
usethis::use_testthat()             # Set up testing infrastructure
usethis::use_vignette("analysis")   # Create an analysis vignette
usethis::use_data_raw("dataset")    # Create data processing script
usethis::use_package("dplyr")       # Add a dependency
usethis::use_pipe()                 # Import pipe operator

# Increment version
usethis::use_version()              # Increment package version
```

### devtools: Development Workflow

`devtools` provides the core development workflow:

```r
# Install devtools
install.packages("devtools")

# Load your package for interactive development
devtools::load_all()                # Like library(), but for development

# Documentation
devtools::document()                # Generate documentation from roxygen2

# Testing
devtools::test()                    # Run all tests
devtools::test_active_file()        # Run tests in current file

# Checking
devtools::check()                   # R CMD check (comprehensive validation)
devtools::check_man()               # Check documentation only

# Dependencies
devtools::install_dev_deps()        # Install all development dependencies

# Building
devtools::build()                   # Build package bundle
devtools::install()                 # Install package locally
```

### pkgdown: Package Websites

`pkgdown` builds beautiful documentation websites from your package:

```r
# Install pkgdown
install.packages("pkgdown")

# Set up pkgdown
usethis::use_pkgdown()

# Build website locally
pkgdown::build_site()

# Preview in browser
pkgdown::build_site(preview = TRUE)

# Build components separately
pkgdown::build_reference()          # Function reference
pkgdown::build_articles()           # Vignettes
pkgdown::build_home()               # Home page from README
```

**Configure your pkgdown site** with `_pkgdown.yml`:

```yaml
url: https://ucd-serg.github.io/myproject

template:
  bootstrap: 5

reference:
  - title: "Data Preparation"
    desc: "Functions for preparing and cleaning data"
    contents:
    - prep_study_data
    - validate_data
  
  - title: "Analysis"
    desc: "Core analysis functions"
    contents:
    - run_primary_analysis
    - sensitivity_analysis

articles:
  - title: "Analysis Workflow"
    navbar: Analysis
    contents:
    - 01-data-preparation
    - 02-primary-analysis
    - 03-sensitivity-analysis
```

## Complete Package Development Workflow {#sec-r-workflow}

Here's the typical workflow for developing an R package in our lab:

### 1. Initial Setup

```r
# Create package structure
usethis::create_package("~/myproject")

# Set up infrastructure
usethis::use_git()
usethis::use_github()
usethis::use_testthat()
usethis::use_pkgdown()
usethis::use_mit_license()
usethis::use_readme_rmd()
```

### 2. Add Dependencies

```r
# Add packages your project depends on
usethis::use_package("dplyr")
usethis::use_package("ggplot2")
usethis::use_package("readr")

# Add packages only needed for development/testing
usethis::use_package("testthat", type = "Suggests")
```

### 3. Write Functions

Create functions in `R/` directory with roxygen2 documentation:

```r
#' Prepare Study Data
#'
#' Clean and prepare raw study data for analysis.
#'
#' @param raw_data A data frame containing raw study data
#' @param validate Logical; whether to run validation checks
#'
#' @returns A cleaned data frame ready for analysis
#'
#' @examples
#' raw_data <- read_csv("data.csv")
#' clean_data <- prep_study_data(raw_data)
#'
#' @export
prep_study_data <- function(raw_data, validate = TRUE) {
  # Function implementation
}
```

### 4. Document

```r
# Generate documentation from roxygen2 comments
devtools::document()
```

### 5. Test

Create tests in `tests/testthat/`:

```r
# tests/testthat/test-data_prep.R
test_that("prep_study_data handles missing values", {
  raw_data <- data.frame(x = c(1, NA, 3))
  result <- prep_study_data(raw_data)
  expect_false(anyNA(result$x))
})
```

Run tests:

```r
devtools::test()
```

### 6. Check

```r
# Comprehensive package check
devtools::check()
```

Fix any warnings or errors before proceeding.

### 7. Build Documentation Site

```r
pkgdown::build_site()
```

### 8. Share and Publish

```r
# Push to GitHub
# The pkgdown site can be automatically deployed to GitHub Pages
# using GitHub Actions
```

## Code Style Guidelines {#sec-r-code-style}

Follow these code style guidelines for all R code:

### General Principles

- **Follow tidyverse style guide**: https://style.tidyverse.org
- **Use native pipe**: `|>` not `%>%` (available in R >= 4.1.0)
- **Naming**: Use `snake_case` for functions and variables; acronyms may be uppercase (e.g., `prep_IDs_data`)
- **Write tidy code**: Keep code clean, readable, and well-organized

### File Organization

Just as your data "flows" through your project, code should flow naturally through files in the package structure:

1. **In `R/` files**: Put reusable functions with roxygen2 documentation
2. **In vignettes**: Put narrative analysis with calls to those functions
3. **In `data-raw/`**: Put scripts that create data objects saved to `data/`
4. **In `inst/analyses/`**: Put analyses using restricted data

### Function Structure and Documentation

Every function should follow this pattern:

```r
#' Short Title (One Line)
#'
#' Longer description providing details about what the function does,
#' when to use it, and important considerations.
#'
#' @param param1 Description of first parameter, including type and constraints
#' @param param2 Description of second parameter
#'
#' @returns Description of return value, including type and structure
#'
#' @examples
#' # Example usage
#' result <- my_function(param1 = "value", param2 = 10)
#'
#' @export
my_function <- function(param1, param2) {
  # Implementation
}
```

### Comments

Use comments to explain *why*, not *what*:

```r
# Good: Explains reasoning
# Use log scale because distribution is highly skewed
ggplot(data, aes(x = log10(income))) + geom_histogram()

# Bad: States the obvious
# Create a histogram
ggplot(data, aes(x = income)) + geom_histogram()
```

**File headers** (for scripts in `data-raw/` or `inst/analyses/`):

```r
################################################################################
# @Project - Myproject
# @Description - Process raw survey data and create analysis dataset
################################################################################
```

**Sections and subsections** in longer files:

```r
# Data Loading -------

## Read survey data -------

## Read lab results -------

# Data Cleaning -------
```

### Messaging and User Communication

Use `cli` package functions for all user-facing messages in package functions:

```r
# Good
cli::cli_inform("Analysis complete")
cli::cli_warn("Missing data detected")
cli::cli_abort("Invalid input: {x}")

# Bad - don't use these in package code
message("Analysis complete")
warning("Missing data detected")
stop("Invalid input")
```

### Package Code Practices

- **No `library()` in package code**: Use `::` notation or declare in DESCRIPTION Imports
- **Document all exports**: Use roxygen2 (@title, @description, @param, @returns, @examples)
- **Avoid code duplication**: Extract repeated logic into helper functions

### Line Breaks and Formatting

For readability, use line breaks in function calls and pipelines:

```r
# Good: Named arguments on separate lines
mean_Y <- calc_fluseas_mean(
  data = flu_data, 
  yname = "maari_yn",
  silent = FALSE
)

# Good: Pipeline with clear flow
cleaned_data <- raw_data |>
  filter(!is.na(outcome)) |>
  mutate(
    log_value = log10(value),
    category = case_when(
      value < 10 ~ "low",
      value < 100 ~ "medium",
      TRUE ~ "high"
    )
  ) |>
  group_by(category) |>
  summarize(
    n = n(),
    mean = mean(log_value)
  )
```

For complex `ggplot` calls:

```r
ggplot(data, aes(x = year, y = rate, group = group)) +
  geom_point(
    aes(col = group, shape = group),
    position = position_dodge(width = 0.2),
    size = 2.5
  ) +
  geom_errorbar(
    aes(ymin = lb, ymax = ub, col = group),
    position = position_dodge(width = 0.2),
    width = 0.2
  ) +
  scale_y_continuous(
    limits = c(0, 100),
    breaks = seq(0, 100, 25)
  ) +
  scale_color_manual(values = colors) +
  theme_minimal() +
  labs(
    title = "Title",
    x = "Year",
    y = "Rate (%)"
  )
```

### Tidyverse Replacements

Use modern tidyverse/alternatives for base R functions:

```r
# Data structures
tibble::tibble()           # instead of data.frame()
tibble::tribble()          # instead of manual data.frame creation

# I/O
readr::read_csv()          # instead of read.csv()
readr::write_csv()         # instead of write.csv()
readr::read_rds()          # instead of readRDS()
readr::write_rds()         # instead of saveRDS()

# Data manipulation
dplyr::bind_rows()         # instead of rbind()
dplyr::bind_cols()         # instead of cbind()

# String operations
stringr::str_which()       # instead of grep()
stringr::str_replace()     # instead of gsub()

# Session info
sessioninfo::session_info() # instead of sessionInfo()
```

### The here Package

The `here` package helps manage file paths in projects:

```r
library(here)

# Automatically finds project root and builds paths
data <- readr::read_csv(here("data-raw", "survey.csv"))
saveRDS(results, here("inst", "analyses", "results.rds"))
```

This works regardless of where collaborators clone the repository.

### Object Naming

Use descriptive names that are both expressive and explicit:

```r
# Good
vaccination_coverage_2017_18
absentee_flu_residuals

# Less good
vaxcov_1718
flu_res
```

Prefer nouns for objects and verbs for functions:

```r
# Good
clean_data <- prep_study_data(raw_data)  # verb for function, noun for object

# Less clear
data <- process(input)
```

## Testing Requirements {#sec-r-testing}

**ALWAYS establish tests BEFORE modifying functions.** This ensures changes preserve existing behavior and new behavior is correctly validated.

### When to Use Snapshot Tests

Use snapshot tests (`expect_snapshot()`, `expect_snapshot_value()`) when:

- Testing complex data structures (data frames, lists, model outputs)
- Validating statistical results where exact values may vary slightly
- Output format stability is important

```r
test_that("prep_study_data produces expected structure", {
  result <- prep_study_data(raw_data)
  expect_snapshot_value(result, style = "serialize")
})
```

### When to Use Explicit Value Tests

Use explicit tests (`expect_equal()`, `expect_identical()`) when:

- Testing simple scalar outputs
- Validating specific numeric thresholds
- Testing Boolean returns or categorical outputs

```r
test_that("calculate_mean returns correct value", {
  expect_equal(calculate_mean(c(1, 2, 3)), 2)
  expect_equal(calculate_ratio(3, 7), 0.4285714, tolerance = 1e-6)
})
```

### Testing Best Practices

- **Seed randomness**: Use `withr::local_seed()` for reproducible tests
- **Use small test cases**: Keep tests fast
- **Test edge cases**: Missing values, empty inputs, boundary conditions
- **Test errors**: Verify functions fail appropriately with invalid input

```r
test_that("prep_study_data handles edge cases", {
  # Empty input
  expect_error(prep_study_data(data.frame()))
  
  # Missing required columns
  expect_error(prep_study_data(data.frame(x = 1)))
  
  # Valid input with missing values
  result <- prep_study_data(data.frame(id = 1:3, value = c(1, NA, 3)))
  expect_true(all(!is.na(result$value)))
})
```

## Iterative Operations {#sec-iteration}

When applying analyses with different variations (outcomes, exposures, subgroups), use functional programming approaches:

### `lapply()` and `sapply()`

```r
# Apply function to each element
results <- lapply(outcomes, function(y) {
  run_analysis(data, outcome = y)
})

# Simplify to vector if possible
summary_stats <- sapply(data_list, mean)
```

### `purrr::map()` Family

The `purrr` package provides type-stable alternatives:

```r
library(purrr)

# Always returns a list
results <- map(outcomes, ~ run_analysis(data, outcome = .x))

# Type-specific variants
means <- map_dbl(data_list, mean)        # Returns numeric vector
models <- map(splits, ~ lm(y ~ x, data = .x))  # Returns list of models
```

### `purrr::pmap()` for Multiple Arguments

When iterating over multiple parameter lists:

```r
params <- tibble(
  outcome = c("outcome1", "outcome2", "outcome3"),
  exposure = c("exp1", "exp2", "exp3"),
  covariate_set = list(c("age", "sex"), c("age"), c("age", "sex", "bmi"))
)

results <- pmap(params, function(outcome, exposure, covariate_set) {
  run_analysis(
    data = study_data,
    outcome = outcome,
    exposure = exposure,
    covariates = covariate_set
  )
})
```

### Parallel Processing

For computationally intensive work, use `future` and `furrr`:

```r
library(future)
library(furrr)

# Set up parallel processing
plan(multisession, workers = availableCores() - 1)

# Parallel version of map()
results <- future_map(large_list, time_consuming_function, .progress = TRUE)
```

## Reading and Saving Data {#sec-data-io}

### RDS Files (Preferred)

Use RDS format for R objects:

```r
# Save single object
readr::write_rds(analysis_results, here("results", "analysis.rds"))

# Read back
results <- readr::read_rds(here("results", "analysis.rds"))
```

**Avoid `.RData` files** because:
- You can't control object names when loading
- Can't load individual objects
- Creates confusion in older code

### CSV Files

For tabular data that may be shared with non-R users:

```r
# Write
readr::write_csv(data, here("data-raw", "clean_data.csv"))

# Read
data <- readr::read_csv(here("data-raw", "clean_data.csv"))

# For very large files, use data.table
data.table::fwrite(large_data, "big_file.csv")
data <- data.table::fread("big_file.csv")
```

## Version Control and Collaboration {#sec-r-version-control}

### Version Numbers

Follow semantic versioning (MAJOR.MINOR.PATCH):

- Development versions: `0.0.0.9000`, `0.0.0.9001`, etc.
- First release: `0.1.0`
- Bug fixes: increment PATCH (e.g., `0.1.0` → `0.1.1`)
- New features: increment MINOR (e.g., `0.1.1` → `0.2.0`)
- Breaking changes: increment MAJOR (e.g., `0.2.0` → `1.0.0`)

```r
# Increment version
usethis::use_version()
```

### NEWS File

Document all user-facing changes in `NEWS.md`:

```markdown
# myproject 0.2.0

## New Features

* Added `prep_study_data()` function for data preparation
* Implemented sensitivity analysis in `sensitivity_analysis()`

## Bug Fixes

* Fixed handling of missing values in `calculate_mean()`

## Breaking Changes

* Renamed `old_function()` to `new_function()`
```

### Git Workflow

```r
# Set up Git and GitHub
usethis::use_git()
usethis::use_github()

# Before committing changes
devtools::document()        # Update documentation
devtools::test()            # Run tests
devtools::check()           # Full package check
```

## Quality Assurance Checklist {#sec-r-qa-checklist}

Before submitting a pull request or finalizing analysis, verify:

- [ ] All functions have complete roxygen2 documentation
- [ ] All functions have corresponding tests
- [ ] `devtools::document()` has been run
- [ ] `devtools::test()` passes with no failures
- [ ] `devtools::check()` passes with no errors, warnings, or notes
- [ ] `lintr::lint_package()` shows no issues (or only acceptable ones)
- [ ] `spelling::spell_check_package()` passes
- [ ] Version number has been incremented
- [ ] `NEWS.md` has been updated with changes
- [ ] `README.Rmd` has been updated (if needed) and `README.md` regenerated
- [ ] `pkgdown::build_site()` builds successfully
- [ ] All changes committed and pushed to GitHub

## Continuous Integration {#sec-r-ci}

Set up automated checks using GitHub Actions:

```r
# Add standard R package CI workflows
usethis::use_github_action("check-standard")    # R CMD check
usethis::use_github_action("test-coverage")     # Code coverage
usethis::use_github_action("pkgdown")           # Deploy website
```

These workflows automatically:

- Run R CMD check on multiple platforms (Linux, macOS, Windows)
- Calculate test coverage
- Build and deploy your pkgdown website
- Ensure code quality before merging

## Automated Code Styling {#sec-auto-styling}

### RStudio Built-in Formatting

Use RStudio's built-in autoformatter (keyboard shortcut: `CMD-Shift-A` or `Ctrl-Shift-A`) to quickly format highlighted code.

### styler Package

For automated styling of entire projects:

```r
# Install styler
install.packages("styler")

# Style all files in R/ directory
styler::style_dir("R/")

# Style entire package
styler::style_pkg()

# Note: styler modifies files in-place
# Always use with version control so you can review changes
```

### lintr Package

For checking code style without modifying files:

```r
# Install lintr
install.packages("lintr")

# Lint the entire package
lintr::lint_package()

# Lint a specific file
lintr::lint("R/my_function.R")
```

The linter checks for:
- Unused variables
- Improper whitespace
- Line length issues
- Style guide violations

You can customize linting rules by creating a `.lintr` file in your project root.

## Additional Resources {#sec-r-resources}

- [R Packages book](https://r-pkgs.org/) by Hadley Wickham and Jenny Bryan
- [Tidyverse style guide](https://style.tidyverse.org/)
- [usethis documentation](https://usethis.r-lib.org/)
- [devtools documentation](https://devtools.r-lib.org/)
- [pkgdown documentation](https://pkgdown.r-lib.org/)
- [testthat documentation](https://testthat.r-lib.org/)
